[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Quantitative Analysis 1",
    "section": "",
    "text": "Welcome to the homepage for Quantitative Analysis 1 (PHD1502-1)!"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Course Syllabus",
    "section": "",
    "text": "To download a pdf version of this syllabus, click here.\nMeeting Time: Tuesdays, 5 PM to 7 PM ET\nLocation: Zoom Meeting and Smith 307 when in person\nEmail: alex.lopilato@gmail.com (for quick responses) or alopilato@bentley.edu (for discussions about grades, personal info, etc.)\nOffice Hours: By request (will likely be virtual as I do not have an office on campus)\nCourse Format: Hybrid Synchronous\n\nCourse Description\nThis course focuses on applications of linear regression to model data collected from observational, quasi-experimental, and experimental study designs. This course will introduce students to the basics of linear regression and its applications to model and theory building.\n\n\nCourse Objectives\nBy the end of this course, you will:\n\nHave an understanding of the general linear model.\nHave an understanding of how to use linear models to empirically test theoretical models.\nHave an understanding of how to use linear regression in your own research.\nFeel comfortable using R to estimate linear models.\n\n\n\nTextbooks\n\nDarlington, A. B. & Hayes, A. F. (2016). Regression Analysis and Linear Models: Concepts, Applications, and Implementation (Referred to as RAGLM)\nBékés, G. & Kézdi, G. (2021). Data Analysis for Business Economics, and Policy. (Referred to as DBEP)\n\n\n\nCourse Technology\nThis course will use Brightspace to post important updates and Zoom recordings. Please do not use Brightspace to email me! Use either of the emails listed above.\n\nCourse Website\nThe website for the course is: https://alopilato88.github.io/quantitative-analysis-1/. All of the lectures can be found there and will be made publicly available on the day of the lecture.\n\n\nStatistical Computing\nThis course will rely solely on the R programming language for all statistical computing. At the very least, you will need to download R to your local machine (or use your lab computer), and I highly recommend also downloading RStudio, which is an Integrated Development Environment (IDE) that makes programming in R (and other programming languages) much easier. Please reach out to me if you are unable to install R.\nWhile you can use another statistical software program such as SPSS, SAS, or STATA, I will not be providing example code for those different programs. I will only be providing example R code.\n\n\n\nGrading Criteria\nA combination of homework and a final research project will be used to determine your grade for this course. Homework will account for 90% of your grade and the research project will account for 10%. While I encourage you to consult with your colleagues (your instructor, classmates, professors, etc.) when you are struggling with any of the homework assignments or the research project, your final products must be your own.\n\nHomework\nI will send out periodic homework assignments in order to give you students experience applying the methods we discuss in class. These assignments will be a mix of conceptual, statistical, and computational exercises. Please reach out to me if you find yourself struggling or overly stressing with these assignments. They are meant to be a learning tool not a major stressor!\n\n\nResearch Project\nOne of the more exciting things about being a graduate student is that you are able to explore the topics you find interesting. Use this research project to apply the methods we learn to any topic of your choice. Alternatively, I have fictitious data you can use if you do not have access to data of your own. Please talk to me by October 17th about your research project, even if your not 100% sure about it.\nYour final product should include four components:\n\nA brief introduction to your topic, the theory you are testing, and your hypotheses.\nA methods section write-up that parallels methods sections found in published articles.\nA results secection write-up that parallels methods sections found in published articles.\nThe code you used to analyze your data along with the dataset (assuming you are allowed to share the data).\n\n\n\n\nUniversity Honor Code, Academic Honesty Policy, Bentley Core Values\nThis class will be conducted in full accordance with The Bentley Core Values. Please reread the Values, which can be found at https://www.bentley.edu/about/mission-and-values.\nBentley College Honor Code: The Bentley College Honor Code formally recognized the responsibility of students to act in an ethical manner. It expects all students to maintain academic honesty in their own work, recognizing that most students will maintain academic honesty because of their own high standards. The Honor Code expects students to promote ethical behavior throughout the Bentley community and to take responsible action when there is a reason to suspect dishonesty.\nPersonal Academic Behavior: A student acknowledges that all submitted work (e.g., examination, papers, cases homework assignments) must be his or her own. The exception is the case in which an instructor permits or encourages students to work together on some or all assignments. When a student is in doubt, he or she should consult the instructor for clarification.\nResponsible Actions: Each student, as an integral member of the academic community, is expected to make a commitment to act honestly and to reject dishonesty on the part of other students. The students as a community are responsible for maintaining an ethical environment. Policies may be found at: http://www.bentley.edu/centers/alliance/academic-integrity\n\n\nBias Incident Reporting\nThe Bias Incident Response Team (BIRT) provides students affected by bias or bias-related incidents with access to appropriate resources. Where appropriate, BIRT assists the University in its response to situations that may impact the overall campus climate related to diversity and inclusion. Working closely with appropriate students, faculty, committees, organizations, and staff, BIRT plays an educational role in fostering an inclusive campus community and supporting targeted individuals when bias or bias-related incidents occur. More information about BIRT and how to file a bias incident report can be found at: https://www.bentley.edu/offices/student-affairs/birt.\n\n\nSpecial Accommodations\nStatement of Disabilities: Bentley University abides by Section 504 of the Rehabilitation Act of 1973 and the Americans with Disabilities Act of 1990 which stipulate no student shall be denied the benefits of an education solely by reason of a disability. If you have a hidden or visible disability which may require classroom accommodations, please call (if you are a residential student or on online student) Disability Services within the first 4 weeks of the semester to schedule an appointment. Disability Services is located in the Office of Academic Services (JEN 336, 781.891.2004). Disability Services is responsible for managing accommodations and services for all students with disabilities.\n\n\nWriting Center\nThe Writing Center offers one-on-one tutoring to students of all years and skill levels. Located on the lower level of the Bentley library (room 023), the Writing Center provides a welcoming and supportive environment in which students can work on writing from any class or discipline. Writers are encouraged to visit at all stages of the writing process; they can come with a draft, an outline, or just some initial thoughts and questions.\nStaffed by highly skilled student tutors, the Writing Center is open six days a week. Most conferences will be conducted online, but limited in-person hours will be held by appointment only. Appointments can be made at bentley.mywconline.net. For specific hours and additional information, please visit the Writing Center SharePoint site.\n\n\nESOL\nThe ESOL Center offers online appointments for helping undergraduate and graduate students strengthen their writing and English language skills. Our ESOL faculty tutors specialize in working with international and multilingual students to provide one-on-one support for all courses writing at any stage in the writing process. Along with individualized help for writing, the ESOL tutors provide guidance and feedback for documenting sources, oral presentation practice, and pronunciation/fluency enrichment.\nThe ESOL Center offers real-time video appointments Monday through Friday between 7:30 a.m. and 10:00 p.m. These can be reserved through our website: https://bentleyesol.mywconline.net. The complete information about booking appointments and uploading papers is clarified on the website’s announcement page.\n\n\nCourse Style\nI want this course to be an enjoyable and engaging experience for all, so although I will have lecture slides to talk through, I will also be using this course more as a discussion about statistical topics, not a lecture about them.\nIn order to meaningfully engage in this discussion, I encourage you to read through the required readings and skim through the supplemental readings (although I think they are all interesting reads!). I understand everyone is busy, so, despite being labeleled “Required Readings”, I will not make the readings required, but to make this course useful you will need to engage with the material and come with questions!\nTo be successful in this course, you will need to:\n\nDo the required readings and skim the supplemental readings\nCome to class and bring questions\nEngage in the course discussions\nMost importantly, ASK QUESTIONS\n\n\n\nTentative Course Schedule\nNOTE: The course syllabus is a general plan for the course and as such there may be deviations throughout the semester. Supplemental readings are any readings that are italicized or hyperlinked.\n\n\n\nDate\nTopic\nReadings\n\n\n\n\n9/5\nBivariate Measures of Association & Simple Regression\n\nRAGLM – Chapter 1: Statistical Control & Linear Models\nRAGLM – Chapter 2: The Simple Regression Model\nDBEP – Chapter 4.6: Dependence, Covariance, & Correlation\nR for Data Science – Chapter 2: Data Visualization\nR for Data Science – Chapter 3: Workflow: Basics\n\n\n\n\n\n\n\n\n9/12\nMeasures of Partial Association & Multiple Regression\n\nRAGLM – Chapter 3: Partial Relationship & the Multiple Regression Model\nR for Data Science – Chapter 4: Data Transformation\nR for Data Science – Chapter 5: Workflow: Code Style\nR for Data Science – Chapter 6: Data Tidying\n\n\n\n\n\n\n\n\n9/19\nStatistical Inference for Regression\n\nRAGLM – Chapter 4: Statistical Inference in Regression\nDBEP – Chapter 9: Generalizing Results of a Regression\nModernDive - Chapter 10.1 to 10.3\n\n\n\n\n\n\n\n\n9/29\nCategorical Predictors & Model Building\nImmersion Day\n\nRAGLM – Chapter 5: Extending Regression Analysis & Principles\nRAGLM – Chapter 8: Assessing the Importance of Regressors\nRAGLM – Chapter 9: Multicategorical Regressors\nModernDive – Chapter 5: Basic Regression\nModernDive – Chapter 6: Multiple Regression\n\n\n\n\n\n\n\n\n10/10\nFall Break - No Class\n\n\n\n\n\n\n\n\n10/17\nNonlinear Effects, Interactions, & Model Building\n\nRAGLM – Chapter 12.1 & 12.2: Nonlinear Relationships\nRAGLM – Chapter 13: Linear Interaction\nR for Data Science – 10: Layers\nR for Data Science – Chapter 11: Exploratory Data Analysis\n\n\n\n\n\n\n\n\n10/24\nMore on Interactions\n\nRAGLM – Chapter 14: Probing Interactions & Various Complexities\nRAGLM – Chapter 11: Mutliple Tests\nDawson (2014). Moderation in management research: What, why, when, and how.\nHaans et al. (2016). Thinking about U: Theorizing and testing U- and inverted U-shaped relationships\n\n\n\n\n\n\n\n\n11/3\nMediation, Path Analysis, & and Diagnostics\nImmersion Day\n\nRAGLM – Chapter 15: Mediation & Path Analysis\nRAGLM – 16: Detecting & Managing Irregularities\nRAGLM – 17.3: An Assortment of Problems\nAguinis et al. (2017). Improving our understanding of moderation and mediation in strategic management research.\nVanderWeele (2016). Mediation analysis: A practitioner’s guide.\nChatterjee & Yilmaz (1992). A review of regression diagnostics for behavioral research.\n\n\n\n\n\n\n\n\n11/7\nPower, Precision, & Measurement Error\n\nRAGLM – 17.1: Power & Precision of Estimation\nRAGLM – 17.2: Measurement Error\nRee & Caretta (2006). The role of measurement error in familiar statistics.\nCortina (1993). What is coefficient alpha: An examination of theory and application\n\n\n\n\n\n\n\n\n11/14\nIntroduction to Causation\n\nDBEP – Chapter 19: A Framework for Causal Analysis\nWest & Thoemmes (2010). Campbell’s and Rubin’s perspectives on causal inference.\n\n\n\n\n\n\n\n\n11/21\nThanksgiving Break - No Class\n\n\n\n\n\n\n\n\n11/28\nExperimental Design & Analysis\n\nDBEP – Chapter 20: Designing & Analyzing Experiments\nRAGLM – Chapter 6: Statistical versus Experimental Control\nPosakoff & Podsakoff (2019). Experimental designs in management and leadership research\nEden (2017). Field experiments in organizations.\nLuca & Bazerman (2020). Want to make better decisions? Start experimenting.\n\n\n\n\n\n\n\n\n12/5\nCausal Inference with Observational Data\n\nDBEP – Chapter 21: Regression & Matching with Observational Data\nNarita et al. (2023). Causal inference with observational data: A tutorial on propensity score analysis.\nHernan (2018). The C-word: Scientific euphemisms do not improve causal inference from observational data.\n\n\n\n\n\n\n\n\n12/12\nCausal Mediation\n\nCuartas & McCoy (2021). Causal mediation in developmental science: A primer.\nNguyen et al. (2021). Clarifying causal mediation analysis for the applied researcher\nImai et al. (2010). A general approach to causal mediation analysis.\nBullock et al. (2010). Yes, but what’s the mechanism? (Don’t expect an easy answer)."
  },
  {
    "objectID": "lectures/01-lecture-page.html",
    "href": "lectures/01-lecture-page.html",
    "title": "Quantitative Analysis 1",
    "section": "",
    "text": "Next Week’s Materials »"
  },
  {
    "objectID": "lectures/01-lecture-page.html#lecture-stats-bootcamp",
    "href": "lectures/01-lecture-page.html#lecture-stats-bootcamp",
    "title": "Quantitative Analysis 1",
    "section": "Lecture: Stats Bootcamp",
    "text": "Lecture: Stats Bootcamp\n\n\nTo download a pdf version of these slides, click here.\nTo download the R script that follows the R portion of the lecture, click here."
  },
  {
    "objectID": "lectures/01-lecture-page.html#r-in-class-assignment",
    "href": "lectures/01-lecture-page.html#r-in-class-assignment",
    "title": "Quantitative Analysis 1",
    "section": "R: In-Class Assignment",
    "text": "R: In-Class Assignment\n\nx <- rnorm(500)\nmean(x)\n\n[1] -0.01222021"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#slide-1",
    "href": "lectures/01-lecture-slides.html#slide-1",
    "title": "Stats Bootcamp",
    "section": "Slide 1",
    "text": "Slide 1"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#slide-2",
    "href": "lectures/01-lecture-slides.html#slide-2",
    "title": "Stats Bootcamp",
    "section": "Slide 2",
    "text": "Slide 2"
  },
  {
    "objectID": "notes/00-notes-template.html",
    "href": "notes/00-notes-template.html",
    "title": "Notes Template",
    "section": "",
    "text": "This is a template for me to use to right my lecture notes."
  },
  {
    "objectID": "notes/00-notes-template.html#test",
    "href": "notes/00-notes-template.html#test",
    "title": "Notes Template",
    "section": "Test",
    "text": "Test"
  },
  {
    "objectID": "notes/00-notes-template.html#test-2",
    "href": "notes/00-notes-template.html#test-2",
    "title": "Notes Template",
    "section": "Test 2",
    "text": "Test 2"
  },
  {
    "objectID": "notes/00-notes-template.html#test-3",
    "href": "notes/00-notes-template.html#test-3",
    "title": "Notes Template",
    "section": "Test 3",
    "text": "Test 3\nTest.\n\nTest 4\nThis is a test."
  },
  {
    "objectID": "notes/01-lecture-notes.html#sampling-distribution",
    "href": "notes/01-lecture-notes.html#sampling-distribution",
    "title": "Lecture 1 Notes",
    "section": "Sampling Distribution",
    "text": "Sampling Distribution"
  },
  {
    "objectID": "notes/01-lecture-notes.html#confidence-interval",
    "href": "notes/01-lecture-notes.html#confidence-interval",
    "title": "Lecture 1 Notes",
    "section": "Confidence Interval",
    "text": "Confidence Interval"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#learning-goals",
    "href": "lectures/01-lecture-slides.html#learning-goals",
    "title": "An Introduction to Programming & Statistics",
    "section": "Learning Goals",
    "text": "Learning Goals\n\nData importing and transformation with R\nEstimate and interpret a statistical test"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#overview",
    "href": "lectures/01-lecture-slides.html#overview",
    "title": "An Introduction to Statistics & Programming",
    "section": "Overview",
    "text": "Overview\n\nIntroduction to Statistical Science\n\nDescriptive Statistics\nQuick look at Probability Theory\nInferential Statistics\n\nIntroduction to Programming with R\n\nBase R\nTidyverse"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#first-steps",
    "href": "lectures/01-lecture-slides.html#first-steps",
    "title": "An Introduction to Statistics & Programming",
    "section": "First Steps",
    "text": "First Steps\n\nDownload R if you haven’t already\nDownload RStudio"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#what-is-r",
    "href": "lectures/01-lecture-slides.html#what-is-r",
    "title": "An Introduction to Statistics & Programming",
    "section": "What is R?",
    "text": "What is R?\nR is a programming language that is generally used for statistical computing."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#why-learn-r",
    "href": "lectures/01-lecture-slides.html#why-learn-r",
    "title": "An Introduction to Statistics & Programming",
    "section": "Why Learn R?",
    "text": "Why Learn R?\nTo analyze the data you collect, you will need to be familiar with some kind of general programming language (R, Python, etc.) or a more specific statistical program (SPSS, SAS). I recommend and use R because it is:\n\nOpen-source (free to download, use, and improve)\nHighly flexible language\nCan estimate A LOT of different statistical models"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#rstudio-and-ides",
    "href": "lectures/01-lecture-slides.html#rstudio-and-ides",
    "title": "An Introduction to Programming & Statistics",
    "section": "RStudio and IDEs",
    "text": "RStudio and IDEs\nWe will be using RStudio to do all of our statistical computing. RStudio is an integrated development environment (IDE) initially developed for R.\nIf you have not already, please go ahead and download RStudio from: https://posit.co/downloads/."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#basics-of-the-integrated-development-environment-ide",
    "href": "lectures/01-lecture-slides.html#basics-of-the-integrated-development-environment-ide",
    "title": "An Introduction to Programming & Statistics",
    "section": "Basics of the Integrated Development Environment (IDE)",
    "text": "Basics of the Integrated Development Environment (IDE)\nAn IDE is an application that makes programming a little easier and organized. As we will see shortly, it includes all of the tools one needs to program effectively and efficiently."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#writing-scripts-in-r",
    "href": "lectures/01-lecture-slides.html#writing-scripts-in-r",
    "title": "An Introduction to Statistics & Programming",
    "section": "Writing Scripts in R",
    "text": "Writing Scripts in R\nJust because you can write R code in just about any kind of digital document (Word, Notes, Notepad) does not mean you should!\nIt is best practice to write your code in an R Script (.R) in RStudio (in my opinion at least)."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#using-comments-in-your-code",
    "href": "lectures/01-lecture-slides.html#using-comments-in-your-code",
    "title": "An Introduction to Statistics & Programming",
    "section": "Using Comments in Your Code",
    "text": "Using Comments in Your Code\nYou can write comments in your own code by beginning a line with #. R will not evaluate any text on a line that begins with #.\n\n# This is a comment. Use comments to leave yourself notes in your script."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#objects-in-r",
    "href": "lectures/01-lecture-slides.html#objects-in-r",
    "title": "An Introduction to Statistics & Programming",
    "section": "Objects in R",
    "text": "Objects in R\nEverything you do in R will involve some kind of object that you have created. Think of an object like a box that you can place data in, so that R can later access and manipulate the data. An important of the code below is the assignment operator &lt;- which is how R knows to assign value to object_name.\n\nobject_name &lt;- value"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#functions-in-r",
    "href": "lectures/01-lecture-slides.html#functions-in-r",
    "title": "An Introduction to Statistics & Programming",
    "section": "Functions in R",
    "text": "Functions in R\nFunctions are objects in R that take user inputs, apply some predefined set of operations, and return an expected output.\n\nsum(c(1, 3))\n\n[1] 4"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#writing-your-own-function",
    "href": "lectures/01-lecture-slides.html#writing-your-own-function",
    "title": "An Introduction to Programming & Statistics",
    "section": "Writing Your Own Function",
    "text": "Writing Your Own Function\nR comes with many predefined functions, but often times you will want to write your own function to accomplish some repetive task. If you find yourself copy and pasting the same lines of code, then it is time to turn those lines into a function."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#the-elements-of-a-function",
    "href": "lectures/01-lecture-slides.html#the-elements-of-a-function",
    "title": "An Introduction to Statistics & Programming",
    "section": "The Elements of a Function",
    "text": "The Elements of a Function\nR comes with a variety of predefined functions and they all follow the same structure:\n\nA name for the function.\nThe arguments that change across different function calls.\nThe body which contains the code that is repeated across different calls."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#the-elements-of-a-function-1",
    "href": "lectures/01-lecture-slides.html#the-elements-of-a-function-1",
    "title": "An Introduction to Statistics & Programming",
    "section": "The Elements of a Function",
    "text": "The Elements of a Function\n\nname &lt;- function(argument) {\n  body\n}"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#example-function",
    "href": "lectures/01-lecture-slides.html#example-function",
    "title": "An Introduction to Programming & Statistics",
    "section": "Example Function",
    "text": "Example Function\n\ncreate_email_address &lt;- function(name) {\n  email &lt;- stringr::str_to_title(name)\n  email &lt;- stringr::str_replace_all(email, \" \", \"_\")\n  email &lt;- paste0(email, \"@organization.com\")\n  return(email)\n}\n\ncreate_email_address(name = \"john doe\")\n\n[1] \"John_Doe@organization.com\"\n\ncreate_email_address(name = \"JOHN DOE\")\n\n[1] \"John_Doe@organization.com\"\n\ncreate_email_address(name = \"jOhN dOe\")\n\n[1] \"John_Doe@organization.com\""
  },
  {
    "objectID": "lectures/01-lecture-slides.html#linking-functions-together",
    "href": "lectures/01-lecture-slides.html#linking-functions-together",
    "title": "An Introduction to Statistics & Programming",
    "section": "Linking Functions Together",
    "text": "Linking Functions Together\nR lets you link any number of functions together by nesting them. R will start with the innermost function and then work its way outward.\n\nsum(abs(c(-1, -1, 1, 1)))\n\n[1] 4"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#packages-the-lifeblood-of-r",
    "href": "lectures/01-lecture-slides.html#packages-the-lifeblood-of-r",
    "title": "An Introduction to Statistics & Programming",
    "section": "Packages: The Lifeblood of R",
    "text": "Packages: The Lifeblood of R\nA lot of what makes R such an effective programming language (especially for statistics) is the sheer number of available R packages. An R package is a collection of functions that complement one another for a given task. New packages are always being developed and anyone can author one!"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#getting-help-with-r",
    "href": "lectures/01-lecture-slides.html#getting-help-with-r",
    "title": "An Introduction to Statistics & Programming",
    "section": "Getting Help with R",
    "text": "Getting Help with R\nThere are two ways to get help in R:\n\nAdd ? in front of your function, which will result in RStudio displaying the help page for that function.\nGoogle what you are trying to do. More often than not, someone else has run into your problem, found a solution, and posted it. Stand on their shoulders!\n\n\n?sum()"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#atomic-vectors",
    "href": "lectures/01-lecture-slides.html#atomic-vectors",
    "title": "An Introduction to Statistics & Programming",
    "section": "Atomic Vectors",
    "text": "Atomic Vectors\n\nAn atomic vector is just a simple vector of data.\nR recognizes six types of atomic vectors:\n\nIntegers\nDoubles (Numeric)\nCharacters\nLogicals\nComplex\nRaw"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#integer-numeric-vectors",
    "href": "lectures/01-lecture-slides.html#integer-numeric-vectors",
    "title": "An Introduction to Statistics & Programming",
    "section": "Integer & Numeric Vectors",
    "text": "Integer & Numeric Vectors\nInteger vectors contain only integers. Add L after each number so R recognizes it as an integer. Numeric (doubles) vectors contain real numbers. These are the default vectors for numbers.\n\ninteger_vec &lt;- c(1L, 2L, 50L)\nnumeric_vec &lt;- c(1, 2, 50, 45.23)"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#character-vector",
    "href": "lectures/01-lecture-slides.html#character-vector",
    "title": "An Introduction to Statistics & Programming",
    "section": "Character Vector",
    "text": "Character Vector\nCharacter vectors contain only text data also referred to as string data. Basically anything surrounded by \"\" or '' is considered string data.\n\ncharacter_vec &lt;- c(\"1\", \"abc\", \"$#2\")"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#logical-vector",
    "href": "lectures/01-lecture-slides.html#logical-vector",
    "title": "An Introduction to Statistics & Programming",
    "section": "Logical Vector",
    "text": "Logical Vector\nLogical vectors are vectors that can only contain TRUE or FALSE values also referred to as boolean values.\n\nlogical_vec &lt;- c(TRUE, FALSE)"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#applying-functions-to-vectors",
    "href": "lectures/01-lecture-slides.html#applying-functions-to-vectors",
    "title": "Stats Bootcamp",
    "section": "Applying Functions to Vectors",
    "text": "Applying Functions to Vectors"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#nesting-multiple-functions",
    "href": "lectures/01-lecture-slides.html#nesting-multiple-functions",
    "title": "Stats Bootcamp",
    "section": "Nesting Multiple Functions",
    "text": "Nesting Multiple Functions"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#using-the-pipe",
    "href": "lectures/01-lecture-slides.html#using-the-pipe",
    "title": "An Introduction to Statistics & Programming",
    "section": "Using the pipe |>",
    "text": "Using the pipe |&gt;\nThe |&gt; operator allows you to take the output of one function and feed it directly into the first argument of the next function. Using the |&gt; makes it easier to read your code, which is a good thing.\n\nc(-1, -1, 1, 1) |&gt;\n  abs() |&gt;\n  sum()\n\n[1] 4"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#adding-attributes",
    "href": "lectures/01-lecture-slides.html#adding-attributes",
    "title": "An Introduction to Statistics & Programming",
    "section": "Adding Attributes",
    "text": "Adding Attributes\nYou can think of attributes as metadata for R objects. As a user you will not need to worry too much about attributes directly, but attributes tell R how to interact with the specific object and allow the user to store information that is secondary to the analyses they are conducting."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#names-attribute",
    "href": "lectures/01-lecture-slides.html#names-attribute",
    "title": "An Introduction to Statistics & Programming",
    "section": "names Attribute",
    "text": "names Attribute\n\ndays_of_week &lt;- 1:7 \nnames(days_of_week) &lt;- c(\"mon\", \"tues\", \"wed\", \"thurs\", \"fri\", \"sat\", \"sun\")\nnames(days_of_week)\n\n[1] \"mon\"   \"tues\"  \"wed\"   \"thurs\" \"fri\"   \"sat\"   \"sun\"  \n\nattributes(days_of_week)\n\n$names\n[1] \"mon\"   \"tues\"  \"wed\"   \"thurs\" \"fri\"   \"sat\"   \"sun\""
  },
  {
    "objectID": "lectures/01-lecture-slides.html#dim-attribute",
    "href": "lectures/01-lecture-slides.html#dim-attribute",
    "title": "An Introduction to Statistics & Programming",
    "section": "dim Attribute",
    "text": "dim Attribute\n\ndays_of_week &lt;- 1:14\ndim(days_of_week) &lt;- c(2, 7) # 2 Rows, 7 Columns\nattributes(days_of_week)\n\n$dim\n[1] 2 7\n\nclass(days_of_week)\n\n[1] \"matrix\" \"array\""
  },
  {
    "objectID": "lectures/01-lecture-slides.html#creating-factors",
    "href": "lectures/01-lecture-slides.html#creating-factors",
    "title": "An Introduction to Statistics & Programming",
    "section": "Creating Factors",
    "text": "Creating Factors\nR stores categorical data using factors, which are integer vectors with two attributes: class and levels.\n\ndays_of_week &lt;- factor(c(\"mon\", \"tues\", \"wed\", \"thurs\", \"fri\", \"sat\", \"sun\"))\ntypeof(days_of_week)\n\n[1] \"integer\"\n\nattributes(days_of_week)\n\n$levels\n[1] \"fri\"   \"mon\"   \"sat\"   \"sun\"   \"thurs\" \"tues\"  \"wed\"  \n\n$class\n[1] \"factor\""
  },
  {
    "objectID": "lectures/01-lecture-slides.html#lists-a-flexible-representation-of-data",
    "href": "lectures/01-lecture-slides.html#lists-a-flexible-representation-of-data",
    "title": "An Introduction to Programming & Statistics",
    "section": "Lists: A Flexible Representation of Data",
    "text": "Lists: A Flexible Representation of Data\nLike vectors, lists are another way to structure data in R. Rather than group individual elements, however, lists group together different types of R objects including other lists.\n\nlist_1 &lt;- list(c(1, 3, 4), c(\"a\", \"b\"), list(3, 4))"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#data-frames-best-way-to-represent-data",
    "href": "lectures/01-lecture-slides.html#data-frames-best-way-to-represent-data",
    "title": "An Introduction to Statistics & Programming",
    "section": "Data Frames: Best way to Represent Data",
    "text": "Data Frames: Best way to Represent Data\nData frames are the best way to structure and store data in R. Data frames are sort of the R equivalent of an excel spreadsheet.\nEach column in a data frame is a vector, so a data frame can combine a numeric vector as one column with a character vector as another column.\n\ndata_frame_1 &lt;- data.frame(NUMERIC = c(1, 3), CHARACTER = c(\"a\", \"b\"), \n                           LOGICAL = c(TRUE, FALSE))\ndata_frame_1\n\n  NUMERIC CHARACTER LOGICAL\n1       1         a    TRUE\n2       3         b   FALSE"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#viewing-your-data",
    "href": "lectures/01-lecture-slides.html#viewing-your-data",
    "title": "An Introduction to Statistics & Programming",
    "section": "Viewing Your Data",
    "text": "Viewing Your Data\nYou can use View() to open up a spreadsheet-like view of your data.\n\nView(data_frame_1)"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#selecting-data-from-vectors-and-data-frames",
    "href": "lectures/01-lecture-slides.html#selecting-data-from-vectors-and-data-frames",
    "title": "Stats Bootcamp",
    "section": "Selecting Data from Vectors and Data Frames",
    "text": "Selecting Data from Vectors and Data Frames"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#selecting-data-from-lists",
    "href": "lectures/01-lecture-slides.html#selecting-data-from-lists",
    "title": "Stats Bootcamp",
    "section": "Selecting Data from Lists",
    "text": "Selecting Data from Lists"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#na-value-missing-data",
    "href": "lectures/01-lecture-slides.html#na-value-missing-data",
    "title": "Stats Bootcamp",
    "section": "NA Value: Missing Data",
    "text": "NA Value: Missing Data"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#reading-writing-data",
    "href": "lectures/01-lecture-slides.html#reading-writing-data",
    "title": "Stats Bootcamp",
    "section": "Reading & Writing Data",
    "text": "Reading & Writing Data"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#what-is-the-tidyverse",
    "href": "lectures/01-lecture-slides.html#what-is-the-tidyverse",
    "title": "An Introduction to Statistics & Programming",
    "section": "What is the Tidyverse?",
    "text": "What is the Tidyverse?\nThe tidyverse is a collection of R packages that “share a common philosophy of data and R programming and are designed to work together.”"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#tibble-data-frame-of-tidyverse",
    "href": "lectures/01-lecture-slides.html#tibble-data-frame-of-tidyverse",
    "title": "An Introduction to Statistics & Programming",
    "section": "tibble: Data frame of Tidyverse",
    "text": "tibble: Data frame of Tidyverse\nTibbles are the tidyverse’s version of a data.frame. They can be loaded from the tidyverse package: tibble.\n\ndata_employees_tbl &lt;- tibble::as_tibble(data_employees)\ndata_employees_tbl\n\n# A tibble: 1,470 × 36\n   employee_id active stock_opt_lvl trainings   age commute_dist ed_lvl ed_field\n         &lt;int&gt; &lt;chr&gt;          &lt;int&gt;     &lt;int&gt; &lt;int&gt;        &lt;int&gt;  &lt;int&gt; &lt;chr&gt;   \n 1        1001 No                 0         0    41            1      2 Life Sc…\n 2        1002 Yes                1         3    49            8      1 Life Sc…\n 3        1003 No                 0         3    37            2      2 Other   \n 4        1004 Yes                0         3    33            3      4 Life Sc…\n 5        1005 Yes                1         3    27            2      1 Medical \n 6        1006 Yes                0         2    32            2      2 Life Sc…\n 7        1007 Yes                3         3    59            3      3 Medical \n 8        1008 Yes                1         2    30           24      1 Life Sc…\n 9        1009 Yes                0         2    38           23      3 Life Sc…\n10        1010 Yes                2         3    36           27      3 Medical \n# ℹ 1,460 more rows\n# ℹ 28 more variables: gender &lt;chr&gt;, marital_sts &lt;chr&gt;, dept &lt;chr&gt;,\n#   engagement &lt;int&gt;, job_lvl &lt;int&gt;, job_title &lt;chr&gt;, overtime &lt;chr&gt;,\n#   business_travel &lt;chr&gt;, hourly_rate &lt;int&gt;, daily_comp &lt;int&gt;,\n#   monthly_comp &lt;int&gt;, annual_comp &lt;int&gt;, ytd_leads &lt;int&gt;, ytd_sales &lt;int&gt;,\n#   standard_hrs &lt;int&gt;, salary_hike_pct &lt;int&gt;, perf_rating &lt;int&gt;,\n#   prior_emplr_cnt &lt;int&gt;, env_sat &lt;int&gt;, job_sat &lt;int&gt;, rel_sat &lt;int&gt;, …"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#dplyr-your-data-multitool",
    "href": "lectures/01-lecture-slides.html#dplyr-your-data-multitool",
    "title": "An Introduction to Statistics & Programming",
    "section": "dplyr: Your Data Multitool",
    "text": "dplyr: Your Data Multitool\nThe package dplyr should become your go-to data manipulation and structuring tool! It contains many useful functions that make it surprisingly easy to manipulate and structure your data."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#selecting-columns-using-dplyrselect",
    "href": "lectures/01-lecture-slides.html#selecting-columns-using-dplyrselect",
    "title": "Stats Bootcamp",
    "section": "Selecting Columns using dplyr::select",
    "text": "Selecting Columns using dplyr::select\n\ndplyr::select(data_frame_1, NUMERIC, LOGICAL)\n\n  NUMERIC LOGICAL\n1       1    TRUE\n2       3   FALSE\n\n\n\ndata_frame_1 |&gt;\n  dplyr::select(NUMERIC, LOGICAL)"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#adding-new-columns-using-dplyrmutate",
    "href": "lectures/01-lecture-slides.html#adding-new-columns-using-dplyrmutate",
    "title": "Stats Bootcamp",
    "section": "Adding New Columns Using dplyr::mutate",
    "text": "Adding New Columns Using dplyr::mutate"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#filtering-rows-using-dplyrfilter",
    "href": "lectures/01-lecture-slides.html#filtering-rows-using-dplyrfilter",
    "title": "Stats Bootcamp",
    "section": "Filtering Rows Using dplyr::filter",
    "text": "Filtering Rows Using dplyr::filter"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#an-example-tibble",
    "href": "lectures/01-lecture-slides.html#an-example-tibble",
    "title": "Stats Bootcamp",
    "section": "An Example tibble",
    "text": "An Example tibble"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#using-to-bring-it-all-together",
    "href": "lectures/01-lecture-slides.html#using-to-bring-it-all-together",
    "title": "Stats Bootcamp",
    "section": "Using |> to bring it all together",
    "text": "Using |&gt; to bring it all together"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#r-resources",
    "href": "lectures/01-lecture-slides.html#r-resources",
    "title": "An Introduction to Statistics & Programming",
    "section": "R Resources",
    "text": "R Resources\nhttps://r4ds.hadley.nz/"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#what-is-statistical-science",
    "href": "lectures/01-lecture-slides.html#what-is-statistical-science",
    "title": "An Introduction to Statistics & Programming",
    "section": "What is Statistical Science?",
    "text": "What is Statistical Science?\n\nStatistical science is the science of developing and applying methods for collecting, analyzing, and interpreting data.\n\n— Agresti & Kateri, 2022"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#three-aspects-of-statistical-science",
    "href": "lectures/01-lecture-slides.html#three-aspects-of-statistical-science",
    "title": "An Introduction to Statistics & Programming",
    "section": "Three Aspects of Statistical Science",
    "text": "Three Aspects of Statistical Science\n\nDesign: Planning on how to gather relevant data.\nDescription: Summarizing the data.\nInference: Making evaluations (generalizations) based on the data."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#design-of-studies",
    "href": "lectures/01-lecture-slides.html#design-of-studies",
    "title": "An Introduction to Statistics & Programming",
    "section": "Design of Studies",
    "text": "Design of Studies\nThe design of a study focuses on planning a study so that it produces useful data. This involves:\n\nDeciding how to sample and who to sample\nConstructing surveys for observational studies\nConstructing treatments for experimental studies"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#descriptive-statistics",
    "href": "lectures/01-lecture-slides.html#descriptive-statistics",
    "title": "Stats Bootcamp",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#inferential-statistics",
    "href": "lectures/01-lecture-slides.html#inferential-statistics",
    "title": "An Introduction to Statistics & Programming",
    "section": "Inferential Statistics",
    "text": "Inferential Statistics\nUsually, when you analyze your data you want to generalize the results from your specific dataset to a broader population or more general phenomenon. This is called statistical inference. We infer something from our data about a more general phenomenon."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#populations",
    "href": "lectures/01-lecture-slides.html#populations",
    "title": "Stats Bootcamp",
    "section": "Populations",
    "text": "Populations\n\nSubject\nParameter"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#actual-vs-conceptual-populations",
    "href": "lectures/01-lecture-slides.html#actual-vs-conceptual-populations",
    "title": "An Introduction to Statistics & Programming",
    "section": "Actual vs Conceptual Populations",
    "text": "Actual vs Conceptual Populations\nDepending on the research question, the population may be real or it may be conceptual. Conceptual populations are often future populations we want to generalize to, but which we have to use data collected on current populations."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#samples-data",
    "href": "lectures/01-lecture-slides.html#samples-data",
    "title": "Stats Bootcamp",
    "section": "Samples & Data",
    "text": "Samples & Data"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#variables",
    "href": "lectures/01-lecture-slides.html#variables",
    "title": "An Introduction to Statistics & Programming",
    "section": "Variables",
    "text": "Variables\nVariables are characteristics of the sample or population that vary across subjects. Data consists of a set of variables:\n\ndata_employees &lt;- peopleanalytics::employees\ndata_employees_tbl &lt;- tibble::as_tibble(data_employees)\n\nset.seed(3)\ndata_employees_tbl |&gt;\n  dplyr::sample_n(5) |&gt;\n  dplyr::select(\n    employee_id,\n    trainings,\n    ed_field,\n    dept\n  )\n\n# A tibble: 5 × 4\n  employee_id trainings ed_field         dept                  \n        &lt;int&gt;     &lt;int&gt; &lt;chr&gt;            &lt;chr&gt;                 \n1        1773         4 Medical          Research & Development\n2        1652         2 Marketing        Sales                 \n3        1999         2 Medical          Research & Development\n4        1548         2 Medical          Research & Development\n5        1698         5 Technical Degree Sales"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#types-of-variables",
    "href": "lectures/01-lecture-slides.html#types-of-variables",
    "title": "Stats Bootcamp",
    "section": "Types of Variables",
    "text": "Types of Variables\n\nQuantitative\nCategorical"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#types-of-quantitative-variables",
    "href": "lectures/01-lecture-slides.html#types-of-quantitative-variables",
    "title": "An Introduction to Statistics & Programming",
    "section": "Types of Quantitative Variables",
    "text": "Types of Quantitative Variables\nQuantitative variables can be further classified into two groups:\n\nDiscrete: Values are distinct, separable numbers (e.g. integers)\nContinuous: Values are on an infinite continuum (e.g. real numbers)"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#types-of-categorical-variables",
    "href": "lectures/01-lecture-slides.html#types-of-categorical-variables",
    "title": "An Introduction to Statistics & Programming",
    "section": "Types of Categorical Variables",
    "text": "Types of Categorical Variables\nSimilar to quantitative variables, categorical variables can also be classified into two groups:\n\nDichotomous / Binary: Two categories\nMulticategorical: Three or more categories"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#roles-of-variables",
    "href": "lectures/01-lecture-slides.html#roles-of-variables",
    "title": "An Introduction to Statistics & Programming",
    "section": "Roles of Variables",
    "text": "Roles of Variables\nVariables can not only be categorized by the kinds and ranges of values they take on, but also by the role they take on in the analysis:\n\nResponse, Outcome, Dependent Variable, Criterion Variable\nPredictor, Independent Variable, Feature, Covariate"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#collecting-data",
    "href": "lectures/01-lecture-slides.html#collecting-data",
    "title": "Stats Bootcamp",
    "section": "Collecting Data",
    "text": "Collecting Data\n\nRandomized Experiment\nQuasi-Experiment\nObservational Studies"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#randomized-experiments",
    "href": "lectures/01-lecture-slides.html#randomized-experiments",
    "title": "Stats Bootcamp",
    "section": "Randomized Experiments",
    "text": "Randomized Experiments"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#quasi-experiments",
    "href": "lectures/01-lecture-slides.html#quasi-experiments",
    "title": "Stats Bootcamp",
    "section": "Quasi-Experiments",
    "text": "Quasi-Experiments"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#observational-studies",
    "href": "lectures/01-lecture-slides.html#observational-studies",
    "title": "An Introduction to Statistics & Programming",
    "section": "Observational Studies",
    "text": "Observational Studies\nIn observational studies, the researchers observe collect a sample of subjects and observe their outcomes across the variables of interest. One type of observational study design is a survey study.\nThe important difference between observational studies and experiments is that subjects are not randomly assigned to treatments."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#what-is-a-statistic",
    "href": "lectures/01-lecture-slides.html#what-is-a-statistic",
    "title": "An Introduction to Statistics & Programming",
    "section": "What is a Statistic?",
    "text": "What is a Statistic?\nStatistics are numbers computed from your data that provide useful numerical information about the characteristics of a variable’s distribution such as its center (mean) or spread (standard deviation).\n\nmean(data_employees_tbl$commute_dist) |&gt; round(2) # Mean\n\n[1] 9.19\n\nmedian(data_employees_tbl$commute_dist) # Median\n\n[1] 7\n\nsd(data_employees_tbl$commute_dist) |&gt; round(2) # SD\n\n[1] 8.11"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#statistic-vs-population-parameter",
    "href": "lectures/01-lecture-slides.html#statistic-vs-population-parameter",
    "title": "An Introduction to Statistics & Programming",
    "section": "Statistic vs Population Parameter",
    "text": "Statistic vs Population Parameter\nPopulation parameters (or parameters) are numerical summaries of our population.\nStatistics are estimates of these parameters calculated from data sampled from this population.\nUsually, we do not have access to our full population of interest, so we sample our data from it and learn about its characteristics (parameters) through the statistics we compute from our sampled data."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#describing-the-center-of-your-data",
    "href": "lectures/01-lecture-slides.html#describing-the-center-of-your-data",
    "title": "An Introduction to Statistics & Programming",
    "section": "Describing the Center of your Data",
    "text": "Describing the Center of your Data\nOne common way to describe your data is to compute a statistic that tells you where the center of your data is—the average or expected value of your data. There are three statistics you can compute:\n\nMean: The average value.\nMedian: The value at which 50% of your data lies below it.\nMode: The most common value."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#describing-the-spread-of-your-data",
    "href": "lectures/01-lecture-slides.html#describing-the-spread-of-your-data",
    "title": "An Introduction to Statistics & Programming",
    "section": "Describing the Spread of your Data",
    "text": "Describing the Spread of your Data\nYou can describe the spread of your data by computing statistics that tells you generally how far the observations are from the mean of your data. There are three statistics you can compute:\n\nVariance: The average squared distance your data falls from the mean.\nStandard Deviation: The average distance your data falls from the mean (square root of variance).\nRange: Maximum value minus the minimum value of your data."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#describing-the-shape-of-your-data",
    "href": "lectures/01-lecture-slides.html#describing-the-shape-of-your-data",
    "title": "An Introduction to Statistics & Programming",
    "section": "Describing the Shape of your Data",
    "text": "Describing the Shape of your Data\nOftentimes, you will also want to talk about the shape of your data. Typically, this is about how skewed or asymmetric your data is in one direction or how heavy the tails of your distribution are:\n\nSkewness: How long the tails of your distribution are in a given direction.\nKurtosis: How heavy the tails of your distribution are."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#describing-the-overall-distribution-of-your-data",
    "href": "lectures/01-lecture-slides.html#describing-the-overall-distribution-of-your-data",
    "title": "Stats Bootcamp",
    "section": "Describing the Overall Distribution of your Data",
    "text": "Describing the Overall Distribution of your Data"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#describing-the-relationship-between-two-variables",
    "href": "lectures/01-lecture-slides.html#describing-the-relationship-between-two-variables",
    "title": "An Introduction to Statistics & Programming",
    "section": "Describing the Relationship between two Variables",
    "text": "Describing the Relationship between two Variables\nWe can also describe the linear relationship between two variables by computing either the covariation or correlation between two variables. Both of these metrics tell us the extent to which two variables are linearly related to one another."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#the-importance-of-plots",
    "href": "lectures/01-lecture-slides.html#the-importance-of-plots",
    "title": "Stats Bootcamp",
    "section": "The Importance of Plots",
    "text": "The Importance of Plots"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#histograms",
    "href": "lectures/01-lecture-slides.html#histograms",
    "title": "Stats Bootcamp",
    "section": "Histograms",
    "text": "Histograms"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#box-plots",
    "href": "lectures/01-lecture-slides.html#box-plots",
    "title": "Stats Bootcamp",
    "section": "Box Plots",
    "text": "Box Plots"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#violin-plots",
    "href": "lectures/01-lecture-slides.html#violin-plots",
    "title": "Stats Bootcamp",
    "section": "Violin Plots",
    "text": "Violin Plots"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#scatter-plots",
    "href": "lectures/01-lecture-slides.html#scatter-plots",
    "title": "Stats Bootcamp",
    "section": "Scatter Plots",
    "text": "Scatter Plots"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#what-is-probability",
    "href": "lectures/01-lecture-slides.html#what-is-probability",
    "title": "An Introduction to Statistics & Programming",
    "section": "What is Probability?",
    "text": "What is Probability?\nProbability is the language of uncertainty.\nAnytime we are dealing with random events such as the outcome of a coin toss or the response to a survey question, we rely on probability to talk about these events."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#probability-as-a-long-run-frequency",
    "href": "lectures/01-lecture-slides.html#probability-as-a-long-run-frequency",
    "title": "An Introduction to Statistics & Programming",
    "section": "Probability as a Long-Run Frequency",
    "text": "Probability as a Long-Run Frequency\n\nFor an observation of a random phenomen, the probability of a particular outcome is the proportion of times that outcome would occur in an indefinitely long sequence of like observations, under the same conditions.\n\n— Agresti & Kateri, 2022"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#sample-space-events",
    "href": "lectures/01-lecture-slides.html#sample-space-events",
    "title": "Quantitative Analysis 1 - Crash Course",
    "section": "Sample Space & Events",
    "text": "Sample Space & Events\n\nprobability-space"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#rules-of-probability-theory",
    "href": "lectures/01-lecture-slides.html#rules-of-probability-theory",
    "title": "Quantitative Analysis 1 - Crash Course",
    "section": "Rules of Probability Theory",
    "text": "Rules of Probability Theory"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#joint-probability",
    "href": "lectures/01-lecture-slides.html#joint-probability",
    "title": "An Introduction to Statistics & Programming",
    "section": "Joint Probability",
    "text": "Joint Probability\nJoint probability is the probability of some event happening for two or more random variables.\nFor example, what is the probability of employment_status == inactive & job_satisfaction == satisfied?"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#conditional-probability",
    "href": "lectures/01-lecture-slides.html#conditional-probability",
    "title": "An Introduction to Statistics & Programming",
    "section": "Conditional Probability",
    "text": "Conditional Probability\nConditional probability is the probability of one event occurring given the occurrence of another event. This is written mathematically as:\n\\[P(\\text{Event 1} \\space | \\space \\text{Event 2})\\]\nwhich is read as the probability of Event 1 conditional on (or given) Event 2."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#random-variables-connecting-probability-statistics-and-data",
    "href": "lectures/01-lecture-slides.html#random-variables-connecting-probability-statistics-and-data",
    "title": "Quantitative Analysis 1 - Crash Course",
    "section": "Random Variables: Connecting Probability, Statistics, and Data",
    "text": "Random Variables: Connecting Probability, Statistics, and Data"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#types-of-random-variables-discrete-continuous",
    "href": "lectures/01-lecture-slides.html#types-of-random-variables-discrete-continuous",
    "title": "An Introduction to Statistics & Programming",
    "section": "Types of Random Variables: Discrete & Continuous",
    "text": "Types of Random Variables: Discrete & Continuous\nRandom variables, like quantitative variables, can be classified into two broad categories:\n\nDiscrete: Separate, distinct outcome values like integers\nContinuous: Infinite continuum of possible outcomes"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#expectation-of-a-random-variable",
    "href": "lectures/01-lecture-slides.html#expectation-of-a-random-variable",
    "title": "Quantitative Analysis 1 - Crash Course",
    "section": "Expectation of a Random Variable",
    "text": "Expectation of a Random Variable"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#connection-between-the-mean-and-expectation",
    "href": "lectures/01-lecture-slides.html#connection-between-the-mean-and-expectation",
    "title": "Quantitative Analysis 1 - Crash Course",
    "section": "Connection Between the Mean and Expectation",
    "text": "Connection Between the Mean and Expectation\n\n1/n @ large n"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#probability-distribution-functions",
    "href": "lectures/01-lecture-slides.html#probability-distribution-functions",
    "title": "Quantitative Analysis 1 - Crash Course",
    "section": "Probability Distribution Functions",
    "text": "Probability Distribution Functions"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#probability-mass-density-functions",
    "href": "lectures/01-lecture-slides.html#probability-mass-density-functions",
    "title": "An Introduction to Statistics & Programming",
    "section": "Probability Mass & Density Functions",
    "text": "Probability Mass & Density Functions\nProbability Mass and Density Functions (PMF & PDF, respectively) are mathematical functions that take the value of a random variable as an input and return the probability of that value occurring as an output. Every statistical model we will use will assume a certain PMF or PDF.\n\nPMF is a probability distribution function for discrete random variables\nPDF is a probability distribution function for continuous random variables"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#cumulative-distribution-function",
    "href": "lectures/01-lecture-slides.html#cumulative-distribution-function",
    "title": "An Introduction to Statistics & Programming",
    "section": "Cumulative Distribution Function",
    "text": "Cumulative Distribution Function\nClosely related to the PMF/PDF, the Cumulative Distribution Function (CDF) specifies the cumulative probability that a random variable takes a value, Y, or any value less than Y:\n\\[F(\\text{Y})=P(\\text{Y} \\leq \\text{y})\\]\nIn our example, what is the probability that involuntary_turnover takes a value of 0? What is the probability involuntary_turnover takes a value less than or equal to 1?"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#joint-probability-distributions",
    "href": "lectures/01-lecture-slides.html#joint-probability-distributions",
    "title": "Stats Bootcamp",
    "section": "Joint Probability Distributions",
    "text": "Joint Probability Distributions"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#conditional-probability-distributions",
    "href": "lectures/01-lecture-slides.html#conditional-probability-distributions",
    "title": "Stats Bootcamp",
    "section": "Conditional Probability Distributions",
    "text": "Conditional Probability Distributions"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#independent-random-variables",
    "href": "lectures/01-lecture-slides.html#independent-random-variables",
    "title": "Quantitative Analysis 1 - Crash Course",
    "section": "Independent Random Variables",
    "text": "Independent Random Variables"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#inferential-statistics-1",
    "href": "lectures/01-lecture-slides.html#inferential-statistics-1",
    "title": "Stats Bootcamp",
    "section": "Inferential Statistics",
    "text": "Inferential Statistics"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#sampling-distributions",
    "href": "lectures/01-lecture-slides.html#sampling-distributions",
    "title": "An Introduction to Statistics & Programming",
    "section": "Sampling Distributions",
    "text": "Sampling Distributions\nImagine you can draw an infinite number of random samples from a population and then for each sample you compute the sample mean. The distribution of these sample means is referred to as the sampling distribution of the mean."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#the-standard-error",
    "href": "lectures/01-lecture-slides.html#the-standard-error",
    "title": "An Introduction to Statistics & Programming",
    "section": "The Standard Error",
    "text": "The Standard Error\nThe standard deviation of a sampling distribution is known by another name: the standard error. The standard error quantifies our uncertainty in a given statistic and is fundamental to inferential statistics.\nFor the mean, the standard error can be calculated as:\n\\[\\sigma_{\\bar{Y}} = \\sqrt{\\frac{\\sigma^{2}}{n}} = \\frac{\\sigma}{\\sqrt{n}}\\]"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#sampling-distribution-of-the-mean",
    "href": "lectures/01-lecture-slides.html#sampling-distribution-of-the-mean",
    "title": "Quantitative Analysis 1 - Crash Course",
    "section": "Sampling Distribution of the Mean",
    "text": "Sampling Distribution of the Mean"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#standard-error-of-the-mean",
    "href": "lectures/01-lecture-slides.html#standard-error-of-the-mean",
    "title": "Quantitative Analysis 1 - Crash Course",
    "section": "Standard Error of the Mean",
    "text": "Standard Error of the Mean"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#central-limit-theorem",
    "href": "lectures/01-lecture-slides.html#central-limit-theorem",
    "title": "An Introduction to Statistics & Programming",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#point-estimates",
    "href": "lectures/01-lecture-slides.html#point-estimates",
    "title": "An Introduction to Programming & Statistics",
    "section": "Point Estimates",
    "text": "Point Estimates"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#law-of-large-numbers",
    "href": "lectures/01-lecture-slides.html#law-of-large-numbers",
    "title": "An Introduction to Programming & Statistics",
    "section": "Law of Large Numbers",
    "text": "Law of Large Numbers"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#estimating-the-mean-variance",
    "href": "lectures/01-lecture-slides.html#estimating-the-mean-variance",
    "title": "An Introduction to Programming & Statistics",
    "section": "Estimating the Mean & Variance",
    "text": "Estimating the Mean & Variance"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#interval-estimates-confidence-interval",
    "href": "lectures/01-lecture-slides.html#interval-estimates-confidence-interval",
    "title": "An Introduction to Programming & Statistics",
    "section": "Interval Estimates (Confidence Interval)",
    "text": "Interval Estimates (Confidence Interval)"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#building-a-confidence-interval-around-the-mean",
    "href": "lectures/01-lecture-slides.html#building-a-confidence-interval-around-the-mean",
    "title": "An Introduction to Programming & Statistics",
    "section": "Building a Confidence Interval Around the Mean",
    "text": "Building a Confidence Interval Around the Mean\nGenerally, to build a confidence interval, you need three pieces of information:\n\nPoint estimate to build the interval around\nThe probability distribution that best approximates the estimate’s sampling distribution (almost always the normal distribution)\nThe Standard Error of the estimate (the standard devation of the sampling distribution)"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#interpreting-a-confidence-interval",
    "href": "lectures/01-lecture-slides.html#interpreting-a-confidence-interval",
    "title": "An Introduction to Statistics & Programming",
    "section": "Interpreting a Confidence Interval",
    "text": "Interpreting a Confidence Interval\nConfidence intervals have a very strict (and kind of odd) interpretation:\nIf you were to randomly sample a large number of samples from a population and create a 95% confidence interval around the sample mean, then 95% of those intervals would contain the population mean."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#what-is-a-statistical-test",
    "href": "lectures/01-lecture-slides.html#what-is-a-statistical-test",
    "title": "An Introduction to Programming & Statistics",
    "section": "What is a statistical test?",
    "text": "What is a statistical test?"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#the-four-elements-of-a-statistical-test",
    "href": "lectures/01-lecture-slides.html#the-four-elements-of-a-statistical-test",
    "title": "An Introduction to Statistics & Programming",
    "section": "The Four Elements of a Statistical Test",
    "text": "The Four Elements of a Statistical Test\n\nAssumptions: Background assumptions that need to hold for our test to be valid.\nHypotheses: The \\(H_{0}\\) and \\(H_{a}\\) hypotheses, which need to be formulated before analyses happen.\nTest Statistic: Summary of how far away a statistical estimate is from the population value predicted by \\(H_{0}\\).\nP-value & Conclusion: A decision on whether to reject or not reject \\(H_{0}\\) if the probability of our data coming from the null population distribution is sufficiently low as measured by a P-value."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#assumptions",
    "href": "lectures/01-lecture-slides.html#assumptions",
    "title": "An Introduction to Programming & Statistics",
    "section": "Assumptions",
    "text": "Assumptions"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#hypotheses",
    "href": "lectures/01-lecture-slides.html#hypotheses",
    "title": "An Introduction to Programming & Statistics",
    "section": "Hypotheses",
    "text": "Hypotheses"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#test-statistic",
    "href": "lectures/01-lecture-slides.html#test-statistic",
    "title": "An Introduction to Programming & Statistics",
    "section": "Test Statistic",
    "text": "Test Statistic"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#p-value-conclusion",
    "href": "lectures/01-lecture-slides.html#p-value-conclusion",
    "title": "An Introduction to Programming & Statistics",
    "section": "P-value & Conclusion",
    "text": "P-value & Conclusion"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#comparing-two-means-t-test",
    "href": "lectures/01-lecture-slides.html#comparing-two-means-t-test",
    "title": "An Introduction to Programming & Statistics",
    "section": "Comparing Two Means (T-Test)",
    "text": "Comparing Two Means (T-Test)"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#assumptions-of-the-t-test",
    "href": "lectures/01-lecture-slides.html#assumptions-of-the-t-test",
    "title": "An Introduction to Programming & Statistics",
    "section": "Assumptions of the T-Test",
    "text": "Assumptions of the T-Test"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#hypotheses-of-the-t-test",
    "href": "lectures/01-lecture-slides.html#hypotheses-of-the-t-test",
    "title": "An Introduction to Programming & Statistics",
    "section": "Hypotheses of the T-Test",
    "text": "Hypotheses of the T-Test"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#t-test-test-statistic",
    "href": "lectures/01-lecture-slides.html#t-test-test-statistic",
    "title": "An Introduction to Programming & Statistics",
    "section": "T-Test Test Statistic",
    "text": "T-Test Test Statistic"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#pooled-standard-deviation",
    "href": "lectures/01-lecture-slides.html#pooled-standard-deviation",
    "title": "Quantitative Analysis 1 - Crash Course",
    "section": "Pooled Standard Deviation",
    "text": "Pooled Standard Deviation"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#standard-error",
    "href": "lectures/01-lecture-slides.html#standard-error",
    "title": "Quantitative Analysis 1 - Crash Course",
    "section": "Standard Error",
    "text": "Standard Error"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#calculating-the-p-value",
    "href": "lectures/01-lecture-slides.html#calculating-the-p-value",
    "title": "Quantitative Analysis 1 - Crash Course",
    "section": "Calculating the P-Value",
    "text": "Calculating the P-Value"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#building-a-confidence-interval",
    "href": "lectures/01-lecture-slides.html#building-a-confidence-interval",
    "title": "An Introduction to Statistics & Programming",
    "section": "Building a Confidence Interval",
    "text": "Building a Confidence Interval\nGenerally, to build a confidence interval, you need three pieces of information:\n\nPoint estimate to build the interval around\nThe probability distribution that best approximates the estimate’s sampling distribution (almost always the normal distribution)\nThe Standard Error of the estimate (the standard deviation of the sampling distribution)\nThe level of “confidence” (i.e. how confident you are that the population parameter is contained in the interval)"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#connection-between-p-values-confidence-intervals",
    "href": "lectures/01-lecture-slides.html#connection-between-p-values-confidence-intervals",
    "title": "Quantitative Analysis 1 - Crash Course",
    "section": "Connection Between P-Values & Confidence Intervals",
    "text": "Connection Between P-Values & Confidence Intervals"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#significance-tests-decision-errors",
    "href": "lectures/01-lecture-slides.html#significance-tests-decision-errors",
    "title": "An Introduction to Programming & Statistics",
    "section": "Significance Tests & Decision Errors",
    "text": "Significance Tests & Decision Errors"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#statistical-significance-vs-practical-significance",
    "href": "lectures/01-lecture-slides.html#statistical-significance-vs-practical-significance",
    "title": "An Introduction to Programming & Statistics",
    "section": "Statistical Significance vs Practical Significance",
    "text": "Statistical Significance vs Practical Significance"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#weaknesses-of-signficance-tests-p-values",
    "href": "lectures/01-lecture-slides.html#weaknesses-of-signficance-tests-p-values",
    "title": "An Introduction to Programming & Statistics",
    "section": "Weaknesses of Signficance Tests & P-Values",
    "text": "Weaknesses of Signficance Tests & P-Values"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#installing-loading-packages",
    "href": "lectures/01-lecture-slides.html#installing-loading-packages",
    "title": "An Introduction to Statistics & Programming",
    "section": "Installing & Loading Packages",
    "text": "Installing & Loading Packages\nYou can use install.packages to install a package once and then library to load that package and gain access to all of its functions.\n\ninstall.packages(\"package_name\")\nlibrary(package_name)"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#selecting-data-from-data-frames",
    "href": "lectures/01-lecture-slides.html#selecting-data-from-data-frames",
    "title": "An Introduction to Statistics & Programming",
    "section": "Selecting Data from Data Frames",
    "text": "Selecting Data from Data Frames\nYou will mainly select data from data frames using one of the two following methods:\n\ndata_frame_1[1, 1] # Index the row and/or column\n\n[1] 1\n\ndata_frame_1[, 1] # Leaving the column or row index blank selects the whole vector\n\n[1] 1 3\n\ndata_frame_1$NUMERIC # Use a $ operator to reference the column name\n\n[1] 1 3"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#na-values-missing-data",
    "href": "lectures/01-lecture-slides.html#na-values-missing-data",
    "title": "An Introduction to Programming & Statistics",
    "section": "NA Values: Missing Data",
    "text": "NA Values: Missing Data\nFor a number of reasons, uou will often having missing data in the datasets you create. R encodes missing data as NA and often interacts differently with vectors that contain NA values compared to vectors that do not contain any NA values.\n\nno_na_value &lt;- c(1, 3, 5)\nmean(no_na_value)\n\n[1] 3\n\nna_value &lt;- c(NA, 3, 5)\nmean(na_value)\n\n[1] NA\n\nmean(na_value, na.rm = TRUE) # Drops NA value from calculation.\n\n[1] 4"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#reading-and-writing-data",
    "href": "lectures/01-lecture-slides.html#reading-and-writing-data",
    "title": "An Introduction to Statistics & Programming",
    "section": "Reading and Writing Data",
    "text": "Reading and Writing Data\nThere are a number of different methods to read and write data into R. The two most common functions are:\n\ndata &lt;- read.csv(\"filepath/file-name.csv\")\n\nwrite.csv(data, \"filepath/file-name.csv\")"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#importing-data-from-an-r-package",
    "href": "lectures/01-lecture-slides.html#importing-data-from-an-r-package",
    "title": "An Introduction to Statistics & Programming",
    "section": "Importing Data from an R Package",
    "text": "Importing Data from an R Package\nOftentimes, R packages will come with their own datasets that we can load into R. The peopleanalytics package has many such datasets that we will use today:\n\ndata_employees &lt;- peopleanalytics::employees"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#installing-packages-from-the-tidyverse",
    "href": "lectures/01-lecture-slides.html#installing-packages-from-the-tidyverse",
    "title": "An Introduction to Statistics & Programming",
    "section": "Installing Packages from the Tidyverse",
    "text": "Installing Packages from the Tidyverse\n\ninstall.packages(\"tidyverse\")"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#the-philosophy-of-dplyr-functions",
    "href": "lectures/01-lecture-slides.html#the-philosophy-of-dplyr-functions",
    "title": "An Introduction to Statistics & Programming",
    "section": "The Philosophy of dplyr Functions",
    "text": "The Philosophy of dplyr Functions\nEvery function in dplyr follows this philosophy:\n\nFirst argument is always a data frame.\nRemaining arguments are usually names of columns on which to operate.\nThe output is always a new data frame (tibble).\n\ndplyr functions are also further grouped by whether they operate on rows, columns, groups, or tables."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#using-dplyr-to-operate-on-rows",
    "href": "lectures/01-lecture-slides.html#using-dplyr-to-operate-on-rows",
    "title": "An Introduction to Statistics & Programming",
    "section": "Using dplyr to Operate on Rows",
    "text": "Using dplyr to Operate on Rows\nThe following dplyr functions can filter, reduce, or reorder the rows of a data frame:\n\ndplyr::filter(data_employees_tbl, job_level %in% c(4, 5))\n\ndplyr::distinct(data_employees_tbl, ed_lvl, ed_field)\n\ndplyr::arrange(data_employees_tbl, work_exp)"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#using-dplyr-to-operate-on-columns",
    "href": "lectures/01-lecture-slides.html#using-dplyr-to-operate-on-columns",
    "title": "An Introduction to Statistics & Programming",
    "section": "Using dplyr to Operate on Columns",
    "text": "Using dplyr to Operate on Columns\nThe following dplyr functions can select, rename, add/change, or relocate the columns of a data frame:\n\ndplyr::select(data_employees_tbl, dept)\n\ndplyr::rename(data_employees_tbl, job_level = job_lvl)\n\ndplyr::mutate(data_employees_tbl, salary = monthly_comp * 12)\n\ndplyr::relocate(data_employees_tbl, job_lvl, .before = employee_id)"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#using-dplyr-to-operate-on-groups",
    "href": "lectures/01-lecture-slides.html#using-dplyr-to-operate-on-groups",
    "title": "An Introduction to Statistics & Programming",
    "section": "Using dplyr to Operate on Groups",
    "text": "Using dplyr to Operate on Groups\nThe following dplyr functions can group and summarize your data by a predefined group indicator:\n\ndata_employees_tbl |&gt;\n  dplyr::group_by(\n    job_lvl\n  ) |&gt;\n  dplyr::summarize(\n    annual_comp_mean = mean(annual_comp),\n    annual_comp_median = median(annual_comp)\n  )\n\nIn this code chunk, we have grouped by an employee’s job level and summarized their annual salary by job level."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#using-dplyr-to-operate-on-tables",
    "href": "lectures/01-lecture-slides.html#using-dplyr-to-operate-on-tables",
    "title": "An Introduction to Statistics & Programming",
    "section": "Using dplyr to Operate on Tables",
    "text": "Using dplyr to Operate on Tables\nThe followingdplyr functions can be used to join different tables (data frames) together by a unique identifier:\n\ndata_job &lt;- peopleanalytics::job |&gt; tibble::as_tibble()\n\ndata_payroll &lt;- peopleanalytics::payroll |&gt; tibble::as_tibble()\n\ndata_job_payroll &lt;- \n  data_job |&gt;\n  dplyr::left_join(\n    data_payroll,\n    by = \"employee_id\"\n  )"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#description",
    "href": "lectures/01-lecture-slides.html#description",
    "title": "An Introduction to Statistics & Programming",
    "section": "Description",
    "text": "Description\nDescription focuses on how to summarize the raw data without losing too much information. Descriptive statistics are statistics calculated from the raw data that summarize all (or most) of the information contained in the data:\n\nMean, median, and mode\nVariance and standard deviation\nCumulative distribution of the data"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#inference",
    "href": "lectures/01-lecture-slides.html#inference",
    "title": "An Introduction to Statistics & Programming",
    "section": "Inference",
    "text": "Inference\nInference focuses on how to make evaluations (generalizations) from the data that take into consideration the uncertainty present in the data. These data-based evaluations take the form of:\n\nPredictions\nInterval and point estimates\nProbability (P) values"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#populations-samples",
    "href": "lectures/01-lecture-slides.html#populations-samples",
    "title": "An Introduction to Statistics & Programming",
    "section": "Populations & Samples",
    "text": "Populations & Samples\nThe purpose of most analyses is to learn something about the population from the collected data or sample.\n\nA population is the collection of every unit or subject (e.g. person) that one wishes to generalize to from the results of their study.\nA sample is the actual collected data one is using to make these generalizations."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#types-of-variables-by-measurement-scale",
    "href": "lectures/01-lecture-slides.html#types-of-variables-by-measurement-scale",
    "title": "An Introduction to Statistics & Programming",
    "section": "Types of Variables by Measurement Scale",
    "text": "Types of Variables by Measurement Scale\nWe can classify variables into two broad categories based on their measurement scale–the types of values the variable can take on:\n\nQuantitative: Values are numbers\nCategorical: Values are categories"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#types-of-variables-by-measurement-scale-1",
    "href": "lectures/01-lecture-slides.html#types-of-variables-by-measurement-scale-1",
    "title": "An Introduction to Statistics & Programming",
    "section": "Types of Variables by Measurement Scale",
    "text": "Types of Variables by Measurement Scale\n\nset.seed(4)\ndata_employees_tbl |&gt; \n  dplyr::sample_n(5) |&gt;\n  dplyr::select(\n    job_tenure,\n    dept\n  )\n\n# A tibble: 5 × 2\n  job_tenure dept                  \n       &lt;int&gt; &lt;chr&gt;                 \n1          0 Research & Development\n2          2 Sales                 \n3          3 Sales                 \n4          0 Sales                 \n5          0 Sales"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#types-of-quantitative-variables-1",
    "href": "lectures/01-lecture-slides.html#types-of-quantitative-variables-1",
    "title": "An Introduction to Statistics & Programming",
    "section": "Types of Quantitative Variables",
    "text": "Types of Quantitative Variables\n\nset.seed(4)\ndata_employees_tbl |&gt;\n  dplyr::sample_n(5) |&gt;\n  dplyr::select(\n    trainings,\n    commute_dist\n  )\n\n# A tibble: 5 × 2\n  trainings commute_dist\n      &lt;int&gt;        &lt;int&gt;\n1         4            9\n2         4           20\n3         2            1\n4         2           19\n5         0           12"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#types-of-categorical-variables-1",
    "href": "lectures/01-lecture-slides.html#types-of-categorical-variables-1",
    "title": "An Introduction to Statistics & Programming",
    "section": "Types of Categorical Variables",
    "text": "Types of Categorical Variables\n\nset.seed(4)\ndata_employees_tbl |&gt;\n  dplyr::sample_n(5) |&gt;\n  dplyr::select(\n    overtime,\n    ed_field\n  )\n\n# A tibble: 5 × 2\n  overtime ed_field     \n  &lt;chr&gt;    &lt;chr&gt;        \n1 No       Life Sciences\n2 No       Life Sciences\n3 No       Life Sciences\n4 Yes      Marketing    \n5 No       Life Sciences"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#data-collection",
    "href": "lectures/01-lecture-slides.html#data-collection",
    "title": "An Introduction to Statistics & Programming",
    "section": "Data Collection",
    "text": "Data Collection\nThe strength of the inferences you can make depends on the quality of your data. The quality of your data is very dependent on the method used to collect it:\n\nExperiments\nObservational Studies"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#experiments",
    "href": "lectures/01-lecture-slides.html#experiments",
    "title": "An Introduction to Statistics & Programming",
    "section": "Experiments",
    "text": "Experiments\nIn experiments—also known as randomized control trials (RCTs)—data are collected by randomly assigning subjects to an experimental trial or condition, then collecting the subsequent outcome data.\nBy randomly assigning subjects to conditions, you are effectively ensuring that any differences in the outcome variable by condition is due solely to the condition not to any other lurking variable."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#statistic-vs-population-parameter-1",
    "href": "lectures/01-lecture-slides.html#statistic-vs-population-parameter-1",
    "title": "An Introduction to Statistics & Programming",
    "section": "Statistic vs Population Parameter",
    "text": "Statistic vs Population Parameter"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#a-reassuring-reminder",
    "href": "lectures/01-lecture-slides.html#a-reassuring-reminder",
    "title": "An Introduction to Statistics & Programming",
    "section": "A Reassuring Reminder",
    "text": "A Reassuring Reminder\n\nStatistics is hard, especially when effects are small and variable and measurements are noisy.\n\n— McShane et al. (2019)"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#describing-the-center-of-your-data-1",
    "href": "lectures/01-lecture-slides.html#describing-the-center-of-your-data-1",
    "title": "An Introduction to Statistics & Programming",
    "section": "Describing the Center of your Data",
    "text": "Describing the Center of your Data"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#describing-the-center-of-your-data-mean",
    "href": "lectures/01-lecture-slides.html#describing-the-center-of-your-data-mean",
    "title": "Stats Bootcamp",
    "section": "Describing the Center of your Data: Mean",
    "text": "Describing the Center of your Data: Mean\n\\(\\frac{\\Sigma{y_i}}{n}\\)"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#describing-the-spread-of-your-data-1",
    "href": "lectures/01-lecture-slides.html#describing-the-spread-of-your-data-1",
    "title": "An Introduction to Statistics & Programming",
    "section": "Describing the Spread of your Data",
    "text": "Describing the Spread of your Data"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#describing-the-shape-of-your-data-1",
    "href": "lectures/01-lecture-slides.html#describing-the-shape-of-your-data-1",
    "title": "An Introduction to Statistics & Programming",
    "section": "Describing the Shape of your Data",
    "text": "Describing the Shape of your Data"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#using-r-to-describe-your-data",
    "href": "lectures/01-lecture-slides.html#using-r-to-describe-your-data",
    "title": "Stats Bootcamp",
    "section": "Using R to Describe your Data",
    "text": "Using R to Describe your Data"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#correlation-coefficient",
    "href": "lectures/01-lecture-slides.html#correlation-coefficient",
    "title": "An Introduction to Statistics & Programming",
    "section": "Correlation Coefficient",
    "text": "Correlation Coefficient\nThe correlation coefficient is just a standardized version of the covariance statistic with values that range from -1 to 1.\n\nPositive Correlation: High (low) values of one variable, X, are frequently seen with high (low) values of another variable.\nNegative Correlation: High (low) values of one variable, X, are frequently seen with low (high) values of another variable.\n\n\nset.seed(324)\nx &lt;- rnorm(100, sd = 50)\ny &lt;- 1 * x + rnorm(100, sd = 100)\n\ncov(x, y) |&gt; round(2)\n\n[1] 2522.9\n\ncor(x, y) |&gt; round(2)\n\n[1] 0.45"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#plotting-the-relationship-between-two-variables",
    "href": "lectures/01-lecture-slides.html#plotting-the-relationship-between-two-variables",
    "title": "An Introduction to Statistics & Programming",
    "section": "Plotting the Relationship Between two Variables",
    "text": "Plotting the Relationship Between two Variables"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#importance-of-plots",
    "href": "lectures/01-lecture-slides.html#importance-of-plots",
    "title": "An Introduction to Statistics & Programming",
    "section": "Importance of Plots",
    "text": "Importance of Plots\nPlotting your data immediately gives you more information than looking at the raw numbers:\n\nVisual information about the center, spread, and shape of your data.\nAlert you to outlier values."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#thinking-in-disributions",
    "href": "lectures/01-lecture-slides.html#thinking-in-disributions",
    "title": "An Introduction to Statistics & Programming",
    "section": "Thinking in Disributions",
    "text": "Thinking in Disributions\nThe distribution of a given variable gives the frequency of each value of the variable. This frequency can be in either:\n\nAbsolute terms: Count of observations\nRelative terms: Proportion or percent of observations\n\nA variable’s distribution completely describes the variable."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#examples-of-plots",
    "href": "lectures/01-lecture-slides.html#examples-of-plots",
    "title": "An Introduction to Statistics & Programming",
    "section": "Examples of Plots",
    "text": "Examples of Plots\n\nset.seed(2311)\nx &lt;- rnorm(1000)\ndata &lt;- tibble::tibble(x = x)"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#probability-of-a-coin-flip",
    "href": "lectures/01-lecture-slides.html#probability-of-a-coin-flip",
    "title": "Quantitative Analysis 1 - Crash Course",
    "section": "Probability of a Coin Flip",
    "text": "Probability of a Coin Flip\nYou flip a coin 100 times. Of those 100 flips, 78 were heads and 22 were tails, so we can (not so safely) say that the probability of this coin coming up heads is 78%."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#the-three-rules-of-probability",
    "href": "lectures/01-lecture-slides.html#the-three-rules-of-probability",
    "title": "An Introduction to Statistics & Programming",
    "section": "The Three Rules of Probability",
    "text": "The Three Rules of Probability\nAll of probability theory rests on three rules:\n\n\\(P(\\text{Event}) \\geq 0\\)\n\\(P(\\text{Any Event}) = 1\\)\nIf two events are mutually exclusive, then the probability of event one or event two happening is equal to \\(P(\\text{Event 1}) + P(\\text{Event 2})\\)"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#a-concrete-example",
    "href": "lectures/01-lecture-slides.html#a-concrete-example",
    "title": "An Introduction to Statistics & Programming",
    "section": "A Concrete Example",
    "text": "A Concrete Example\nYou’re a Human Capital Analytics researcher at a large, multinational organization and you have access to all of the firm’s HR data over the past fiscal year, which includes three key variables: voluntary_turnover, job_satisfacation, and office_region.\nYour manager asks you to determine the likelihood that an employee leaves the firm. How do you approach this project?"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#probability-of-voluntary-turnover",
    "href": "lectures/01-lecture-slides.html#probability-of-voluntary-turnover",
    "title": "An Introduction to Statistics & Programming",
    "section": "Probability of Voluntary Turnover",
    "text": "Probability of Voluntary Turnover\nA quick way to determine the probability of voluntary turnover is to look at the proportion of employees who left the firm in the last year. This proportion is .16, so the \\(P(\\text{Status = Inactive})=.16\\)."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#a-concrete-example-1",
    "href": "lectures/01-lecture-slides.html#a-concrete-example-1",
    "title": "An Introduction to Statistics & Programming",
    "section": "A Concrete Example",
    "text": "A Concrete Example\n\n\n# A tibble: 20,000 × 3\n   voluntary_turnover job_satisfaction office_region\n   &lt;chr&gt;              &lt;fct&gt;            &lt;fct&gt;        \n 1 active             Neutral          China        \n 2 active             Satisfied        Latin Am.    \n 3 active             Satisfied        North America\n 4 inactive           Neutral          North America\n 5 active             Neutral          China        \n 6 active             Neutral          China        \n 7 inactive           Neutral          Latin Am.    \n 8 active             Dissatisfied     Europe       \n 9 active             Neutral          Europe       \n10 active             Dissatisfied     Europe       \n# ℹ 19,990 more rows"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#employment-status-job-satisfaction-joint-probability",
    "href": "lectures/01-lecture-slides.html#employment-status-job-satisfaction-joint-probability",
    "title": "An Introduction to Statistics & Programming",
    "section": "Employment Status & Job Satisfaction: Joint Probability",
    "text": "Employment Status & Job Satisfaction: Joint Probability\n\n\n\n\n\n\nactive\ninactive\n\n\n\n\nDissatisfied\n0.06\n0.04\n\n\nNeutral\n0.50\n0.10\n\n\nSatisfied\n0.28\n0.02"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#employment-status-job-satisfaction-joint-probability-1",
    "href": "lectures/01-lecture-slides.html#employment-status-job-satisfaction-joint-probability-1",
    "title": "Quantitative Analysis 1 - Crash Course",
    "section": "Employment Status & Job Satisfaction: Joint Probability",
    "text": "Employment Status & Job Satisfaction: Joint Probability\n\n\n\n\n\n\nactive\ninactive\n\n\n\n\nDissatisfied\n0.06\n0.04\n\n\nNeutral\n0.50\n0.10\n\n\nSatisfied\n0.28\n0.02"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#emplyment-status-given-job-satisfaction-conditional-probability",
    "href": "lectures/01-lecture-slides.html#emplyment-status-given-job-satisfaction-conditional-probability",
    "title": "Quantitative Analysis 1 - Crash Course",
    "section": "Emplyment Status Given Job Satisfaction: Conditional Probability",
    "text": "Emplyment Status Given Job Satisfaction: Conditional Probability\nConditional probability is the probability of one event occurring given the occurence of another event. This is written mathematically as:\n\\[P(\\text{Event 1} \\space | \\space \\text{Event 2})\\]\nwhich is read as the probability of Event 1 conditional on (or given) Event 2."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#employment-status-given-job-satisfaction-conditional-probability",
    "href": "lectures/01-lecture-slides.html#employment-status-given-job-satisfaction-conditional-probability",
    "title": "An Introduction to Statistics & Programming",
    "section": "Employment Status Given Job Satisfaction: Conditional Probability",
    "text": "Employment Status Given Job Satisfaction: Conditional Probability\nWhat is the probability that an employee’s status is inactive given that they had responded they were dissatisfied with their job on an an earlier attitude survey? What happens to this probability as job satisfaction moves from dissatisfied to satisfied?\n\n\n\n\n\n\nactive\ninactive\n\n\n\n\nDissatisfied\n0.63\n0.37\n\n\nNeutral\n0.84\n0.16\n\n\nSatisfied\n0.93\n0.07"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#employment-status-given-job-satisfaction-conditional-probability-1",
    "href": "lectures/01-lecture-slides.html#employment-status-given-job-satisfaction-conditional-probability-1",
    "title": "Quantitative Analysis 1 - Crash Course",
    "section": "Employment Status Given Job Satisfaction: Conditional Probability",
    "text": "Employment Status Given Job Satisfaction: Conditional Probability\nWhat is the probability that an employee’s status is inactive given that they had responded they were disstaisfied with their job on an an earlier attitude survey? What happens to this probability as job satisfaction moves from dissatisfied to satisfied?\n\n\n\n\n\n\nactive\ninactive\n\n\n\n\nDissatisfied\n0.63\n0.37\n\n\nNeutral\n0.84\n0.16\n\n\nSatisfied\n0.93\n0.07"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#independent-probabilities",
    "href": "lectures/01-lecture-slides.html#independent-probabilities",
    "title": "Quantitative Analysis 1 - Crash Course",
    "section": "Independent Probabilities",
    "text": "Independent Probabilities\nTwo events are said to be independent if the probability of one event occurring does not change given the occurence of the other event:\n\\[P(\\text{Event 1} \\space | \\space \\text{Event 2})=P(\\text{Event 1)}\\]"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#office-region-job-satisfaction-independent-probabilities",
    "href": "lectures/01-lecture-slides.html#office-region-job-satisfaction-independent-probabilities",
    "title": "Quantitative Analysis 1 - Crash Course",
    "section": "Office Region & Job Satisfaction: Independent Probabilities",
    "text": "Office Region & Job Satisfaction: Independent Probabilities\nHow does the probability of an employee’s job satisfaction response change depending on the region they’re working in? Or does it change?\n\n\n\n\n\n\nDissatisfied\nNeutral\nSatisfied\n\n\n\n\nNorth America\n0.10\n0.59\n0.31\n\n\nAsia\n0.11\n0.62\n0.27\n\n\nChina\n0.11\n0.59\n0.30\n\n\nEEMEA\n0.10\n0.59\n0.31\n\n\nEurope\n0.09\n0.61\n0.29\n\n\nLatin Am.\n0.10\n0.60\n0.29"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#independent-events",
    "href": "lectures/01-lecture-slides.html#independent-events",
    "title": "An Introduction to Statistics & Programming",
    "section": "Independent Events",
    "text": "Independent Events\nTwo events are said to be independent if the probability of one event occurring does not change given the occurrence of the other event:\n\\[P(\\text{Event 1} \\space | \\space \\text{Event 2})=P(\\text{Event 1)}\\]"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#office-region-job-satisfaction-independent-events",
    "href": "lectures/01-lecture-slides.html#office-region-job-satisfaction-independent-events",
    "title": "An Introduction to Statistics & Programming",
    "section": "Office Region & Job Satisfaction: Independent Events",
    "text": "Office Region & Job Satisfaction: Independent Events\nHow does the probability of an employee’s job satisfaction response change depending on the region they’re working in? Or does it change?\n\n\n\n\n\n\nDissatisfied\nNeutral\nSatisfied\n\n\n\n\nNorth America\n0.10\n0.59\n0.31\n\n\nAsia\n0.11\n0.62\n0.27\n\n\nChina\n0.11\n0.59\n0.30\n\n\nEEMEA\n0.10\n0.59\n0.31\n\n\nEurope\n0.09\n0.61\n0.29\n\n\nLatin Am.\n0.10\n0.60\n0.29"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#random-variables",
    "href": "lectures/01-lecture-slides.html#random-variables",
    "title": "An Introduction to Statistics & Programming",
    "section": "Random Variables",
    "text": "Random Variables\nWhether you realize it or not, we have been talking about voluntary_turnover as a random variable.\nA random variable is a function of a random phenomenon that maps an outcome of that phenomenon to a real number.\nIn our example, voluntary_turnover is a random variable that maps the outcome of an employee’s decision to leave or remain with their organization to a real number: 1 or 0."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#voluntary-turnover-as-a-random-variable",
    "href": "lectures/01-lecture-slides.html#voluntary-turnover-as-a-random-variable",
    "title": "An Introduction to Statistics & Programming",
    "section": "Voluntary Turnover as a Random Variable",
    "text": "Voluntary Turnover as a Random Variable\nAs a random variable, voluntary_turnover maps inactive to 1 and active to 0:\n\\[\\text{Y}(\\text{inactive})=1 \\\\ \\text{Y}(\\text{active})=0\\]\nBecause the random variable is a function of a random phenomenon, we can still calculate probabilities for the outcome:\n\\[P(\\text{Y}=1)=.16 \\\\ P(\\text{Y}=0) = .84\\]"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#connecting-probability-to-statistics-with-random-variables",
    "href": "lectures/01-lecture-slides.html#connecting-probability-to-statistics-with-random-variables",
    "title": "An Introduction to Statistics & Programming",
    "section": "Connecting Probability to Statistics with Random Variables",
    "text": "Connecting Probability to Statistics with Random Variables\nThe big gain from introducing random variables is that we can now apply mathematical and statistical models to the numerical values, and we can use more general probability distributions to describe the distributions of these random variables.\nFor instance, we can say voluntary_turnover can be modeled using a binomial distribution."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#probability-distributions",
    "href": "lectures/01-lecture-slides.html#probability-distributions",
    "title": "An Introduction to Statistics & Programming",
    "section": "Probability Distributions",
    "text": "Probability Distributions\nProbability distributions are mathematical models that can be used to summarize the random variation in the random variables by specifying the probabilities of all possible outcomes of the random variable."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#modeling-voluntary-turnover-with-a-binomial-distribution",
    "href": "lectures/01-lecture-slides.html#modeling-voluntary-turnover-with-a-binomial-distribution",
    "title": "An Introduction to Statistics & Programming",
    "section": "Modeling Voluntary Turnover with a Binomial Distribution",
    "text": "Modeling Voluntary Turnover with a Binomial Distribution\nThe Bernoulli distribution is a probability distribution that can be used to model a random variable that has two outcomes. It specifies the probability of the first outcome, 1, as p and the second outcome, as 1 - p:\n\\[\\begin{equation}\n    f(\\text{Employment Status};p) =\n    \\left\\{\n        \\begin{array}{cc}\n                p & \\mathrm{if\\ } status=1 \\\\\n                1-p & \\mathrm{if\\ } status=0 \\\\\n        \\end{array}\n    \\right.\n\\end{equation}\\]"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#plotting-pmfs-and-pdfs",
    "href": "lectures/01-lecture-slides.html#plotting-pmfs-and-pdfs",
    "title": "An Introduction to Statistics & Programming",
    "section": "Plotting PMFs and PDFs",
    "text": "Plotting PMFs and PDFs"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#cumulative-distribution-function-1",
    "href": "lectures/01-lecture-slides.html#cumulative-distribution-function-1",
    "title": "An Introduction to Statistics & Programming",
    "section": "Cumulative Distribution Function",
    "text": "Cumulative Distribution Function"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#example-recruiters-trust-in-ai",
    "href": "lectures/01-lecture-slides.html#example-recruiters-trust-in-ai",
    "title": "An Introduction to Statistics & Programming",
    "section": "Example: Recruiters’ trust in AI",
    "text": "Example: Recruiters’ trust in AI\nYour organization is considering adopting an AI automated resume scraper program to lessen the burden on the recruiters. Before committing to the tool, however, your manager has asked you to determine if the recruiters would trust the outcomes provided by the AI system.\nTo assess this, you administer a single survey question to three random samples of 50 recruiters:\n\nI trust the outcome provided by an artificially intelligent system."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#population-sample-response-distributions",
    "href": "lectures/01-lecture-slides.html#population-sample-response-distributions",
    "title": "Quantitative Analysis 1 - Crash Course",
    "section": "Population & Sample Response Distributions",
    "text": "Population & Sample Response Distributions"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#example-do-recruiters-trust-ai",
    "href": "lectures/01-lecture-slides.html#example-do-recruiters-trust-ai",
    "title": "An Introduction to Statistics & Programming",
    "section": "Example: Do Recruiters’ trust AI?",
    "text": "Example: Do Recruiters’ trust AI?\nTo determine how recruiters feel about AI, on average, you compute the mean of each sample and find the following:\n\n\n\n\n\nsample_1\nsample_2\nsample_3\n\n\n\n\n3.74\n3.76\n3.82\n\n\n\n\n\nThe average response is different across the three different samples. Is this expected? What should you do?"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#populations-sample-variation",
    "href": "lectures/01-lecture-slides.html#populations-sample-variation",
    "title": "An Introduction to Statistics & Programming",
    "section": "Populations & Sample Variation",
    "text": "Populations & Sample Variation"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#statistics-as-random-variables",
    "href": "lectures/01-lecture-slides.html#statistics-as-random-variables",
    "title": "An Introduction to Statistics & Programming",
    "section": "Statistics as Random Variables",
    "text": "Statistics as Random Variables\nBecause statistics like the sample mean are computed from a sample that contains random variation, we expect our statistics to behave like random variables.\nLike a random variable, we can specify a probability distribution for our statistic called a sampling distribution."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#the-mean-standard-deviation-of-a-sampling-distribution",
    "href": "lectures/01-lecture-slides.html#the-mean-standard-deviation-of-a-sampling-distribution",
    "title": "An Introduction to Statistics & Programming",
    "section": "The Mean & Standard Deviation of a Sampling Distribution",
    "text": "The Mean & Standard Deviation of a Sampling Distribution\nLike all distributions, we can compute the mean and standard deviation of a sampling distribution and obtain useful information:\n\nMean of a sampling distribution = Population Parameter\nStandard deviation of a sampling distribution = Uncertainty in our statistic"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#the-standard-error-1",
    "href": "lectures/01-lecture-slides.html#the-standard-error-1",
    "title": "An Introduction to Statistics & Programming",
    "section": "The Standard Error",
    "text": "The Standard Error\nWe can reduce the standard error, thereby reducing our uncertainty, by increasing our sample size:"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#the-norm-of-normality-the-central-limit-theorem",
    "href": "lectures/01-lecture-slides.html#the-norm-of-normality-the-central-limit-theorem",
    "title": "An Introduction to Statistics & Programming",
    "section": "The Norm of Normality: The Central Limit Theorem",
    "text": "The Norm of Normality: The Central Limit Theorem\nThe Central Limit Theorem (CLT) is a mathematical finding that tells us that the sampling distribution of a statistic like the mean starts to closely resemble a normal distribution as the sample size increases regardless of the distribution of the sample data itself!\nThe CLT plays a very important role in all of our statistical inference!"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#what-is-statistical-estimation",
    "href": "lectures/01-lecture-slides.html#what-is-statistical-estimation",
    "title": "An Introduction to Statistics & Programming",
    "section": "What is Statistical Estimation?",
    "text": "What is Statistical Estimation?\nThe goal of every data analytic project is to estimate some population parameter by computing some statistic or point estimate. This is statistical estimation."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#point-estimates-interval-estimates",
    "href": "lectures/01-lecture-slides.html#point-estimates-interval-estimates",
    "title": "An Introduction to Statistics & Programming",
    "section": "Point Estimates & Interval Estimates",
    "text": "Point Estimates & Interval Estimates\nStatistical estimates can either come as point estimates or interval estimates:\n\nPoint Estimate: A single value that estimates the population parameter.\nInterval Estimate: An interval of values centered around the point estimate."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#differences-in-recruiters-trust-in-ai-by-job-experience",
    "href": "lectures/01-lecture-slides.html#differences-in-recruiters-trust-in-ai-by-job-experience",
    "title": "An Introduction to Statistics & Programming",
    "section": "Differences in Recruiters’ trust in AI by Job Experience",
    "text": "Differences in Recruiters’ trust in AI by Job Experience\nYour manager asks you to administer the survey one more time to a larger sample of 300 recruiters, but this time they would like you to measure the amount of years the employee has been in the recruiting industry as a proxy for job experience job_exp.\nYour manager would like to know if employees’ trust differs based on their level of job experience. How should you approach this project?"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#plotting-your-data",
    "href": "lectures/01-lecture-slides.html#plotting-your-data",
    "title": "An Introduction to Statistics & Programming",
    "section": "Plotting your Data",
    "text": "Plotting your Data"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#estimating-the-mean-variance-by-job-experience",
    "href": "lectures/01-lecture-slides.html#estimating-the-mean-variance-by-job-experience",
    "title": "An Introduction to Statistics & Programming",
    "section": "Estimating the Mean & Variance by Job Experience",
    "text": "Estimating the Mean & Variance by Job Experience\nFor each job experience group, we can estimate the population means by computing the sample mean and standard deviation of their responses to the trust in AI question:\n\n\n\n\n\nJob Exp.\nMean Trust\nSD Trust\nN\n\n\n\n\nLow\n4.33\n0.90\n100\n\n\nMedium\n4.28\n0.78\n100\n\n\nHigh\n2.94\n1.02\n100"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#a-confidence-interval-estimate-for-the-mean",
    "href": "lectures/01-lecture-slides.html#a-confidence-interval-estimate-for-the-mean",
    "title": "An Introduction to Statistics & Programming",
    "section": "A Confidence (Interval) Estimate for the Mean",
    "text": "A Confidence (Interval) Estimate for the Mean\nBecause there is uncertainty in our data, we would like to move away from providing a single estimate of trust in AI for each group and provide an interval of estimates that adequately quantifies the uncertainty we have in our estimate:\n\n\n\n\n\nJob Exp.\nMean Trust\n95% Conf. Int.\nSE\nSD Trust\nN\n\n\n\n\nLow\n4.33\n4.15 - 4.51\n0.090\n0.90\n100\n\n\nMedium\n4.28\n4.13 - 4.43\n0.078\n0.78\n100\n\n\nHigh\n2.94\n2.74 - 3.14\n0.102\n1.02\n100"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#the-cdf-of-the-normal-distribution",
    "href": "lectures/01-lecture-slides.html#the-cdf-of-the-normal-distribution",
    "title": "An Introduction to Statistics & Programming",
    "section": "The CDF of the Normal Distribution",
    "text": "The CDF of the Normal Distribution\nThink about the CDF as a way to compute the percentiles of a distribution. What is the 50th percentile—the value where 50% or less of the observations fall—for the Normal CDF below?"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#a-99-confidence-interval",
    "href": "lectures/01-lecture-slides.html#a-99-confidence-interval",
    "title": "An Introduction to Programming & Statistics",
    "section": "A 99% Confidence Interval",
    "text": "A 99% Confidence Interval"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#getting-to-know-the-normal-distribution-better",
    "href": "lectures/01-lecture-slides.html#getting-to-know-the-normal-distribution-better",
    "title": "An Introduction to Statistics & Programming",
    "section": "Getting to Know the Normal Distribution Better",
    "text": "Getting to Know the Normal Distribution Better\n\nIt has two parameters: Mean & Variance.\n68% of its mass is between \\(\\pm1\\) SDs from its mean, 95% of its mass is between \\(\\pm2\\) SDs from its mean, and 99.7% of its mass is between \\(\\pm3\\) SDs from its mean."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#building-a-confidence-interval-1",
    "href": "lectures/01-lecture-slides.html#building-a-confidence-interval-1",
    "title": "An Introduction to Statistics & Programming",
    "section": "Building a Confidence Interval",
    "text": "Building a Confidence Interval\n\\[\\overline{Y} \\space \\pm \\space 1.96 \\times\\text{SE}\\]\n\n\\(\\overline{Y}\\): Point estimate (sample mean)\n\\(\\pm 1.96\\) is the value at which 95% of the mass of the standard normal distribution falls\nSE: The standard error of the estimate"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#are-the-differences-in-recruiters-trust-in-ai-by-job-experience-real",
    "href": "lectures/01-lecture-slides.html#are-the-differences-in-recruiters-trust-in-ai-by-job-experience-real",
    "title": "An Introduction to Statistics & Programming",
    "section": "Are the Differences in Recruiters’ trust in AI by Job Experience Real?",
    "text": "Are the Differences in Recruiters’ trust in AI by Job Experience Real?\nWe saw that the sample means of trust in AI differed by job experience level, but are those differences real or are they a result of random noise (sample variation)?\nWe can use a statistical test to determine if the difference we see in the sample means is indicative of true population-level differences."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#stating-a-statistical-hypothesis",
    "href": "lectures/01-lecture-slides.html#stating-a-statistical-hypothesis",
    "title": "An Introduction to Statistics & Programming",
    "section": "Stating a Statistical Hypothesis",
    "text": "Stating a Statistical Hypothesis\nIn statistics, a hypothesis is a statement about the population distribution. Researchers typically formulate two kinds of hypotheses: null hypothesis (\\(H_{0}\\)) and alternative (researcher’s) hypothesis (\\(H_{a}\\)).\n\n\\(H_{0}\\) is a statement that the population parameter takes on some value—usually 0.\n\\(H_{a}\\) is a statement that the population parameters takes on an alternative set of values that fit with the researcher’s theory."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#trust-in-ai-hypotheses",
    "href": "lectures/01-lecture-slides.html#trust-in-ai-hypotheses",
    "title": "An Introduction to Statistics & Programming",
    "section": "Trust in AI Hypotheses",
    "text": "Trust in AI Hypotheses\n\\(H_{0}\\): In this organization, there are no differences between mean-level trust in AI for employees with low job experience and mean-level trust in AI for employees with either medium or high job experience.\n\\(H_{a}\\): In this organization, the mean-level trust in AI for employees with low job experience is higher than the mean-level trust in AI for employees with medium job experience and high experience."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#what-is-a-statistical-significance-test",
    "href": "lectures/01-lecture-slides.html#what-is-a-statistical-significance-test",
    "title": "An Introduction to Statistics & Programming",
    "section": "What is a Statistical Significance Test?",
    "text": "What is a Statistical Significance Test?\nA statistical significance test, or just test, uses data to summarize the evidence about a hypothesis, usually the null, by comparing a point estimate of the parameter of interest (e.g. sample mean) to the value predicted by the hypothesis (e.g. 0 in the case of the null hypothesis)."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#a-significance-test-for-trust-in-ai",
    "href": "lectures/01-lecture-slides.html#a-significance-test-for-trust-in-ai",
    "title": "An Introduction to Statistics & Programming",
    "section": "A Significance Test for Trust in AI",
    "text": "A Significance Test for Trust in AI\nTo determine if trust in AI differs by job experience, we are going to use a z-test to test our two null hypotheses, which can be framed as hypotheses about the mean differences between trust in AI by job experience:\n\n\n\n\\(H_{o}\\):\n\\(\\mu_{\\text{AI Low Exp.}} - \\mu_{\\text{AI Med. Exp.}} = 0\\) \\(\\mu_{\\text{AI Low Exp.}} - \\mu_{\\text{AI Hifg Exp.}} = 0\\)\n\n\\(H_{a}\\):\n\\(\\mu_{\\text{AI Low Exp.}} - \\mu_{\\text{AI Med. Exp.}} &gt; 0\\) \\(\\mu_{\\text{AI Low Exp.}} - \\mu_{\\text{AI Hifg Exp.}} &gt; 0\\)"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#understanding-a-t-test",
    "href": "lectures/01-lecture-slides.html#understanding-a-t-test",
    "title": "An Introduction to Programming & Statistics",
    "section": "Understanding a T-Test",
    "text": "Understanding a T-Test"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#differnce-in-trust-in-ai-means-by-job-experience",
    "href": "lectures/01-lecture-slides.html#differnce-in-trust-in-ai-means-by-job-experience",
    "title": "An Introduction to Programming & Statistics",
    "section": "Differnce in Trust in AI Means by Job Experience",
    "text": "Differnce in Trust in AI Means by Job Experience\n\n\n\n\n\nComparison\nMean Trust\nMean Difference\n\n\n\n\nLow - Low\n4.33\n0.00\n\n\nLow - Medium\n4.28\n0.05\n\n\nLow - High\n2.94\n1.39"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#understanding-a-z-test",
    "href": "lectures/01-lecture-slides.html#understanding-a-z-test",
    "title": "An Introduction to Statistics & Programming",
    "section": "Understanding a Z-Test",
    "text": "Understanding a Z-Test\nA Z-test is a test that compares the mean of one variable to a specific population parameter specified by the null hypothesis (usually 0) or to the mean of a different variable. To conduct a Z-test you can follow these steps:\n\nEnsure the Z-test assumptions are met.\nSet the the probability threshold you need to surpass for an effect to be considered significant—your alpha level.\nCompute your test statistic and determine if it is significant at your specified alpha-level."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#z-test-assumptions",
    "href": "lectures/01-lecture-slides.html#z-test-assumptions",
    "title": "An Introduction to Statistics & Programming",
    "section": "Z-Test Assumptions",
    "text": "Z-Test Assumptions\n\nThe populations from which the samples were taken from must be normal.\nThe population SDs must be known or the sample sizes for each group must be large (~30 or more observations per group)."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#z-test-test-statistic",
    "href": "lectures/01-lecture-slides.html#z-test-test-statistic",
    "title": "An Introduction to Statistics & Programming",
    "section": "Z-Test Test Statistic",
    "text": "Z-Test Test Statistic\nWhen you are comparing two groups to one another, like we are, the test statistic, \\(Z\\), is defined as:\n\\[Z = \\frac{\\overline{Y_{1}}-\\overline{Y_{2}}}{\\sqrt{\\frac{\\sigma^{2}_{1}}{n_{1}} + \\frac{\\sigma^{2}_{2}}{n_{2}}}}\\]"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#difference-in-trust-in-ai-means-by-job-experience",
    "href": "lectures/01-lecture-slides.html#difference-in-trust-in-ai-means-by-job-experience",
    "title": "An Introduction to Statistics & Programming",
    "section": "Difference in Trust in AI Means by Job Experience",
    "text": "Difference in Trust in AI Means by Job Experience\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComp.\nLow Exp. Mean\nMean Trust\nMean Diff.\nLow Exp. Var.\nVar. Trust\nn\n\n\n\n\nLow - Medium\n4.33\n4.28\n0.05\n0.81\n0.61\n100\n\n\nLow - High\n4.33\n2.94\n1.39\n0.81\n1.04\n100\n\n\n\n\n\n\n\\[Z = \\frac{4.33 - 4.28}{\\sqrt{\\frac{.81}{100} + \\frac{.61}{100}}} = \\frac{0.05}{.12}=.42\\]"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#determining-the-p-value",
    "href": "lectures/01-lecture-slides.html#determining-the-p-value",
    "title": "An Introduction to Statistics & Programming",
    "section": "Determining the P-Value",
    "text": "Determining the P-Value\nA P-value is a tricky thing to think about, it is the probability of seeing a value greater than or equal to your test value given that the sampling distribution specified by the null hypothesis is true.\n\\[P(Z \\geq z \\space | \\space H_{0} )\\]\nIf your P-value is small (usually less than .05), then you can conclude that your test statistic is very unlikely to have come from the null distribution and thus you can reject the null hypothesis."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#visualizing-the-p-value",
    "href": "lectures/01-lecture-slides.html#visualizing-the-p-value",
    "title": "An Introduction to Statistics & Programming",
    "section": "Visualizing the P-Value",
    "text": "Visualizing the P-Value\nWe assume (somewhat safely thanks to the CLT), the our estimate has a normal sampling distribution and according to our null hypothesis of no effect the mean of the normal distribution should be 0 and the SD should be 1. We can use the CDF of the standard normal distribution to compute the P-value."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#using-r-to-conduct-a-z-test",
    "href": "lectures/01-lecture-slides.html#using-r-to-conduct-a-z-test",
    "title": "An Introduction to Programming & Statistics",
    "section": "Using R to Conduct a Z-test",
    "text": "Using R to Conduct a Z-test\n\nlow_group &lt;- data$trust_in_ai[data$job_exp == \"Low\"]\nmedium_group &lt;- data$trust_in_ai[data$job_exp == \"Medium\"]\n\nz_low_medium &lt;- BSDA::z.test(\n  x = low_group, y = medium_group, alternative = \"greater\",\n  sigma.x = sd(low_group), sigma.y = sd(medium_group) \n)\n\nz_low_medium\n\n\n    Two-sample z-Test\n\ndata:  low_group and medium_group\nz = 0.42005, p-value = 0.3372\nalternative hypothesis: true difference in means is greater than 0\n95 percent confidence interval:\n -0.1457907         NA\nsample estimates:\nmean of x mean of y \n     4.33      4.28"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#using-r-to-conduct-a-z-test-1",
    "href": "lectures/01-lecture-slides.html#using-r-to-conduct-a-z-test-1",
    "title": "An Introduction to Programming & Statistics",
    "section": "Using R to Conduct a Z-test",
    "text": "Using R to Conduct a Z-test\n\nlow_group &lt;- data$trust_in_ai[data$job_exp == \"Low\"]\nhigh_group &lt;- data$trust_in_ai[data$job_exp == \"High\"]\n\nz_low_high &lt;- BSDA::z.test(\n  x = low_group, y = high_group, alternative = \"greater\",\n  sigma.x = sd(low_group), sigma.y = sd(high_group) \n)\n\nz_low_high\n\n\n    Two-sample z-Test\n\ndata:  low_group and high_group\nz = 10.203, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means is greater than 0\n95 percent confidence interval:\n 1.16591      NA\nsample estimates:\nmean of x mean of y \n     4.33      2.94"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#visualizing-a-high-p-value",
    "href": "lectures/01-lecture-slides.html#visualizing-a-high-p-value",
    "title": "An Introduction to Programming & Statistics",
    "section": "Visualizing a High P-Value",
    "text": "Visualizing a High P-Value"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#using-r-to-conduct-a-z-test-medium-job-experience",
    "href": "lectures/01-lecture-slides.html#using-r-to-conduct-a-z-test-medium-job-experience",
    "title": "An Introduction to Statistics & Programming",
    "section": "Using R to Conduct a Z-test: Medium Job Experience",
    "text": "Using R to Conduct a Z-test: Medium Job Experience\n\nlow_group &lt;- data$trust_in_ai[data$job_exp == \"Low\"]\nmedium_group &lt;- data$trust_in_ai[data$job_exp == \"Medium\"]\n\nz_low_medium &lt;- BSDA::z.test(\n  x = low_group, y = medium_group, alternative = \"greater\",\n  sigma.x = sd(low_group), sigma.y = sd(medium_group) \n)\n\nz_low_medium\n\n\n    Two-sample z-Test\n\ndata:  low_group and medium_group\nz = 0.42005, p-value = 0.3372\nalternative hypothesis: true difference in means is greater than 0\n95 percent confidence interval:\n -0.1457907         NA\nsample estimates:\nmean of x mean of y \n     4.33      4.28"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#using-r-to-conduct-a-z-test-high-job-experience",
    "href": "lectures/01-lecture-slides.html#using-r-to-conduct-a-z-test-high-job-experience",
    "title": "An Introduction to Statistics & Programming",
    "section": "Using R to Conduct a Z-test: High Job Experience",
    "text": "Using R to Conduct a Z-test: High Job Experience\n\nlow_group &lt;- data$trust_in_ai[data$job_exp == \"Low\"]\nhigh_group &lt;- data$trust_in_ai[data$job_exp == \"High\"]\n\nz_low_high &lt;- BSDA::z.test(\n  x = low_group, y = high_group, alternative = \"greater\",\n  sigma.x = sd(low_group), sigma.y = sd(high_group) \n)\n\nz_low_high\n\n\n    Two-sample z-Test\n\ndata:  low_group and high_group\nz = 10.203, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means is greater than 0\n95 percent confidence interval:\n 1.16591      NA\nsample estimates:\nmean of x mean of y \n     4.33      2.94"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#visualizing-a-very-small-p-value",
    "href": "lectures/01-lecture-slides.html#visualizing-a-very-small-p-value",
    "title": "An Introduction to Statistics & Programming",
    "section": "Visualizing a Very Small P-Value",
    "text": "Visualizing a Very Small P-Value"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#a-glimpse-of-your-data",
    "href": "lectures/01-lecture-slides.html#a-glimpse-of-your-data",
    "title": "An Introduction to Statistics & Programming",
    "section": "A Glimpse of your Data",
    "text": "A Glimpse of your Data\n\nset.seed(435)\ndata |&gt;\n  dplyr::slice_sample(n = 10) |&gt;\n  dplyr::select(\n    job_exp,\n    trust_in_ai\n  ) |&gt;\n  dplyr::arrange(\n    job_exp\n  )\n\n# A tibble: 10 × 2\n   job_exp trust_in_ai\n   &lt;fct&gt;         &lt;dbl&gt;\n 1 Low               5\n 2 Low               3\n 3 Low               5\n 4 Low               5\n 5 High              1\n 6 High              3\n 7 High              3\n 8 High              3\n 9 Medium            5\n10 Medium            5"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#the-direction-of-a-statistical-hypothesis",
    "href": "lectures/01-lecture-slides.html#the-direction-of-a-statistical-hypothesis",
    "title": "An Introduction to Statistics & Programming",
    "section": "The Direction of a Statistical Hypothesis",
    "text": "The Direction of a Statistical Hypothesis\nHypotheses can be directional or non-directional.\nA directional hypothesis is a hypothesis that makes an explicit statement about whether one group will have a larger (or smaller) mean than another group.\nA non-directional hypothesis is a hypothesis that states that the means of the two groups differ, but does not specify which group has a larger (or smaller) mean."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#determing-your-alpha-level",
    "href": "lectures/01-lecture-slides.html#determing-your-alpha-level",
    "title": "An Introduction to Statistics & Programming",
    "section": "Determing your Alpha-Level",
    "text": "Determing your Alpha-Level\nThe \\(\\alpha\\)-level, also called the significance level, is a number \\(\\alpha\\) between 0 and 1 such that we reject \\(H_{0}\\) if the P-value of the test statistic is less than or equal to \\(\\alpha\\).\nGenerally, we set \\(\\alpha\\) to .05 or .01. To reject the \\(H_{0}\\), the P-value needs to be less than or equal to .05 or .01, respectively."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#decision-errors",
    "href": "lectures/01-lecture-slides.html#decision-errors",
    "title": "An Introduction to Statistics & Programming",
    "section": "Decision Errors",
    "text": "Decision Errors\nThe conclusion you come to thanks to a statistical test is not guaranteed to be the right one. There is always a risk of making a decision error:\n\n\n\n\nReject \\(H_{0}\\)\nDo Not Reject \\(H_{0}\\)\n\n\n\n\n\\(H_{0}\\) is true\nType 1 Error\nCorrect Decision\n\n\n\\(H_{0}\\) is false\nCorrect Decision\nType 2 Error\n\n\n\nTo protect against a Type 1 Error, we can make our \\(\\alpha\\)-level very small, which will make it very difficult to reject \\(H_{0}\\), but this increases Type 2 Error. One way to guard against Type 2 Error is by using an appropriate statistical test on a large sample of data."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#example-base-function",
    "href": "lectures/01-lecture-slides.html#example-base-function",
    "title": "An Introduction to Statistics & Programming",
    "section": "Example Base Function",
    "text": "Example Base Function\n\nx &lt;- c(1, 4, 6)\nsum(x) \n\n[1] 11\n\nmean(x)\n\n[1] 3.666667\n\nmin(x)\n\n[1] 1"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#integrated-development-environment-rstudio",
    "href": "lectures/01-lecture-slides.html#integrated-development-environment-rstudio",
    "title": "An Introduction to Statistics & Programming",
    "section": "Integrated Development Environment & RStudio",
    "text": "Integrated Development Environment & RStudio\nAn integrated development environment (IDE) is an application that makes programming a little easier and organized. It includes all of the tools one needs to program effectively and efficiently.\nRStudio is an IDE initially developed for R, but it can be used for other programming languages too.\nIf you have not already, please go ahead and download RStudio from here."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#welcome",
    "href": "lectures/01-lecture-slides.html#welcome",
    "title": "An Introduction to Statistics & Programming",
    "section": "Welcome!",
    "text": "Welcome!\nFirst off, breathe! We will all make it through this together!\nMy quick teaching philosophy:\n\nI love talking not lecturing—ask me questions!\nWe are all here because we enjoy learning, which is the goal of my course: learning. DO NOT WORRY ABOUT YOUR GRADES—take that stress off of yourself."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#goals-for-today",
    "href": "lectures/01-lecture-slides.html#goals-for-today",
    "title": "An Introduction to Statistics & Programming",
    "section": "Goals for Today",
    "text": "Goals for Today\n\nRefresh yourself on statistics!\nLearn about statistical estimation and tests\nData importing and transformation with R"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#schedule-for-today",
    "href": "lectures/01-lecture-slides.html#schedule-for-today",
    "title": "An Introduction to Statistics & Programming",
    "section": "Schedule for Today",
    "text": "Schedule for Today\nIn addition to a 15 minute break at 2:30 PM, we will take two 5-ish minute breaks at:\n\n5 min break @ 1:45 PM\n5 min break @ 4:00 PM\n\nFeel free to ask me even more questions during this time!"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#types-of-decision-errors",
    "href": "lectures/01-lecture-slides.html#types-of-decision-errors",
    "title": "An Introduction to Statistics & Programming",
    "section": "Types of Decision Errors",
    "text": "Types of Decision Errors\nThe conclusion you come to thanks to a statistical test is not guaranteed to be the right one. There is always a risk of making a decision error:\n\n\n\n\nReject \\(H_{0}\\)\nDo Not Reject \\(H_{0}\\)\n\n\n\n\n\\(H_{0}\\) is true\nType 1 Error\nCorrect Decision\n\n\n\\(H_{0}\\) is false\nCorrect Decision\nType 2 Error"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#protecting-against-errors",
    "href": "lectures/01-lecture-slides.html#protecting-against-errors",
    "title": "An Introduction to Statistics & Programming",
    "section": "Protecting Against Errors",
    "text": "Protecting Against Errors\nTo protect against a Type 1 Error, we can make our \\(\\alpha\\)-level very small, which will make it very difficult to reject \\(H_{0}\\), but this increases Type 2 Error.\nOne way to guard against Type 2 Error is by using an appropriate statistical test on a large sample of data."
  },
  {
    "objectID": "notes/01-lecture-notes.html#what-is-statistics",
    "href": "notes/01-lecture-notes.html#what-is-statistics",
    "title": "Lecture 1 Notes",
    "section": "What is Statistics",
    "text": "What is Statistics"
  },
  {
    "objectID": "notes/01-lecture-notes.html#probability-theory-mathematics-of-uncertainty",
    "href": "notes/01-lecture-notes.html#probability-theory-mathematics-of-uncertainty",
    "title": "Lecture 1 Notes",
    "section": "Probability Theory: Mathematics of Uncertainty",
    "text": "Probability Theory: Mathematics of Uncertainty\n\nWhat is Probability?\n\n\nFundamentals of Probability Theory\n\n\nJoint Probability: The Probability of Two or More Events\n\n\nConditional Probability: The Probability of One Event Given Another"
  },
  {
    "objectID": "notes/01-lecture-notes.html#random-variables-connecting-probability-statistics-and-data",
    "href": "notes/01-lecture-notes.html#random-variables-connecting-probability-statistics-and-data",
    "title": "Lecture 1 Notes",
    "section": "Random Variables: Connecting Probability, Statistics, and Data",
    "text": "Random Variables: Connecting Probability, Statistics, and Data\n\nTypes of Random Variables: Discrete or Continuous\n\n\nProbability Distributions\n\n\nProbability Mass Function: Describing Discrete Random Variables\n\n\nProbability Density Function: Describing Continuous Random Variables\n\n\nCumulative Distribution Function\n\n\nJoint & Conditional Distributions: Modeling Two (or More) Random Variables"
  },
  {
    "objectID": "notes/01-lecture-notes.html#descriptive-statistics-describing-your-data",
    "href": "notes/01-lecture-notes.html#descriptive-statistics-describing-your-data",
    "title": "Lecture 1 Notes",
    "section": "Descriptive Statistics: Describing Your Data",
    "text": "Descriptive Statistics: Describing Your Data\n\nPopulation vs Sample\n\n\nMeasures of Central Tendency\n\nMean (or Expected Value)\n\n\nMedian\n\n\nMode\n\n\n\nMeasures of Spread\n\nRange\n\n\nVariance\n\n\nStandard Deviation"
  },
  {
    "objectID": "notes/01-lecture-notes.html#inferential-statistics-generalizing-from-your-data",
    "href": "notes/01-lecture-notes.html#inferential-statistics-generalizing-from-your-data",
    "title": "Lecture 1 Notes",
    "section": "Inferential Statistics: Generalizing from Your Data",
    "text": "Inferential Statistics: Generalizing from Your Data"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#schedule-for-today",
    "href": "lectures/02-lecture-slides.html#schedule-for-today",
    "title": "An Introduction to Simple Regression",
    "section": "Schedule for Today",
    "text": "Schedule for Today\n\nTalk about stats for ~75 mins (5 PM - 6:15 PM ET)\nBreak for 5 minutes\nR Introduction / Hands-on coding for 40 mins (6:20 PM - 7:00 PM ET)"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#overview",
    "href": "lectures/02-lecture-slides.html#overview",
    "title": "An Introduction to Simple Regression",
    "section": "Overview",
    "text": "Overview\n\nA quick review of last week\nOverview of conditional distributions & statistics\nIntroduction to Simple Regression\nR Intro"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#goals",
    "href": "lectures/02-lecture-slides.html#goals",
    "title": "An Introduction to Simple Regression",
    "section": "Goals",
    "text": "Goals\n\nDevelop an understanding of simple regression\nWrite your first R script"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#theory-construction-in-the-managerial-sciences",
    "href": "lectures/02-lecture-slides.html#theory-construction-in-the-managerial-sciences",
    "title": "Correlation & Covariation",
    "section": "Theory Construction in the Managerial Sciences",
    "text": "Theory Construction in the Managerial Sciences"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#empirically-testing-theories",
    "href": "lectures/02-lecture-slides.html#empirically-testing-theories",
    "title": "An Introduction to Simple Regression",
    "section": "Empirically Testing Theories",
    "text": "Empirically Testing Theories\nWhat is theory without evidence? We need methods that allow us to test the relationships among variables hypothesized by our theory and we need to adjust for other variables (control)"
  },
  {
    "objectID": "lectures/02-lecture-page.html",
    "href": "lectures/02-lecture-page.html",
    "title": "Quantitative Analysis 1",
    "section": "",
    "text": "Next Week’s Materials »"
  },
  {
    "objectID": "lectures/02-lecture-page.html#lecture-stats-bootcamp",
    "href": "lectures/02-lecture-page.html#lecture-stats-bootcamp",
    "title": "Quantitative Analysis 1",
    "section": "Lecture: Stats Bootcamp",
    "text": "Lecture: Stats Bootcamp\n\n\nTo download a pdf version of these slides, click here.\nTo download the R script that follows the R portion of the lecture, click here."
  },
  {
    "objectID": "lectures/02-lecture-slides.html#why-do-re-care-about-statistical-modeling",
    "href": "lectures/02-lecture-slides.html#why-do-re-care-about-statistical-modeling",
    "title": "An Introduction to Simple Regression",
    "section": "Why Do Re Care About Statistical Modeling?",
    "text": "Why Do Re Care About Statistical Modeling?\nIn research and practice, you will likely come up with questions or hypotheses of the form:\n\n\n“Are changes in Variable X associated with changes in Variable Y?”\n\n\nWe can apply the models we will talk about in class to our data in order to empirically test our hypotheses (or answer our questions). These models allow us to make data-driven evaluations of our theories."
  },
  {
    "objectID": "lectures/02-lecture-slides.html#the-role-of-theory-in-research-practice",
    "href": "lectures/02-lecture-slides.html#the-role-of-theory-in-research-practice",
    "title": "An Introduction to Simple Regression",
    "section": "The Role of Theory in Research & Practice",
    "text": "The Role of Theory in Research & Practice\n\nA theory is a set of interrelated constructs (concepts), definitions, and propositions that present a systematic view of phenomena by specifying relations among variables with &gt; the purpose of explaining and predicting the phenomenon.\n\n— Kerlinger & Lee (2000)"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#from-theory-to-model-to-data",
    "href": "lectures/02-lecture-slides.html#from-theory-to-model-to-data",
    "title": "An Introduction to Simple Regression",
    "section": "From Theory to Model to Data",
    "text": "From Theory to Model to Data\nPath model diagrams"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#enter-general-linear-models",
    "href": "lectures/02-lecture-slides.html#enter-general-linear-models",
    "title": "Correlation, Covariation, & Simple Regression",
    "section": "Enter General Linear Models",
    "text": "Enter General Linear Models\nSpecfiically regression"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#example-theory-hypothesis",
    "href": "lectures/02-lecture-slides.html#example-theory-hypothesis",
    "title": "An Introduction to Simple Regression",
    "section": "Example Theory & Hypothesis",
    "text": "Example Theory & Hypothesis"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#visualzing-the-relationship-between-two-variables",
    "href": "lectures/02-lecture-slides.html#visualzing-the-relationship-between-two-variables",
    "title": "An Introduction to Simple Regression",
    "section": "Visualzing the Relationship Between Two Variables",
    "text": "Visualzing the Relationship Between Two Variables\nScatter Plot"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#improving-our-scatter-plot",
    "href": "lectures/02-lecture-slides.html#improving-our-scatter-plot",
    "title": "An Introduction to Simple Regression",
    "section": "Improving our Scatter Plot",
    "text": "Improving our Scatter Plot\nWe can use an R function called geom_jitter to add a bit of random noise to each of our data points, which improves the usefulness of scatter plots when working with discrete data like survey responses."
  },
  {
    "objectID": "lectures/02-lecture-slides.html#conditional-distributions",
    "href": "lectures/02-lecture-slides.html#conditional-distributions",
    "title": "An Introduction to Simple Regression",
    "section": "Conditional Distributions",
    "text": "Conditional Distributions\nConditional distributions are distributions of one variable, Y, conditional (fixed) on a value of one or more additional variables."
  },
  {
    "objectID": "lectures/02-lecture-slides.html#conditional-mean-expectation",
    "href": "lectures/02-lecture-slides.html#conditional-mean-expectation",
    "title": "An Introduction to Simple Regression",
    "section": "Conditional Mean (Expectation)",
    "text": "Conditional Mean (Expectation)\n\\[E[Y|X]\\]"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#conditional-mean-plot",
    "href": "lectures/02-lecture-slides.html#conditional-mean-plot",
    "title": "An Introduction to Simple Regression",
    "section": "Conditional Mean Plot",
    "text": "Conditional Mean Plot"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#using-covariation-correlation-to-measure-linear-dependence",
    "href": "lectures/02-lecture-slides.html#using-covariation-correlation-to-measure-linear-dependence",
    "title": "An Introduction to Simple Regression",
    "section": "Using Covariation & Correlation to Measure Linear Dependence",
    "text": "Using Covariation & Correlation to Measure Linear Dependence\nTwo of the most basic measures of linear dependence we have between two variables are the covariance and correlation between two variables.\n\\[Cov(X, Y)=\\frac{\\Sigma{(X - \\overline{X})(Y-\\overline{Y})}}{N}\\] \\[r_{XY}=\\frac{Cov(X,Y)}{S_{X}S_{Y}}\\]"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#using-the-slope-of-a-line-to-measure-dependence",
    "href": "lectures/02-lecture-slides.html#using-the-slope-of-a-line-to-measure-dependence",
    "title": "Correlation, Covariation, & Simple Regression",
    "section": "Using the Slope of a Line to Measure Dependence",
    "text": "Using the Slope of a Line to Measure Dependence"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#what-line-to-choose",
    "href": "lectures/02-lecture-slides.html#what-line-to-choose",
    "title": "Correlation, Covariation, & Simple Regression",
    "section": "What Line to Choose?",
    "text": "What Line to Choose?"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#errors-of-estimate-or-prediction-errors",
    "href": "lectures/02-lecture-slides.html#errors-of-estimate-or-prediction-errors",
    "title": "An Introduction to Simple Regression",
    "section": "Errors of Estimate (or Prediction Errors)",
    "text": "Errors of Estimate (or Prediction Errors)\nWe say that the “best” line is the line that minimizes the squared distance between our outcome variable, \\(Y\\), and what our line predicts that outcome variable to be, \\(\\hat{Y}\\), on average:\n\\[SS_{error}=\\Sigma(Y_{i}-\\hat{Y}_{i})^2=\\Sigma(e^{2}_{i})\\]\n\\[\\hat{Y}_{i}=\\beta_{0}+\\beta_{1}X_{i}\\]\nWe call this the sum of squared error."
  },
  {
    "objectID": "lectures/02-lecture-slides.html#linear-regression-the-line-that-best-fits-your-data",
    "href": "lectures/02-lecture-slides.html#linear-regression-the-line-that-best-fits-your-data",
    "title": "An Introduction to Simple Regression",
    "section": "Linear Regression: The Line that “Best” Fits your Data",
    "text": "Linear Regression: The Line that “Best” Fits your Data\nLinear regression is the statistical method that estimates the “best” fitting line by minimizing the sum of squared errors (\\(SS_{error}\\)).\nThere is no other line that will produce a smaller value of \\(SS_{error}\\)!"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#the-linear-regression-model",
    "href": "lectures/02-lecture-slides.html#the-linear-regression-model",
    "title": "An Introduction to Simple Regression",
    "section": "The Linear Regression Model",
    "text": "The Linear Regression Model"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#estimating-your-model-using-lm",
    "href": "lectures/02-lecture-slides.html#estimating-your-model-using-lm",
    "title": "An Introduction to Simple Regression",
    "section": "Estimating your Model Using lm",
    "text": "Estimating your Model Using lm\n\nmod_ai &lt;- lm(freq_use_ai ~ pos_attitude_ai, data = data_ai)\nmod_ai |&gt; summary()\n\n\nCall:\nlm(formula = freq_use_ai ~ pos_attitude_ai, data = data_ai)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.9683 -1.0050  0.0072  1.0317  3.9950 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      1.51112    0.05575   27.11   &lt;2e-16 ***\npos_attitude_ai  0.49388    0.01285   38.45   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.483 on 4998 degrees of freedom\nMultiple R-squared:  0.2282,    Adjusted R-squared:  0.2281 \nF-statistic:  1478 on 1 and 4998 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#interpreting-the-coefficients",
    "href": "lectures/02-lecture-slides.html#interpreting-the-coefficients",
    "title": "An Introduction to Simple Regression",
    "section": "Interpreting the Coefficients",
    "text": "Interpreting the Coefficients"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#residuals",
    "href": "lectures/02-lecture-slides.html#residuals",
    "title": "An Introduction to Simple Regression",
    "section": "Residuals",
    "text": "Residuals"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#residual-analysis",
    "href": "lectures/02-lecture-slides.html#residual-analysis",
    "title": "An Introduction to Simple Regression",
    "section": "Residual Analysis",
    "text": "Residual Analysis"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#variance-explained",
    "href": "lectures/02-lecture-slides.html#variance-explained",
    "title": "An Introduction to Simple Regression",
    "section": "Variance Explained",
    "text": "Variance Explained"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#r2",
    "href": "lectures/02-lecture-slides.html#r2",
    "title": "An Introduction to Simple Regression",
    "section": "R2",
    "text": "R2"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#m",
    "href": "lectures/02-lecture-slides.html#m",
    "title": "An Introduction to Simple Regression",
    "section": "M",
    "text": "M"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#using-the-slope-of-a-line-to-measure-linear-dependence",
    "href": "lectures/02-lecture-slides.html#using-the-slope-of-a-line-to-measure-linear-dependence",
    "title": "An Introduction to Simple Regression",
    "section": "Using the Slope of a Line to Measure Linear Dependence",
    "text": "Using the Slope of a Line to Measure Linear Dependence\n\\[Y = \\beta_{0}+\\beta_{1}X_{1}+\\epsilon\\]\n\\(\\beta_{0}\\): The intercept of the line\n\\(\\beta_{1}\\): The slope of the line\n\\(\\epsilon\\): Error"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#which-line-to-choose",
    "href": "lectures/02-lecture-slides.html#which-line-to-choose",
    "title": "An Introduction to Simple Regression",
    "section": "Which Line to Choose?",
    "text": "Which Line to Choose?\nWe could, however, choose many different lines. How do we determine what the “best” line is? First, we have to define exactly what we mean by “best.”"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#quick-review-of-terminology",
    "href": "lectures/02-lecture-slides.html#quick-review-of-terminology",
    "title": "An Introduction to Simple Regression",
    "section": "Quick Review of Terminology",
    "text": "Quick Review of Terminology\n\nMean of X: \\(\\overline{X}\\) or \\(E[X]\\)\nVariance of X: \\(\\sigma^{2}_{x}\\) or \\(Var(X)\\)\nStandard Deviation of X: \\(\\sigma_{x}\\) or \\(S_{X}\\)\nCovariance of X & Y: \\(Cov(XY)\\)\nCorrelation of X & Y: \\(r_{xy}\\)\nPopulation Parameter: \\(\\beta\\) or any other Greek letter\nPopulation Estimate: \\(\\hat{\\beta}\\)"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#conditional-statistics",
    "href": "lectures/02-lecture-slides.html#conditional-statistics",
    "title": "An Introduction to Simple Regression",
    "section": "Conditional Statistics",
    "text": "Conditional Statistics\nConditional statistics are statistics computed from conditional distributions. The characteristics of a distribution (mean, variance, etc.) can change based on the values of another variable.\nLinear regression models are largely concerned with two conditional statistics:\n\nConditional Mean (or Expectation): \\(E[Y|X]\\)\nConditional Variance: \\(\\sigma^2_{Y|X}\\)"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#conditional-statistics-visualized",
    "href": "lectures/02-lecture-slides.html#conditional-statistics-visualized",
    "title": "An Introduction to Simple Regression",
    "section": "Conditional Statistics Visualized",
    "text": "Conditional Statistics Visualized"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#conditional-mean-dependence",
    "href": "lectures/02-lecture-slides.html#conditional-mean-dependence",
    "title": "An Introduction to Simple Regression",
    "section": "Conditional Mean Dependence",
    "text": "Conditional Mean Dependence"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#modeling-mean-dependence",
    "href": "lectures/02-lecture-slides.html#modeling-mean-dependence",
    "title": "An Introduction to Simple Regression",
    "section": "Modeling Mean Dependence",
    "text": "Modeling Mean Dependence\nIn statistics, we are mostly interested in modeling mean dependence—this is why statistics has been referred to as “the science of averages.”\nEvery statistical model we will use in this class is ultimately trying to find a function that best describes how the mean of some variable Y changes across the values of some set of variables, X.\n\\[E[Y|X] = f(X)\\]"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#visualizing-mean-dependence",
    "href": "lectures/02-lecture-slides.html#visualizing-mean-dependence",
    "title": "An Introduction to Simple Regression",
    "section": "Visualizing Mean Dependence",
    "text": "Visualizing Mean Dependence"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#measuring-mean-dependence-with-covariance-correlation",
    "href": "lectures/02-lecture-slides.html#measuring-mean-dependence-with-covariance-correlation",
    "title": "An Introduction to Simple Regression",
    "section": "Measuring Mean Dependence with Covariance & Correlation",
    "text": "Measuring Mean Dependence with Covariance & Correlation\nTwo of the most basic measures of the linear mean dependence between two variables are the covariance and correlation:\n\\[Cov(X, Y)=\\frac{\\Sigma{(X - \\overline{X})(Y-\\overline{Y})}}{N}\\] \\[r_{XY}=\\frac{Cov(X,Y)}{S_{X}S_{Y}}\\]"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#measuring-mean-dependence-with-a-linear-function-a-line",
    "href": "lectures/02-lecture-slides.html#measuring-mean-dependence-with-a-linear-function-a-line",
    "title": "An Introduction to Simple Regression",
    "section": "Measuring Mean Dependence with a Linear Function (a Line)",
    "text": "Measuring Mean Dependence with a Linear Function (a Line)\nA different, but related way, to measure mean dependence is to choose a linear function (a line) to describe how the mean of \\(Y\\) changes across values of \\(X\\).\n\\[E[Y|X] = \\beta_{0}+\\beta_{1}X_{1}\\]\n\\(\\beta_{0}\\): The intercept of the line\n\\(\\beta_{1}\\): The slope of the line, which is a measure of linear mean dependence"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#the-simple-regression-model",
    "href": "lectures/02-lecture-slides.html#the-simple-regression-model",
    "title": "An Introduction to Simple Regression",
    "section": "The Simple Regression Model",
    "text": "The Simple Regression Model\nThe simple regression model is just a linear regression model with one independent variable.\n\\[Y=\\beta_0+\\beta_1X_1+\\epsilon\\]\n\n\\(\\beta_0\\): The expected value (mean) of Y when X = 0.\n\\(\\beta_1\\): The average change in Y for a one-unit increase in X.\n\\(\\epsilon\\): Variation in Y that is not explained by our model—unexplained variance."
  },
  {
    "objectID": "lectures/02-lecture-slides.html#estimating-the-regression-coefficients",
    "href": "lectures/02-lecture-slides.html#estimating-the-regression-coefficients",
    "title": "An Introduction to Simple Regression",
    "section": "Estimating the Regression Coefficients",
    "text": "Estimating the Regression Coefficients\nWe will never know the population values of the regression coefficients (\\(\\beta_0\\) & \\(\\beta_1\\)), but we can use our data to estimate them:\n\\[\\hat{\\beta}_1=\\frac{Cov(X,Y)}{Var(X)}\\]\n\\[\\hat{\\beta}_0=\\overline{Y}-\\hat{\\beta}_1\\overline{X}\\]"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#what-is-a-statistical-model",
    "href": "lectures/02-lecture-slides.html#what-is-a-statistical-model",
    "title": "An Introduction to Simple Regression",
    "section": "What is a Statistical Model?",
    "text": "What is a Statistical Model?\nA statistical model is an approximation of some random process that uses probability theory and other mathematical tools to describe the process.\nIn your own research, you will likely rely on theory to develop your own statistical models."
  },
  {
    "objectID": "lectures/02-lecture-slides.html#example-hypothesis",
    "href": "lectures/02-lecture-slides.html#example-hypothesis",
    "title": "An Introduction to Simple Regression",
    "section": "Example Hypothesis",
    "text": "Example Hypothesis\nYour organization has just implemented a new generative AI tool (fancy chat bot) to help improve the efficiency of the organizations sales force. Sales employees, however, have adopted the technology at different rates.\nYou have been asked to understand if an employee’s general positive attitude toward AI is related to how frequently they are using the new AI tool and you form the following hypothesis:\n\nAn employee’s general positive attitude toward AI will be positively related to the frequency with which they use the new AI tool."
  },
  {
    "objectID": "lectures/02-lecture-slides.html#view-our-dataset",
    "href": "lectures/02-lecture-slides.html#view-our-dataset",
    "title": "An Introduction to Simple Regression",
    "section": "View our Dataset",
    "text": "View our Dataset\n\ndata_ai |&gt;\n  dplyr::select(\n    pos_attitude_ai:freq_use_ai_label\n  )\n\n# A tibble: 5,000 × 4\n   pos_attitude_ai freq_use_ai pos_attitude_ai_label freq_use_ai_label\n             &lt;int&gt;       &lt;dbl&gt; &lt;chr&gt;                 &lt;chr&gt;            \n 1               4           4 Neutral               Fairly Often     \n 2               6           6 Strongly Agree        All the time     \n 3               7           3 Completely Agree      Occasionally     \n 4               7           6 Completely Agree      All the time     \n 5               4           3 Neutral               Occasionally     \n 6               5           4 Agree                 Fairly Often     \n 7               5           6 Agree                 All the time     \n 8               3           4 Disagree              Fairly Often     \n 9               3           5 Disagree              Very Often       \n10               4           4 Neutral               Fairly Often     \n# ℹ 4,990 more rows"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#visualizing-the-relationship-between-two-variables",
    "href": "lectures/02-lecture-slides.html#visualizing-the-relationship-between-two-variables",
    "title": "An Introduction to Simple Regression",
    "section": "Visualizing the Relationship Between Two Variables",
    "text": "Visualizing the Relationship Between Two Variables\nWe can use a scatter plot to visually explore the relationship between our two variables. Why does it look so odd?"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#partitioning-variance",
    "href": "lectures/02-lecture-slides.html#partitioning-variance",
    "title": "An Introduction to Simple Regression",
    "section": "Partitioning Variance",
    "text": "Partitioning Variance\n\\[(Y - \\overline{Y})^2=(Y - \\hat{Y})^2 + (\\hat{Y}-\\overline{Y})^2\\]"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#r-squared-variance-explained",
    "href": "lectures/02-lecture-slides.html#r-squared-variance-explained",
    "title": "An Introduction to Simple Regression",
    "section": "R-Squared: Variance Explained",
    "text": "R-Squared: Variance Explained"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#exploring-our-data",
    "href": "lectures/02-lecture-slides.html#exploring-our-data",
    "title": "An Introduction to Simple Regression",
    "section": "Exploring our Data",
    "text": "Exploring our Data"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#interpreting-the-coefficients-as-group-comparisons",
    "href": "lectures/02-lecture-slides.html#interpreting-the-coefficients-as-group-comparisons",
    "title": "An Introduction to Simple Regression",
    "section": "Interpreting the Coefficients as Group Comparisons",
    "text": "Interpreting the Coefficients as Group Comparisons\nThe most appropriate way to interpret the slope coefficient (\\(\\beta_1\\)) is as a comparison."
  },
  {
    "objectID": "lectures/02-lecture-slides.html#interpreting-the-coefficients-of-our-model",
    "href": "lectures/02-lecture-slides.html#interpreting-the-coefficients-of-our-model",
    "title": "An Introduction to Simple Regression",
    "section": "Interpreting the Coefficients of our Model",
    "text": "Interpreting the Coefficients of our Model\nComparing employees who differ in their attidues towards AI by one point, the average difference in the frequency with which those employees use the AI tool is 0.49 points."
  },
  {
    "objectID": "lectures/02-lecture-slides.html#interpreting-the-coefficients-as-comparisons",
    "href": "lectures/02-lecture-slides.html#interpreting-the-coefficients-as-comparisons",
    "title": "An Introduction to Simple Regression",
    "section": "Interpreting the Coefficients as Comparisons",
    "text": "Interpreting the Coefficients as Comparisons\nThe most appropriate way to interpret the slope coefficient (\\(\\beta_1\\)) is as a comparison. In our example:\nComparing employees who differ in their attitudes towards AI by one point, the average difference in the frequency with which those employees use the AI tool is 0.49 points."
  },
  {
    "objectID": "lectures/02-lecture-slides.html#visualizing-the-comparison",
    "href": "lectures/02-lecture-slides.html#visualizing-the-comparison",
    "title": "An Introduction to Simple Regression",
    "section": "Visualizing the Comparison",
    "text": "Visualizing the Comparison"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#regression-coefficients-the-strength-of-the-relationship",
    "href": "lectures/02-lecture-slides.html#regression-coefficients-the-strength-of-the-relationship",
    "title": "An Introduction to Simple Regression",
    "section": "Regression Coefficients & the Strength of the Relationship",
    "text": "Regression Coefficients & the Strength of the Relationship\nIt is very difficult to determine the strength of the relationship between a dependent variable, Y, and an independent variable, X, using just the regression slope, \\(\\beta_1\\).\nIt is difficult because the magnitude of \\(\\beta_1\\) depends on the scale of both the dependent and independent variable, so you can artificially change the magnitude of the slope by changing the scale of your variables."
  },
  {
    "objectID": "lectures/02-lecture-slides.html#the-connection-between-a-correlation-and-a-regression-coefficient",
    "href": "lectures/02-lecture-slides.html#the-connection-between-a-correlation-and-a-regression-coefficient",
    "title": "An Introduction to Simple Regression",
    "section": "The Connection between a Correlation and a Regression Coefficient",
    "text": "The Connection between a Correlation and a Regression Coefficient\nYou can use the correlation between the independent and dependent variable to estimate how strongly related the two variables are.\nIn simple linear regression there is a straightforward relationship between a correlation and a regression coefficient:\n\\[r_{XY}=\\beta_1\\times\\frac{SD_X}{SD_Y}\\]"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#scaling-your-dependent-and-independent-variable",
    "href": "lectures/02-lecture-slides.html#scaling-your-dependent-and-independent-variable",
    "title": "An Introduction to Simple Regression",
    "section": "Scaling your Dependent and Independent Variable",
    "text": "Scaling your Dependent and Independent Variable\nAnother way to determine the strength of the relationship between a dependent and an indpendent variable is to scale (standardize) each variable, so that it has a mean of 0 and a SD of 1:\n\\[Z_X=\\frac{X-\\overline{X}}{SD_X}\\]\nYou can then estimate a new regression model using the scaled variables and use the magnitude of \\(\\beta_1\\) to judge the strength of the relationship between the independent and dependent variable."
  },
  {
    "objectID": "lectures/02-lecture-slides.html#example-of-scaling-variables",
    "href": "lectures/02-lecture-slides.html#example-of-scaling-variables",
    "title": "An Introduction to Simple Regression",
    "section": "Example of Scaling Variables",
    "text": "Example of Scaling Variables\n\ndata_ai &lt;-\n  data_ai |&gt;\n  dplyr::mutate(\n    pos_attitude_ai_scale = (pos_attitude_ai - mean(pos_attitude_ai)) / sd(pos_attitude_ai),\n    freq_use_ai_scale = scale(freq_use_ai)[,1]\n  )\n\nmod_scale &lt;- lm(freq_use_ai_scale ~ pos_attitude_ai_scale, data_ai)\nmod_scale$coefficients |&gt; round(2)\n\n          (Intercept) pos_attitude_ai_scale \n                 0.00                  0.48 \n\ncor(data_ai$pos_attitude_ai, data_ai$freq_use_ai) |&gt; round(2)\n\n[1] 0.48"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#measuring-strength-with-correlation-coefficient",
    "href": "lectures/02-lecture-slides.html#measuring-strength-with-correlation-coefficient",
    "title": "An Introduction to Simple Regression",
    "section": "Measuring Strength with Correlation Coefficient",
    "text": "Measuring Strength with Correlation Coefficient\nYou can use the correlation between the independent and dependent variable to estimate how strongly related the two variables are.\nIn simple linear regression there is a straightforward relationship between a correlation and a regression coefficient:\n\\[r_{XY}=\\beta_1\\times\\frac{SD_X}{SD_Y}\\]"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#measuring-strength-with-scaled-variables",
    "href": "lectures/02-lecture-slides.html#measuring-strength-with-scaled-variables",
    "title": "An Introduction to Simple Regression",
    "section": "Measuring Strength with Scaled Variables",
    "text": "Measuring Strength with Scaled Variables\nIf you can estimate a regression model using the scaled (standardized) independent and dependent variables, then you can use the magnitude of \\(\\beta_1\\) to judge the strength of the relationship between the independent and dependent variable.\nScaling or standardizing a variable transforms the mean of the variable to 0 and its variance and SD to 1:\n\\[\\text{Scaled X = }Z_X=\\frac{X-\\overline{X}}{SD_X}\\]"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#the-two-approaches-will-lead-to-the-same-answer",
    "href": "lectures/02-lecture-slides.html#the-two-approaches-will-lead-to-the-same-answer",
    "title": "An Introduction to Simple Regression",
    "section": "The Two Approaches will Lead to the Same Answer",
    "text": "The Two Approaches will Lead to the Same Answer\n\ndata_ai &lt;-\n  data_ai |&gt;\n  dplyr::mutate(\n    pos_attitude_ai_scale = (pos_attitude_ai - mean(pos_attitude_ai)) / sd(pos_attitude_ai),\n    freq_use_ai_scale = scale(freq_use_ai)[,1]\n  )\n\nmod_scale &lt;- lm(freq_use_ai_scale ~ pos_attitude_ai_scale, data_ai)\nreg_coef &lt;- mod_scale$coefficients[2] |&gt; round(2)\ncor_coef &lt;- cor(data_ai$pos_attitude_ai, data_ai$freq_use_ai) |&gt; round(2)\ntibble::tibble(`Reg. Coef.` = reg_coef, `Corr. Coef` = cor_coef)\n\n# A tibble: 1 × 2\n  `Reg. Coef.` `Corr. Coef`\n         &lt;dbl&gt;        &lt;dbl&gt;\n1         0.48         0.48"
  },
  {
    "objectID": "lectures/02-lecture-slides.html",
    "href": "lectures/02-lecture-slides.html",
    "title": "An Introduction to Simple Regression",
    "section": "",
    "text": "Talk about stats for ~75 mins (5 PM - 6:15 PM ET)\nBreak for 5 minutes\nR Introduction / Hands-on coding for 40 mins (6:20 PM - 7:00 PM ET)"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#how-well-does-your-model-fit",
    "href": "lectures/02-lecture-slides.html#how-well-does-your-model-fit",
    "title": "An Introduction to Simple Regression",
    "section": "How Well Does Your Model Fit?",
    "text": "How Well Does Your Model Fit?\nYou will often want to determine how well your model fits your data—how well does your model predict your observed outcome, Y.\nTo determine this, we will partition our observed outcome into three additive pieces:\n\\[Y_{i} = \\underbrace{\\overline{Y}}_\\text{Mean Component}+\\underbrace{(\\hat{Y}_{i}-\\overline{Y})}_\\text{Model Component}+\\underbrace{(Y_{i} - \\hat{Y}_{i})}_\\text{Error Component}\\]"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#partitioning-our-observed-score-variance",
    "href": "lectures/02-lecture-slides.html#partitioning-our-observed-score-variance",
    "title": "An Introduction to Simple Regression",
    "section": "Partitioning our Observed Score Variance",
    "text": "Partitioning our Observed Score Variance\nWe can use the model and error components to summarize the variability in our outcome by partitioning it into variability because of our model and variability because of error and other unexplained causes.\n\\[\\underbrace{\\Sigma(Y_{i}-\\overline{Y})^2}_\\text{Total SS}=\\underbrace{\\Sigma(\\hat{Y}_i-\\overline{Y})^2}_\\text{SS Model}+\\underbrace{\\Sigma(Y_i-\\hat{Y}_i)}_\\text{SS Error}\\]\n\\[\\hat{\\sigma}^2_Y=\\hat{\\sigma}^2_{model}+\\hat{\\sigma}^2_{error}\\]"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#measuring-model-fit-with-r-squared",
    "href": "lectures/02-lecture-slides.html#measuring-model-fit-with-r-squared",
    "title": "An Introduction to Simple Regression",
    "section": "Measuring Model Fit with R-Squared",
    "text": "Measuring Model Fit with R-Squared\nUsing partitioned variance we can compute a statistic, \\(R^2\\), that tells us how well your model fits your data overall.\n\\[R^2 = \\frac{\\hat{\\sigma}^2_{model}}{\\hat{\\sigma}^2_{Y}}=1-\\frac{\\hat{\\sigma}^2_{error}}{\\hat{\\sigma}^2_Y} \\]\nYou can interpret \\(R^2\\) as the proportion of variance in your observed outcome that is explained by your model."
  },
  {
    "objectID": "lectures/02-lecture-slides.html#r-squared-in-our-example",
    "href": "lectures/02-lecture-slides.html#r-squared-in-our-example",
    "title": "An Introduction to Simple Regression",
    "section": "R-Squared In Our Example",
    "text": "R-Squared In Our Example\n23% of the variance in the frequency with which the sales representatives use the new AI tool can be explained by their general attitudes toward AI.\n\nsummary(mod_ai)\n\n\nCall:\nlm(formula = freq_use_ai ~ pos_attitude_ai, data = data_ai)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.9683 -1.0050  0.0072  1.0317  3.9950 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      1.51112    0.05575   27.11   &lt;2e-16 ***\npos_attitude_ai  0.49388    0.01285   38.45   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.483 on 4998 degrees of freedom\nMultiple R-squared:  0.2282,    Adjusted R-squared:  0.2281 \nF-statistic:  1478 on 1 and 4998 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#looking-at-the-model-component",
    "href": "lectures/02-lecture-slides.html#looking-at-the-model-component",
    "title": "An Introduction to Simple Regression",
    "section": "Looking at the Model Component",
    "text": "Looking at the Model Component\nThe model component tells us if our model is better able to predict our outcome than its own mean.\nIf we take a closer look at this component, what happens to it as \\(\\hat{\\beta}_1\\) becomes a perfect predictor of Y? Decreases to 0?\n\\[\\hat{Y}_{i}-\\overline{Y}\\\\=\\hat{\\beta}_0+\\hat{\\beta}_1X_{i}-\\overline{Y}\\\\=\\overline{Y}-\\hat{\\beta}_1\\overline{X}+\\hat{\\beta}_{1}X_{i}-\\overline{Y}\\\\=\\hat{\\beta}_{1}(X_{i}-\\overline{X})\\]"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#looking-at-the-error-component",
    "href": "lectures/02-lecture-slides.html#looking-at-the-error-component",
    "title": "An Introduction to Simple Regression",
    "section": "Looking at the Error Component",
    "text": "Looking at the Error Component\nThe error component or residual plays an important role in linear regression. It tell us the extent to which our model is able to predict our outcome.\nIf we take a closer look at this component, what happens to it as \\(\\hat{\\beta}_1\\) becomes a perfect predictor of Y? Decreases to 0?\n\\[Y_{i}-\\hat{Y}_i\\\\=Y_{i}-\\hat{\\beta}_0-\\hat{\\beta}_1X_{i}\\\\=Y_{i}-\\overline{Y}-\\hat{\\beta}_{1}(X_{i}-\\overline{X})\\]"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#is-your-effect-significant",
    "href": "lectures/02-lecture-slides.html#is-your-effect-significant",
    "title": "An Introduction to Simple Regression",
    "section": "Is Your Effect Significant?",
    "text": "Is Your Effect Significant?\nOften researchers determine the significance of their coefficients by comparing the coefficients to a null distribution with a mean of 0.\nThe p-value tells you the probability of seeing an estimate equal to or greater than the absolute value of your estimate given that the true effect is 0.\n\n\n# A tibble: 2 × 5\n  term            estimate std.error statistic   p.value\n  &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)         1.51      0.06      27.1 4.39e-151\n2 pos_attitude_ai     0.49      0.01      38.4 1.56e-283"
  },
  {
    "objectID": "lectures/02-lecture-page.html#lecture-introduction-to-simple-regression",
    "href": "lectures/02-lecture-page.html#lecture-introduction-to-simple-regression",
    "title": "Quantitative Analysis 1",
    "section": "Lecture: Introduction to Simple Regression",
    "text": "Lecture: Introduction to Simple Regression\n\n\nTo download a pdf version of these slides, click here.\nTo download the R script that follows the R portion of the lecture, click here."
  },
  {
    "objectID": "lectures/02-lecture-slides.html#why-do-we-care-about-statistical-modeling",
    "href": "lectures/02-lecture-slides.html#why-do-we-care-about-statistical-modeling",
    "title": "An Introduction to Simple Regression",
    "section": "Why Do We Care About Statistical Modeling?",
    "text": "Why Do We Care About Statistical Modeling?\nIn research and practice, you will likely come up with questions or hypotheses of the form:\n\n\n“Are changes in Variable X associated with changes in Variable Y?”\n\n\nWe can apply the models we will talk about in class to our data in order to empirically test our hypotheses (or answer our questions). These models allow us to make data-driven evaluations of our theories."
  },
  {
    "objectID": "lectures/02-lecture-slides.html#our-measures",
    "href": "lectures/02-lecture-slides.html#our-measures",
    "title": "An Introduction to Simple Regression",
    "section": "Our Measures",
    "text": "Our Measures\nYou measure positive attitudes toward AI and frequency of tool use with the following questions:\n\nMuch of society will benefit from a future full of Artificial Intelligence. [Positive Attiude]\nIn the past two months, how frequently have you used the organization’s new AI powered chat bot? [Frequency of Use]"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#schedule-for-today",
    "href": "lectures/03-lecture-slides.html#schedule-for-today",
    "title": "An Introduction to Multiple Regression",
    "section": "Schedule for Today",
    "text": "Schedule for Today\n\nTalk about stats for ~75 mins (5 PM - 6:15 PM ET)\nBreak for 5 minutes\nFinish up stats / R for 40 mins (6:20 PM - 7:00 PM ET)"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#overview",
    "href": "lectures/03-lecture-slides.html#overview",
    "title": "An Introduction to Multiple Regression",
    "section": "Overview",
    "text": "Overview\n\nA quick review of last week\nSetup of our working example\nIntroduction to multiple regression"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#goals",
    "href": "lectures/03-lecture-slides.html#goals",
    "title": "An Introduction to Multiple Regression",
    "section": "Goals",
    "text": "Goals\n\nDevelop an understanding of multiple regression\nWrite your first R script"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#example",
    "href": "lectures/03-lecture-slides.html#example",
    "title": "An Introduction to Multiple Regression",
    "section": "Example",
    "text": "Example\nYour organization has just implemented a new generative AI tool (fancy chat bot) to help improve the efficiency of the organizations sales force. Sales employees, however, have adopted the technology at different rates.\nYou have been asked to determine why employees differ in their rates of adoption. Using the Unified Theory of Acceptance and Use of Technology, you developed a model that states:\n\n\nThe frequency of technology use is related to an employee’s positive attitudes towards AI, their anxiety around technology use, and their intentions to use the new technology."
  },
  {
    "objectID": "lectures/03-lecture-slides.html#measures",
    "href": "lectures/03-lecture-slides.html#measures",
    "title": "An Introduction to Multiple Regression",
    "section": "Measures",
    "text": "Measures\nYou measure these variables with the following questions:\n\nUsing technology such as chatbots makes me anxious. [Technology Anxiety]\n\n1: Completely Disagree to 7: Completely Agree\n\nI intend to use the chatbot to help me with my sales. [Behavioral Intentions]\n\n1: Completely Disagree to 7: Completely Agree\n\nIn the past two months, how frequently have you used the organization’s new AI powered chat bot? [Frequency of Use]\n\n1: Never to 6: All the Time"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#what-is-linear-regression",
    "href": "lectures/03-lecture-slides.html#what-is-linear-regression",
    "title": "An Introduction to Multiple Regression",
    "section": "What is Linear Regression?",
    "text": "What is Linear Regression?\nLinear regression is a statistical model that allows you to test whether a change in a predictor variable, like positive attitudes towards AI, is linearly related to change in an outcome variable, like frequency of AI tool use."
  },
  {
    "objectID": "lectures/03-lecture-slides.html#review-of-simple-regression",
    "href": "lectures/03-lecture-slides.html#review-of-simple-regression",
    "title": "An Introduction to Multiple Regression",
    "section": "Review of Simple Regression",
    "text": "Review of Simple Regression\nThe simple regression model is just a linear regression model with one independent variable.\n\\[Y=\\beta_0+\\beta_1X_1+\\epsilon\\]\n\n\\(\\beta_0\\): The expected value (mean) of Y when X = 0.\n\\(\\beta_1\\): The average change in Y for a one-unit increase in X.\n\\(\\epsilon\\): Variation in Y that is not explained by our model—unexplained variance."
  },
  {
    "objectID": "lectures/03-lecture-slides.html#interpreting-the-regression-slope",
    "href": "lectures/03-lecture-slides.html#interpreting-the-regression-slope",
    "title": "An Introduction to Multiple Regression",
    "section": "Interpreting the Regression Slope",
    "text": "Interpreting the Regression Slope\nThe most appropriate way to interpret the slope coefficient (\\(\\beta_1\\)) is as a comparison:\n\nThe average difference in the frequency with which employees use the AI tool, when comparing two people who differ in their positive attitudes towards AI by one point (e.g. 5: Agree vs 6: Strongly Agree), is equal to the value of \\(\\beta_1\\)."
  },
  {
    "objectID": "lectures/03-lecture-slides.html#ai-adoption-example-problem-statement",
    "href": "lectures/03-lecture-slides.html#ai-adoption-example-problem-statement",
    "title": "An Introduction to Multiple Regression",
    "section": "AI Adoption Example: Problem Statement",
    "text": "AI Adoption Example: Problem Statement\nYour organization has just implemented a new generative AI tool (fancy chat bot) to help improve the efficiency of the organizations sales force. Sales employees, however, have started using the technology at different rates.\nYou have been asked to determine why employees differ in their usage rates."
  },
  {
    "objectID": "lectures/03-lecture-slides.html#ai-adoption-example-hypothesis",
    "href": "lectures/03-lecture-slides.html#ai-adoption-example-hypothesis",
    "title": "An Introduction to Multiple Regression",
    "section": "AI Adoption Example: Hypothesis",
    "text": "AI Adoption Example: Hypothesis\nUsing the Unified Theory of Acceptance and Use of Technology, you develop the following hypotheses:\n\nAn employee’s technology anxiety is negatively related to the frequency with which they use the AI tool.\nAn employee’s intention to use the AI tool is positively related to the frequency with which they use the AI tool."
  },
  {
    "objectID": "lectures/03-lecture-slides.html#determining-the-strength-of-the-regression-slope",
    "href": "lectures/03-lecture-slides.html#determining-the-strength-of-the-regression-slope",
    "title": "An Introduction to Multiple Regression",
    "section": "Determining the Strength of the Regression Slope",
    "text": "Determining the Strength of the Regression Slope\nWe should be careful about using the magnitude of the regression slope to make inferences about the strength of the relationship between the predictor variable and the outcome variable unless we have transformed both our predictor and outcome so that their units are in standard deviations.\nThe magnitude of the regression coefficient is directly related to the scale of the predictor variable, so permissible changes to the scale (e.g. converting hours to minutes or Lbs to Kilograms) will change the magnitude of the regression coefficient."
  },
  {
    "objectID": "lectures/03-lecture-slides.html#understanding-the-overall-fit-of-our-model",
    "href": "lectures/03-lecture-slides.html#understanding-the-overall-fit-of-our-model",
    "title": "An Introduction to Multiple Regression",
    "section": "Understanding the Overall Fit of our Model",
    "text": "Understanding the Overall Fit of our Model\nWe can separate the variance of our outcome into two additive pieces: Variance due to things we have modeled & Variance due to things we have not modeled:\n\\[\\sigma^2_Y=\\sigma^2_{model} + \\sigma^2_{error}\\]\nThe ratio of \\(\\sigma^2_{model}\\) to \\(\\sigma^2_{Y}\\) (\\(\\frac{\\sigma^2_{model}}{\\sigma^2_{Y}}\\)) is equal to the model’s \\(R^2\\), which tells us the proportion of variance in our outcome that is due to our model. The larger the \\(R^2\\), the better our model fits the data."
  },
  {
    "objectID": "lectures/03-lecture-slides.html#the-multiple-regression-model",
    "href": "lectures/03-lecture-slides.html#the-multiple-regression-model",
    "title": "An Introduction to Multiple Regression",
    "section": "The Multiple Regression Model",
    "text": "The Multiple Regression Model\nThe multiple regression model is just a linear regression model with more than one predictor variable.\n\\[Y = \\beta_0 + \\beta_1X_1+\\beta_2X_2+...+\\beta_pX_p+\\epsilon\\]\n\n\\(\\beta_0\\) or Intercept: The expected value (mean) of \\(Y\\) when all \\(X\\) = 0.\n\\(\\beta_p\\) or Partial Regression Coefficient: The average change in \\(Y\\) for a one-unit increase in \\(X_p\\), holding all other \\(Xs\\) constant.\n\\(\\epsilon\\) or Residual: The part of Y that is not explained by our model—unexplained variance."
  },
  {
    "objectID": "lectures/03-lecture-slides.html#why-do-we-use-multiple-regression",
    "href": "lectures/03-lecture-slides.html#why-do-we-use-multiple-regression",
    "title": "An Introduction to Multiple Regression",
    "section": "Why Do We Use Multiple Regression?",
    "text": "Why Do We Use Multiple Regression?\nWe use multiple regression because it allows us to:\n\nEstimate the effect of a predictor variable while controlling for the effects of the other predictor variables in the model.\nTo estimate and test the effects of multiple predictors in a single model.\nAllows us to understand how much of the total variance in our outcome variable we can explain with our predictor variables."
  },
  {
    "objectID": "lectures/03-lecture-slides.html#what-do-we-mean-by-control",
    "href": "lectures/03-lecture-slides.html#what-do-we-mean-by-control",
    "title": "An Introduction to Multiple Regression",
    "section": "What Do We Mean by Control?",
    "text": "What Do We Mean by Control?"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#intepreting-a-partial-regression-coefficient",
    "href": "lectures/03-lecture-slides.html#intepreting-a-partial-regression-coefficient",
    "title": "An Introduction to Multiple Regression",
    "section": "Intepreting a Partial Regression Coefficient",
    "text": "Intepreting a Partial Regression Coefficient\nThe average difference in frequency of AI tool use is 0.5671689 when comparing two employees who have equal levels of technological anxiety but differ in their intention to use the AI tool by one point (unit)."
  },
  {
    "objectID": "lectures/03-lecture-slides.html#visualizing-a-partial-regression-coefficient",
    "href": "lectures/03-lecture-slides.html#visualizing-a-partial-regression-coefficient",
    "title": "An Introduction to Multiple Regression",
    "section": "Visualizing a Partial Regression Coefficient",
    "text": "Visualizing a Partial Regression Coefficient"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#what-happens-if-we-fit-several-simple-regression-models",
    "href": "lectures/03-lecture-slides.html#what-happens-if-we-fit-several-simple-regression-models",
    "title": "An Introduction to Multiple Regression",
    "section": "What Happens if We Fit Several Simple Regression Models?",
    "text": "What Happens if We Fit Several Simple Regression Models?\nAs an example of how our understanding of the relationship between a predictor variable and an outcome variable changes as we include additional predictor variables, let’s look at how the relationships between technological anxiety, intentions to use the AI tool, and frequency of AI tool use change as we move from simple regression to multiple regression."
  },
  {
    "objectID": "lectures/03-lecture-slides.html#model-1-positive-attitudes-prediciting-ai-use",
    "href": "lectures/03-lecture-slides.html#model-1-positive-attitudes-prediciting-ai-use",
    "title": "An Introduction to Multiple Regression",
    "section": "Model 1: Positive Attitudes Prediciting AI Use",
    "text": "Model 1: Positive Attitudes Prediciting AI Use\n\nmodel_1 &lt;- lm(freq_use_ai ~ pos_attitude_ai, data = data_ai)\nsummary(model_1)\n\n\nCall:\nlm(formula = freq_use_ai ~ pos_attitude_ai, data = data_ai)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.9942 -0.7443  0.2557  0.8391  3.5057 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      2.07760    0.04603   45.13   &lt;2e-16 ***\npos_attitude_ai  0.41666    0.01061   39.28   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.224 on 4998 degrees of freedom\nMultiple R-squared:  0.2359,    Adjusted R-squared:  0.2358 \nF-statistic:  1543 on 1 and 4998 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#model-2-technological-anxiety-predicting-ai-use",
    "href": "lectures/03-lecture-slides.html#model-2-technological-anxiety-predicting-ai-use",
    "title": "An Introduction to Multiple Regression",
    "section": "Model 2: Technological Anxiety Predicting AI Use",
    "text": "Model 2: Technological Anxiety Predicting AI Use\n\nmodel_2 &lt;- lm(freq_use_ai ~ tech_anx, data = data_ai)\nsummary(model_2)\n\n\nCall:\nlm(formula = freq_use_ai ~ tech_anx, data = data_ai)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.4698 -0.7707  0.2293  0.9284  2.9284 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  4.81931    0.04424  108.94   &lt;2e-16 ***\ntech_anx    -0.34954    0.01317  -26.55   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.311 on 4998 degrees of freedom\nMultiple R-squared:  0.1236,    Adjusted R-squared:  0.1234 \nF-statistic: 704.9 on 1 and 4998 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#model-3-attitudes-anxiety-predicting-ai-use",
    "href": "lectures/03-lecture-slides.html#model-3-attitudes-anxiety-predicting-ai-use",
    "title": "An Introduction to Multiple Regression",
    "section": "Model 3: Attitudes & Anxiety Predicting AI Use",
    "text": "Model 3: Attitudes & Anxiety Predicting AI Use\n\nmodel_3 &lt;- lm(freq_use_ai ~ pos_attitude_ai + tech_anx, data = data_ai)\nsummary(model_3)\n\n\nCall:\nlm(formula = freq_use_ai ~ pos_attitude_ai + tech_anx, data = data_ai)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.8202 -0.7524  0.2476  0.8917  3.1796 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      2.73606    0.08277  33.057   &lt;2e-16 ***\npos_attitude_ai  0.35595    0.01229  28.961   &lt;2e-16 ***\ntech_anx        -0.13582    0.01424  -9.535   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.214 on 4997 degrees of freedom\nMultiple R-squared:  0.2496,    Adjusted R-squared:  0.2493 \nF-statistic: 830.9 on 2 and 4997 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#omitted-variable-bias-what-happens-when-we-leave-out-a-variable",
    "href": "lectures/03-lecture-slides.html#omitted-variable-bias-what-happens-when-we-leave-out-a-variable",
    "title": "An Introduction to Multiple Regression",
    "section": "Omitted Variable Bias: What Happens When We Leave Out a Variable",
    "text": "Omitted Variable Bias: What Happens When We Leave Out a Variable\nIf we leave out a predictor variable from our model that is related to both the predictor variable we included in our model and our outcome variable, then we should almost always find a way to include the left out variable.\nIf we do not include this variable, we commit the omitted variable bias, where the effect of the omitted predictor variable on the outcome variable gets mixed together with the effect of the included predictor variable."
  },
  {
    "objectID": "lectures/03-lecture-slides.html#algebra-behind-ommitted-variable-bias",
    "href": "lectures/03-lecture-slides.html#algebra-behind-ommitted-variable-bias",
    "title": "An Introduction to Multiple Regression",
    "section": "Algebra Behind Ommitted Variable Bias",
    "text": "Algebra Behind Ommitted Variable Bias\n\\[X_{\\text{Tech Anxiey}}=\\alpha_0 + \\alpha_1X_{\\text{AI Attitude}}\\]\n\\[Y_{\\text{AI Use}}=\\beta_0 + \\beta_1X_{AI Attitude}+\\beta_2X_{\\text{Tech Anxiety}}\\]\n\\[Y_{\\text{AI Use}}=\\beta_0 + \\beta_1X_{AI Attitude}+\\beta_2(\\alpha_0 + \\alpha_1X_{\\text{AI Attitude}})\\]\n\\[Y_{\\text{AI Use}}=(\\beta_0 + \\beta_2*\\alpha_0) + (\\beta_1+\\beta_2*\\alpha_1)X_{AI Attitude}\\]"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#prediction-error-in-multiple-regression",
    "href": "lectures/03-lecture-slides.html#prediction-error-in-multiple-regression",
    "title": "An Introduction to Multiple Regression",
    "section": "Prediction & Error in Multiple Regression",
    "text": "Prediction & Error in Multiple Regression"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#judging-the-strength-of-a-partial-regression-coefficient",
    "href": "lectures/03-lecture-slides.html#judging-the-strength-of-a-partial-regression-coefficient",
    "title": "An Introduction to Multiple Regression",
    "section": "Judging the Strength of a Partial Regression Coefficient",
    "text": "Judging the Strength of a Partial Regression Coefficient"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#estimating-our-full-model",
    "href": "lectures/03-lecture-slides.html#estimating-our-full-model",
    "title": "An Introduction to Multiple Regression",
    "section": "Estimating Our Full Model",
    "text": "Estimating Our Full Model\n\nmodel_full &lt;- lm(freq_use_ai ~ tech_anx + beh_intent_ai,\n                 data = data_ai)\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n1.342\n0.070\n19.090\n0.000\n\n\ntech_anx\n-0.014\n0.012\n-1.132\n0.258\n\n\nbeh_intent_ai\n0.567\n0.010\n56.349\n0.000"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#model-1-technological-anxiety-prediciting-ai-use",
    "href": "lectures/03-lecture-slides.html#model-1-technological-anxiety-prediciting-ai-use",
    "title": "An Introduction to Multiple Regression",
    "section": "Model 1: Technological Anxiety Prediciting AI Use",
    "text": "Model 1: Technological Anxiety Prediciting AI Use\nShould we conclude that technological anxiety is significantly and negatively related to frequency of AI tool use?\n\nmodel_1 &lt;- lm(freq_use_ai ~ tech_anx, data = data_ai)\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n4.740\n0.046\n102.682\n0\n\n\ntech_anx\n-0.322\n0.014\n-23.144\n0"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#model-2-behavioral-intentions-predicting-ai-use",
    "href": "lectures/03-lecture-slides.html#model-2-behavioral-intentions-predicting-ai-use",
    "title": "An Introduction to Multiple Regression",
    "section": "Model 2: Behavioral Intentions Predicting AI Use",
    "text": "Model 2: Behavioral Intentions Predicting AI Use\nShould we conclude that intention to use the AI tool is significantly and positively related to frequency of AI tool use?\n\nmodel_2 &lt;- lm(freq_use_ai ~ beh_intent_ai, data = data_ai)\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n1.278\n0.042\n30.665\n0\n\n\nbeh_intent_ai\n0.572\n0.009\n63.636\n0"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#model-3-anxiety-intentions-predicting-ai-use",
    "href": "lectures/03-lecture-slides.html#model-3-anxiety-intentions-predicting-ai-use",
    "title": "An Introduction to Multiple Regression",
    "section": "Model 3: Anxiety & Intentions Predicting AI Use",
    "text": "Model 3: Anxiety & Intentions Predicting AI Use\nHow do our previous conclusions change? What happened to technological anxiety?\n\nmodel_3 &lt;- lm(freq_use_ai ~ tech_anx + beh_intent_ai, data = data_ai)\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n1.342\n0.070\n19.090\n0.000\n\n\ntech_anx\n-0.014\n0.012\n-1.132\n0.258\n\n\nbeh_intent_ai\n0.567\n0.010\n56.349\n0.000"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#algebra-behind-the-ommitted-variable-bias",
    "href": "lectures/03-lecture-slides.html#algebra-behind-the-ommitted-variable-bias",
    "title": "An Introduction to Multiple Regression",
    "section": "Algebra Behind the Ommitted Variable Bias",
    "text": "Algebra Behind the Ommitted Variable Bias\n\\[X_{\\text{Behavioral Intent.}}=\\alpha_0 + \\alpha_1X_{\\text{Tech. Anxiety}}\\]"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#multicollinearity-problems-when-our-predictors-are-correlated",
    "href": "lectures/03-lecture-slides.html#multicollinearity-problems-when-our-predictors-are-correlated",
    "title": "An Introduction to Multiple Regression",
    "section": "Multicollinearity: Problems when our Predictors are Correlated",
    "text": "Multicollinearity: Problems when our Predictors are Correlated"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#algebra-behind-the-ommitted-variable-bias-1",
    "href": "lectures/03-lecture-slides.html#algebra-behind-the-ommitted-variable-bias-1",
    "title": "An Introduction to Multiple Regression",
    "section": "Algebra Behind the Ommitted Variable Bias",
    "text": "Algebra Behind the Ommitted Variable Bias\n\\[X_{\\text{Behavioral Intent.}}=\\alpha_0 + \\alpha_1X_{\\text{Tech. Anxiety}}\\]\n\\[Y_{\\text{AI Use}}=\\beta_0 + \\beta_1X_{\\text{Tech. Anxiety}}+\\beta_2X_{\\text{Behavioral Intent.}}\\]"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#algebra-behind-the-ommitted-variable-bias-2",
    "href": "lectures/03-lecture-slides.html#algebra-behind-the-ommitted-variable-bias-2",
    "title": "An Introduction to Multiple Regression",
    "section": "Algebra Behind the Ommitted Variable Bias",
    "text": "Algebra Behind the Ommitted Variable Bias\n\\[X_{\\text{Behavioral Intent.}}=\\alpha_0 + \\alpha_1X_{\\text{Tech. Anxiety}}\\]\n\\[Y_{\\text{AI Use}}=\\beta_0 + \\beta_1X_{\\text{Tech. Anxiety}}+\\beta_2X_{\\text{Behavioral Intent.}}\\]\n\\[Y_{\\text{AI Use}}=\\beta_0 + \\beta_1X_{\\text{Tech. Anxiety}}+\\beta_2(\\alpha_0 + \\alpha_1X_{\\text{Tech. Anxiety}})\\]"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#algebra-behind-the-ommitted-variable-bias-3",
    "href": "lectures/03-lecture-slides.html#algebra-behind-the-ommitted-variable-bias-3",
    "title": "An Introduction to Multiple Regression",
    "section": "Algebra Behind the Ommitted Variable Bias",
    "text": "Algebra Behind the Ommitted Variable Bias\n\\[X_{\\text{Behavioral Intent.}}=\\alpha_0 + \\alpha_1X_{\\text{Tech. Anxiety}}\\]\n\\[Y_{\\text{AI Use}}=\\beta_0 + \\beta_1X_{\\text{Tech. Anxiety}}+\\beta_2X_{\\text{Behavioral Intent.}}\\]\n\\[Y_{\\text{AI Use}}=\\beta_0 + \\beta_1X_{\\text{Tech. Anxiety}}+\\beta_2(\\alpha_0 + \\alpha_1X_{\\text{Tech. Anxiety}})\\]\n\\[Y_{\\text{AI Use}}=(\\beta_0 + \\beta_2*\\alpha_0) + (\\beta_1+\\beta_2*\\alpha_1)X_{\\text{Tech. Anxiety}}\\]"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#what-do-we-mean-by-statistical-control",
    "href": "lectures/03-lecture-slides.html#what-do-we-mean-by-statistical-control",
    "title": "An Introduction to Multiple Regression",
    "section": "What Do We Mean by Statistical Control?",
    "text": "What Do We Mean by Statistical Control?\nStatistical control is a fancy way of saying “what is the effect of a predictor variable on an outcome variable for people (or units) who have the same measurements on all of the other predictor variables in the model?”\n\nWhat effect does an individual’s intention to use the AI tool have on the frequency they use an AI tool, if we hold their level of technological anxiety constant?"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#an-example-of-statistical-control",
    "href": "lectures/03-lecture-slides.html#an-example-of-statistical-control",
    "title": "An Introduction to Multiple Regression",
    "section": "An Example of Statistical Control",
    "text": "An Example of Statistical Control\nConceptually, if we wanted to estimate the relationship that intention to use the AI tool has on the frequency of AI tool use, while controlling (adjusting) for the level of technological anxiety, we could create multiple new datasets from our original dataset where each new dataset had a fixed level of technological anxiety (e.g. in the first dataset, we would only look at observtations where tech_anxiety == 1).\nThen we could estimate a simple regression model that esimates the relationship between intentions to use the AI tool (beh_intent_ai) and frequency of tool use (freq_use_ai) to each dataset. The average of those seven regression slopes (one for each dataset) would be nearly equivalent to the partial coefficient estimated from a multiple regression model that included beh_intent_ai and tech_anxiety."
  },
  {
    "objectID": "lectures/03-lecture-slides.html#why-do-we-control",
    "href": "lectures/03-lecture-slides.html#why-do-we-control",
    "title": "An Introduction to Multiple Regression",
    "section": "Why Do We Control",
    "text": "Why Do We Control"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#understanding-prediction-in-multiple-regression",
    "href": "lectures/03-lecture-slides.html#understanding-prediction-in-multiple-regression",
    "title": "An Introduction to Multiple Regression",
    "section": "Understanding Prediction in Multiple Regression",
    "text": "Understanding Prediction in Multiple Regression\nSimilar to simple regression, we can decompose the observed value of the outcome variable, \\(Y\\), into a model component and an error component. For multiple regression, the prediction is a linear combination of all \\(p\\) predictor variables.\n\\[\\hat{Y}_i=\\hat{\\beta}_0+\\hat{\\beta}_1X_{1}+\\hat{\\beta}_2X_{2}+...+\\hat{\\beta}_{p}\\]"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#understanding-error-in-multiple-regression",
    "href": "lectures/03-lecture-slides.html#understanding-error-in-multiple-regression",
    "title": "An Introduction to Multiple Regression",
    "section": "Understanding Error in Multiple Regression",
    "text": "Understanding Error in Multiple Regression\nRemember to think of error as anything that is not predicted (or modeled) by our model. This could be random noise as well as other predictor variables that we did not measure.\n\\[\\hat{e}=Y_i-\\hat{Y}_i\\] \\[\\hat{e}=\\text{Observed} - \\text{Predicted}\\]"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#judging-overall-model-strength-the-multiple-r-and-r-squared",
    "href": "lectures/03-lecture-slides.html#judging-overall-model-strength-the-multiple-r-and-r-squared",
    "title": "An Introduction to Multiple Regression",
    "section": "Judging Overall Model Strength: The Multiple R and R-Squared",
    "text": "Judging Overall Model Strength: The Multiple R and R-Squared"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#standardized-partial-regression-coefficient",
    "href": "lectures/03-lecture-slides.html#standardized-partial-regression-coefficient",
    "title": "An Introduction to Multiple Regression",
    "section": "Standardized Partial Regression Coefficient",
    "text": "Standardized Partial Regression Coefficient\nIf we put all of our predictor variables on the same scale, then we can make relative comparisons. One way to do this is by standardizing all of our predictor variables:\n\\[Z_{p}=\\frac{X_p-\\overline{X}_p}{SD(X_p)}\\]\nThe scale for each predictor variable would then be in standard deviation units."
  },
  {
    "objectID": "lectures/03-lecture-slides.html#semipartial-correlation",
    "href": "lectures/03-lecture-slides.html#semipartial-correlation",
    "title": "An Introduction to Multiple Regression",
    "section": "Semipartial Correlation",
    "text": "Semipartial Correlation"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#understanding-the-semipartial-correlation",
    "href": "lectures/03-lecture-slides.html#understanding-the-semipartial-correlation",
    "title": "An Introduction to Multiple Regression",
    "section": "Understanding the Semipartial Correlation",
    "text": "Understanding the Semipartial Correlation"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#should-you-use-the-standardized-partial-coefficeint-or-semipartial-correlation",
    "href": "lectures/03-lecture-slides.html#should-you-use-the-standardized-partial-coefficeint-or-semipartial-correlation",
    "title": "An Introduction to Multiple Regression",
    "section": "Should You Use the Standardized Partial Coefficeint or Semipartial Correlation?",
    "text": "Should You Use the Standardized Partial Coefficeint or Semipartial Correlation?"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#why-do-we-inlcude-multiple-predictors",
    "href": "lectures/03-lecture-slides.html#why-do-we-inlcude-multiple-predictors",
    "title": "An Introduction to Multiple Regression",
    "section": "Why Do We Inlcude Multiple Predictors?",
    "text": "Why Do We Inlcude Multiple Predictors?"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#visualize-control",
    "href": "lectures/03-lecture-slides.html#visualize-control",
    "title": "An Introduction to Multiple Regression",
    "section": "Visualize Control",
    "text": "Visualize Control"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#visualize-control-1",
    "href": "lectures/03-lecture-slides.html#visualize-control-1",
    "title": "An Introduction to Multiple Regression",
    "section": "Visualize Control",
    "text": "Visualize Control"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#an-example-of-statistical-control-example-dataset-1",
    "href": "lectures/03-lecture-slides.html#an-example-of-statistical-control-example-dataset-1",
    "title": "An Introduction to Multiple Regression",
    "section": "An Example of Statistical Control: Example Dataset 1",
    "text": "An Example of Statistical Control: Example Dataset 1\n\ndata_subset_1 &lt;- data_ai |&gt; dplyr::filter(tech_anx == 1)\n\n\n\n# A tibble: 721 × 4\n   employee_id freq_use_ai tech_anx beh_intent_ai\n   &lt;fct&gt;             &lt;dbl&gt;    &lt;dbl&gt;         &lt;dbl&gt;\n 1 760406                3        1             5\n 2 778236                5        1             4\n 3 339526                4        1             7\n 4 161547                4        1             5\n 5 708206                4        1             3\n 6 900585                5        1             5\n 7 558405                5        1             7\n 8 869289                5        1             5\n 9 739811                5        1             5\n10 584814                3        1             5\n# ℹ 711 more rows"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#visualizing-statistical-control",
    "href": "lectures/03-lecture-slides.html#visualizing-statistical-control",
    "title": "An Introduction to Multiple Regression",
    "section": "Visualizing Statistical Control",
    "text": "Visualizing Statistical Control\nThe plot below shows seven different regression lines each of which estimate the effect that one’s intention to use an AI tool has on the frequency with which they actually use the AI tool at a fixed level of technological anxiety (e.g. tech_anx == 1 or tech_anx == 2)."
  },
  {
    "objectID": "lectures/03-lecture-slides.html#visualizing-statistical-control-1",
    "href": "lectures/03-lecture-slides.html#visualizing-statistical-control-1",
    "title": "An Introduction to Multiple Regression",
    "section": "Visualizing Statistical Control",
    "text": "Visualizing Statistical Control\nThe plot below shows the same seven simple regression lines, but now the slope of the partial regression coefficient for the effect of intention to use the AI tool on frequency of tool use controlling for technological anxiety is laid over the simple regression lines."
  },
  {
    "objectID": "lectures/03-lecture-slides.html#why-do-we-include-multiple-predictor-variables",
    "href": "lectures/03-lecture-slides.html#why-do-we-include-multiple-predictor-variables",
    "title": "An Introduction to Multiple Regression",
    "section": "Why Do We Include Multiple Predictor Variables?",
    "text": "Why Do We Include Multiple Predictor Variables?\nThere are two main reasons to include additional predictor variables in your regression model:\n\nThe effect of a predictor variable on an outcome variable in a simple regression model may be incorrectly estimated if we do not take into consideration other predictor variables.\nBy adding additional predictor variables into our model, we reduce the error component of our model, which makes it more likely we will find a true significant relationship between our predictor variables and outcome variable."
  },
  {
    "objectID": "lectures/03-lecture-slides.html#omitted-variable-bias-in-our-data",
    "href": "lectures/03-lecture-slides.html#omitted-variable-bias-in-our-data",
    "title": "An Introduction to Multiple Regression",
    "section": "Omitted Variable Bias in Our Data",
    "text": "Omitted Variable Bias in Our Data\nThis is what happens with tech_anxiety and beh_intent_ai. When we leave out beh_intent_ai in our model, part of its effect on freq_use_ai gets mixed into the effect that tech_anx has on freq_use_ai.\n\n\n\nlm(beh_intent_ai ~ tech_anx, data_ai)\n\n\n\n\n\n\nterm\nestimate\np.value\n\n\n\n\n(Intercept)\n5.99\n0\n\n\ntech_anx\n-0.54\n0\n\n\n\n\n\n\nRight column\n\nRight column"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#interpreting-a-partial-regression-coefficient",
    "href": "lectures/03-lecture-slides.html#interpreting-a-partial-regression-coefficient",
    "title": "An Introduction to Multiple Regression",
    "section": "Interpreting a Partial Regression Coefficient",
    "text": "Interpreting a Partial Regression Coefficient\nLike with simple regression, the most appropriate interpretation of a partial regression coefficient is as a mean comparison between two groups that differ on their predictor variable by one point, but have identical values for the other predictor variables."
  },
  {
    "objectID": "lectures/03-lecture-slides.html#interpreting-the-partial-coefficient-for-behavioral-intentions",
    "href": "lectures/03-lecture-slides.html#interpreting-the-partial-coefficient-for-behavioral-intentions",
    "title": "An Introduction to Multiple Regression",
    "section": "Interpreting the Partial Coefficient for Behavioral Intentions",
    "text": "Interpreting the Partial Coefficient for Behavioral Intentions\nThe average difference in frequency of AI tool use is 0.57 points when comparing two employees who have equal levels of technological anxiety but differ in their intention to use the AI tool by one point (unit)."
  },
  {
    "objectID": "lectures/03-lecture-slides.html#predicting-frequency-of-ai-tool-use",
    "href": "lectures/03-lecture-slides.html#predicting-frequency-of-ai-tool-use",
    "title": "An Introduction to Multiple Regression",
    "section": "Predicting Frequency of AI Tool Use",
    "text": "Predicting Frequency of AI Tool Use\nOur best prediction for an employee who is highly anxious about technology (tech_anx == 7) and does not intend to use the AI tool (beh_intent_ai == 1):\n\\[1.84 = 1.34 + -.01_{\\text{Tech. Anx.}}*7+.57_{\\text{Beh. Int.}}*1\\]\nOur best prediction for an employee who is not anxious about technology (tech_anx == 1) and intends to use the AI tool (beh_intent_ai == 7):\n\\[5.32 = 1.34 + -.01_{\\text{Tech. Anx.}}*1+.57_{\\text{Beh. Int.}}*7\\]"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#error-in-our-ai-model",
    "href": "lectures/03-lecture-slides.html#error-in-our-ai-model",
    "title": "An Introduction to Multiple Regression",
    "section": "Error in Our AI Model",
    "text": "Error in Our AI Model\nBelow is a data frame that contains a sample of some of the errors our model made. How would you interpret a negative error? A positive error?\n\n\n# A tibble: 10 × 6\n   employee_id tech_anx beh_intent_ai freq_use_ai predicted_freq_use_ai error\n   &lt;fct&gt;          &lt;dbl&gt;         &lt;dbl&gt;       &lt;dbl&gt;                 &lt;dbl&gt; &lt;dbl&gt;\n 1 381064             1             7           4                  5.3  -1.3 \n 2 338015             7             1           1                  1.81 -0.81\n 3 703375             1             7           6                  5.3   0.7 \n 4 611741             1             7           6                  5.3   0.7 \n 5 812441             1             7           5                  5.3  -0.3 \n 6 475459             1             7           5                  5.3  -0.3 \n 7 921092             1             7           5                  5.3  -0.3 \n 8 567786             1             7           5                  5.3  -0.3 \n 9 680605             1             7           5                  5.3  -0.3 \n10 239815             7             1           2                  1.81  0.19"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#judging-overall-model-strength-the-multiple-correlation",
    "href": "lectures/03-lecture-slides.html#judging-overall-model-strength-the-multiple-correlation",
    "title": "An Introduction to Multiple Regression",
    "section": "Judging Overall Model Strength: The Multiple Correlation",
    "text": "Judging Overall Model Strength: The Multiple Correlation\nThe multiple correlation coefficient, \\(R\\), is the correlation between our model prediction, \\(\\hat{Y}\\) and our observed outcome variable, \\(Y\\):\n\\[\\text{Multiple Corelation = }r_{Y,\\hat{Y}}\\]"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#the-muliple-correlation-in-our-ai-data",
    "href": "lectures/03-lecture-slides.html#the-muliple-correlation-in-our-ai-data",
    "title": "An Introduction to Multiple Regression",
    "section": "The Muliple Correlation in Our AI Data",
    "text": "The Muliple Correlation in Our AI Data\n\ndata_ai_pred &lt;-\n  data_ai |&gt;\n  dplyr::mutate(\n    pred_y = predict(model_3)\n  )\n\n\n\nHere is a look at our data frame which contains the actual and predicted values of freq_use_ai:\n\n\n# A tibble: 5,000 × 2\n   freq_use_ai pred_y\n         &lt;dbl&gt;  &lt;dbl&gt;\n 1           5   4.69\n 2           4   4.11\n 3           4   3.57\n 4           4   5.27\n 5           1   1.87\n 6           4   3.57\n 7           5   2.42\n 8           3   4.16\n 9           5   3.60\n10           5   3.57\n# ℹ 4,990 more rows\n\n\n\nHere is the correlation between our actual and predicted values:\n\n\n            freq_use_ai pred_y\nfreq_use_ai       1.000  0.669\npred_y            0.669  1.000"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#judging-overall-model-strength-r-squared",
    "href": "lectures/03-lecture-slides.html#judging-overall-model-strength-r-squared",
    "title": "An Introduction to Multiple Regression",
    "section": "Judging Overall Model Strength: R-Squared",
    "text": "Judging Overall Model Strength: R-Squared\nSimilar to simple regression, we can calculate an \\(R^2\\) value which will tell us the proportion of variance in our outcome variable that is explained by all of our predictor variables:\n\n\\(R^2\\) is the square of the multiple correlation coefficient.\n\\(R^2\\) falls between 0 (no prediction) and 1 (perfect prediction).\nThe larger the value of \\(R^2\\), the better the set of predictor variables collectively predicts \\(Y\\).\n\\(R^2\\) will equal 0 only when all of the partial regression coefficients equal 0.\n\\(R^2\\) will never decrease when a new predictor variable is added to a model."
  },
  {
    "objectID": "lectures/03-lecture-slides.html#the-r-squared-in-our-ai-data",
    "href": "lectures/03-lecture-slides.html#the-r-squared-in-our-ai-data",
    "title": "An Introduction to Multiple Regression",
    "section": "The R-Squared in Our AI Data",
    "text": "The R-Squared in Our AI Data\nThe multiple correlation is equal to 0.669, which is equal to 0.4477 when squared.\n\n\n\nCall:\nlm(formula = freq_use_ai ~ tech_anx + beh_intent_ai, data = data_ai)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.7311 -0.6897 -0.1087  0.8499  3.1599 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    1.34183    0.07029  19.090   &lt;2e-16 ***\ntech_anx      -0.01379    0.01218  -1.132    0.258    \nbeh_intent_ai  0.56717    0.01007  56.349   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.029 on 4997 degrees of freedom\nMultiple R-squared:  0.4477,    Adjusted R-squared:  0.4475 \nF-statistic:  2025 on 2 and 4997 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#judging-the-relative-strength-of-a-partial-regression-coefficient",
    "href": "lectures/03-lecture-slides.html#judging-the-relative-strength-of-a-partial-regression-coefficient",
    "title": "An Introduction to Multiple Regression",
    "section": "Judging the Relative Strength of a Partial Regression Coefficient",
    "text": "Judging the Relative Strength of a Partial Regression Coefficient\nBecause the magnitude of the partial regression coefficient depends on the scale of its predictor variable, we cannot compare the partial regression coefficient of one predictor variable to the partial regression of another predictor variable unless both predictor variables have identical scales."
  },
  {
    "objectID": "lectures/03-lecture-slides.html#comparing-the-effects-of-tech-anxiety-behavioral-intentions",
    "href": "lectures/03-lecture-slides.html#comparing-the-effects-of-tech-anxiety-behavioral-intentions",
    "title": "An Introduction to Multiple Regression",
    "section": "Comparing the Effects of Tech Anxiety & Behavioral Intentions",
    "text": "Comparing the Effects of Tech Anxiety & Behavioral Intentions\nIf we rescale the technological anxiety scale so that instead of going from 1 to 7 by 1 unit intervals it goes from 0 to .06 by .01 intervals, then we get the following results:\n\n\n\n# A tibble: 7 × 2\n  tech_anx tech_anx_rescale\n     &lt;dbl&gt;            &lt;dbl&gt;\n1        1             0   \n2        2             0.01\n3        3             0.02\n4        4             0.03\n5        5             0.04\n6        6             0.05\n7        7             0.06\n\n\n\n\n\nOriginal tech_anx scale:\n\n\n# A tibble: 3 × 5\n  term          estimate    se statistic p.value\n  &lt;chr&gt;            &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)       1.34 0.07      19.1    0    \n2 tech_anx         -0.01 0.012     -1.13   0.258\n3 beh_intent_ai     0.57 0.01      56.3    0    \n\n\n\nRescaled tech_anx scale:\n\n\n# A tibble: 3 × 5\n  term             estimate    se statistic p.value\n  &lt;chr&gt;               &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)          1.33 0.061     21.8    0    \n2 tech_anx_rescale    -1.38 1.22      -1.13   0.258\n3 beh_intent_ai        0.57 0.01      56.3    0"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#rescale-technological-anxiety",
    "href": "lectures/03-lecture-slides.html#rescale-technological-anxiety",
    "title": "An Introduction to Multiple Regression",
    "section": "Rescale Technological Anxiety",
    "text": "Rescale Technological Anxiety\nIf we rescale the technological anxiety scale so that instead of going from 1 to 7 it goes from 0 to .06 by .01, then we get the following results:\n\n\n# A tibble: 7 × 2\n  tech_anx tech_anx_rescale\n     &lt;dbl&gt;            &lt;dbl&gt;\n1        1             0   \n2        2             0.01\n3        3             0.02\n4        4             0.03\n5        5             0.04\n6        6             0.05\n7        7             0.06\n\n\n\n\nOriginal tech_anx scale:\n\n\n\n\n\nterm\nestimate\nse\nt.stat\np.value\n\n\n\n\n(Intercept)\n1.34\n0.07\n19.09\n0.00\n\n\ntech_anx\n-0.01\n0.01\n-1.13\n0.26\n\n\nbeh_intent_ai\n0.57\n0.01\n56.35\n0.00\n\n\n\n\n\n\nRescaled tech_anx scale:\n\n\n\n\n\nterm\nestimate\nse\nt.stat\np.value\n\n\n\n\n(Intercept)\n1.33\n0.06\n21.80\n0.00\n\n\ntech_anx_rescale\n-1.38\n1.22\n-1.13\n0.26\n\n\nbeh_intent_ai\n0.57\n0.01\n56.35\n0.00"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#standardizing-our-predictor-variables",
    "href": "lectures/03-lecture-slides.html#standardizing-our-predictor-variables",
    "title": "An Introduction to Multiple Regression",
    "section": "Standardizing Our Predictor Variables",
    "text": "Standardizing Our Predictor Variables\nIf we standardize our predictor variables and our outcome variable, we get the following results:\n\n\n\n\n# A tibble: 7 × 3\n  original_scale tech_anx_scale beh_intent_ai_scale\n           &lt;dbl&gt;          &lt;dbl&gt;               &lt;dbl&gt;\n1              1        -1.52                -2.06 \n2              2        -0.774               -1.45 \n3              3        -0.0263              -0.829\n4              4         0.722               -0.211\n5              5         1.47                 0.407\n6              6         2.22                 1.02 \n7              7         2.97                 1.64 \n\n\n\n\n\n\n\n\nterm\nestimate\nse\np.value\n\n\n\n\n(Intercept)\n0.00\n0.01\n1.00\n\n\ntech_anx_scale\n-0.01\n0.01\n0.26\n\n\nbeh_intent_ai_scale\n0.66\n0.01\n0.00"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#understanding-the-squared-semipartial-correlation",
    "href": "lectures/03-lecture-slides.html#understanding-the-squared-semipartial-correlation",
    "title": "An Introduction to Multiple Regression",
    "section": "Understanding the Squared Semipartial Correlation",
    "text": "Understanding the Squared Semipartial Correlation\nAnother way to compare relative effects is the squared semipartial correlation: \\(sr_{j}^2\\).\nThe squared semipartial correlation tells us how much unique variance a predictor variable explains in an outcome variable when controlling for all the other predictor variables.\n\\[sr^2_p=R^2_{Y. X_1...X_p}-R^2_{Y.X_1...X_{p-1}}\\]"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#semipartial-correlation-with-our-ai-data",
    "href": "lectures/03-lecture-slides.html#semipartial-correlation-with-our-ai-data",
    "title": "An Introduction to Multiple Regression",
    "section": "Semipartial Correlation with our AI Data",
    "text": "Semipartial Correlation with our AI Data\n\n\n\n\n\nMetric\nValue\nR-Squared % Change\n\n\n\n\nOverall R-Squared\n0.450\n0\n\n\nSemipartial r-squared: Behvioral Intent.\n0.351\n355\n\n\nSemipartial r-squared: Tech. Anx.\n0.000\n0"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#understanding-statistical-control",
    "href": "lectures/03-lecture-slides.html#understanding-statistical-control",
    "title": "An Introduction to Multiple Regression",
    "section": "Understanding Statistical Control",
    "text": "Understanding Statistical Control\nIf we wanted to estimate the relationship that intention to use the AI tool has on the frequency of AI tool use, while controlling (adjusting) for the level of technological anxiety, we could create multiple new data sets from our original dataset where each new dataset had a fixed level of technological anxiety (e.g. one dataset, where tech_anxiety == 1).\nWe could then estimate the relationship between intentions to use the AI tool and frequency of tool use to each dataset. The average of those estimated slopes (relationships) would be nearly equivalent to the partial coefficient estimated from a multiple regression model that included beh_intent_ai and tech_anxiety."
  },
  {
    "objectID": "lectures/03-lecture-slides.html#should-you-use-the-standardized-partial-coefficient-or-squared-semipartial-correlation",
    "href": "lectures/03-lecture-slides.html#should-you-use-the-standardized-partial-coefficient-or-squared-semipartial-correlation",
    "title": "An Introduction to Multiple Regression",
    "section": "Should You Use the Standardized Partial Coefficient or Squared Semipartial Correlation?",
    "text": "Should You Use the Standardized Partial Coefficient or Squared Semipartial Correlation?\nPersonally, I do not think you can go wrong either way, but the best practice would be to use the squared semipartial correlation to compare the relative importance of predictor variables."
  },
  {
    "objectID": "lectures/03-lecture-page.html",
    "href": "lectures/03-lecture-page.html",
    "title": "Quantitative Analysis 1",
    "section": "",
    "text": "Next Week’s Materials »"
  },
  {
    "objectID": "lectures/03-lecture-page.html#lecture-introduction-to-simple-regression",
    "href": "lectures/03-lecture-page.html#lecture-introduction-to-simple-regression",
    "title": "Quantitative Analysis 1",
    "section": "Lecture: Introduction to Simple Regression",
    "text": "Lecture: Introduction to Simple Regression\n\n\nTo download a pdf version of these slides, click here.\nTo download the R script that follows the R portion of the lecture, click here."
  },
  {
    "objectID": "lectures/03-lecture-page.html#lecture-introduction-to-multiple-regression",
    "href": "lectures/03-lecture-page.html#lecture-introduction-to-multiple-regression",
    "title": "Quantitative Analysis 1",
    "section": "Lecture: Introduction to Multiple Regression",
    "text": "Lecture: Introduction to Multiple Regression\n\n\nTo download a pdf version of these slides, click here.\nTo download the R script that follows the R portion of the lecture, click here."
  },
  {
    "objectID": "lectures/03-lecture-slides.html#model-1-technological-anxiety-and-ai-use",
    "href": "lectures/03-lecture-slides.html#model-1-technological-anxiety-and-ai-use",
    "title": "An Introduction to Multiple Regression",
    "section": "Model 1: Technological Anxiety and AI Use",
    "text": "Model 1: Technological Anxiety and AI Use\nShould we conclude that technological anxiety is significantly and negatively related to frequency of AI tool use?\n\nmodel_1 &lt;- lm(freq_use_ai ~ tech_anx, data = data_ai)\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n4.740\n0.046\n102.682\n0\n\n\ntech_anx\n-0.322\n0.014\n-23.144\n0"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#model-2-behavioral-intentions-and-ai-use",
    "href": "lectures/03-lecture-slides.html#model-2-behavioral-intentions-and-ai-use",
    "title": "An Introduction to Multiple Regression",
    "section": "Model 2: Behavioral Intentions and AI Use",
    "text": "Model 2: Behavioral Intentions and AI Use\nShould we conclude that intention to use the AI tool is significantly and positively related to frequency of AI tool use?\n\nmodel_2 &lt;- lm(freq_use_ai ~ beh_intent_ai, data = data_ai)\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n1.278\n0.042\n30.665\n0\n\n\nbeh_intent_ai\n0.572\n0.009\n63.636\n0"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#model-3-anxiety-intentions-and-ai-use",
    "href": "lectures/03-lecture-slides.html#model-3-anxiety-intentions-and-ai-use",
    "title": "An Introduction to Multiple Regression",
    "section": "Model 3: Anxiety & Intentions and AI Use",
    "text": "Model 3: Anxiety & Intentions and AI Use\nHow do our previous conclusions change? What happened to technological anxiety?\n\nmodel_3 &lt;- lm(freq_use_ai ~ tech_anx + beh_intent_ai, data = data_ai)\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n1.342\n0.070\n19.090\n0.000\n\n\ntech_anx\n-0.014\n0.012\n-1.132\n0.258\n\n\nbeh_intent_ai\n0.567\n0.010\n56.349\n0.000"
  }
]
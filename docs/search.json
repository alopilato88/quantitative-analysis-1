[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Quantitative Analysis 1",
    "section": "",
    "text": "Welcome to the homepage for Quantitative Analysis 1 (PHD1502-1)!"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Course Syllabus",
    "section": "",
    "text": "To download a pdf version of this syllabus, click here.\nMeeting Time: Tuesdays, 5 PM to 7 PM ET\nLocation: Zoom Meeting and Smith 307 when in person\nEmail: alex.lopilato@gmail.com (for quick responses) or alopilato@bentley.edu (for discussions about grades, personal info, etc.)\nOffice Hours: By request (will likely be virtual as I do not have an office on campus)\nCourse Format: Hybrid Synchronous\n\nCourse Description\nThis course focuses on applications of linear regression to model data collected from observational, quasi-experimental, and experimental study designs. This course will introduce students to the basics of linear regression and its applications to model and theory building.\n\n\nCourse Objectives\nBy the end of this course, you will:\n\nHave an understanding of the general linear model.\nHave an understanding of how to use linear models to empirically test theoretical models.\nHave an understanding of how to use linear regression in your own research.\nFeel comfortable using R to estimate linear models.\n\n\n\nTextbooks\n\nDarlington, A. B. & Hayes, A. F. (2016). Regression Analysis and Linear Models: Concepts, Applications, and Implementation (Referred to as RAGLM)\nBékés, G. & Kézdi, G. (2021). Data Analysis for Business Economics, and Policy. (Referred to as DBEP)\n\n\n\nCourse Technology\nThis course will use Brightspace to post important updates and Zoom recordings. Please do not use Brightspace to email me! Use either of the emails listed above.\n\nCourse Website\nThe website for the course is: https://alopilato88.github.io/quantitative-analysis-1/. All of the lectures can be found there and will be made publicly available on the day of the lecture.\n\n\nStatistical Computing\nThis course will rely solely on the R programming language for all statistical computing. At the very least, you will need to download R to your local machine (or use your lab computer), and I highly recommend also downloading RStudio, which is an Integrated Development Environment (IDE) that makes programming in R (and other programming languages) much easier. Please reach out to me if you are unable to install R.\nWhile you can use another statistical software program such as SPSS, SAS, or STATA, I will not be providing example code for those different programs. I will only be providing example R code.\n\n\n\nGrading Criteria\nA combination of homework and a final research project will be used to determine your grade for this course. Homework will account for 90% of your grade and the research project will account for 10%. While I encourage you to consult with your colleagues (your instructor, classmates, professors, etc.) when you are struggling with any of the homework assignments or the research project, your final products must be your own.\n\nHomework\nI will send out periodic homework assignments in order to give you students experience applying the methods we discuss in class. These assignments will be a mix of conceptual, statistical, and computational exercises. Please reach out to me if you find yourself struggling or overly stressing with these assignments. They are meant to be a learning tool not a major stressor!\n\n\nResearch Project\nOne of the more exciting things about being a graduate student is that you are able to explore the topics you find interesting. Use this research project to apply the methods we learn to any topic of your choice. Alternatively, I have fictitious data you can use if you do not have access to data of your own. Please talk to me by October 17th about your research project, even if your not 100% sure about it.\nYour final product should include four components:\n\nA brief introduction to your topic, the theory you are testing, and your hypotheses.\nA methods section write-up that parallels methods sections found in published articles.\nA results secection write-up that parallels methods sections found in published articles.\nThe code you used to analyze your data along with the dataset (assuming you are allowed to share the data).\n\n\n\n\nUniversity Honor Code, Academic Honesty Policy, Bentley Core Values\nThis class will be conducted in full accordance with The Bentley Core Values. Please reread the Values, which can be found at https://www.bentley.edu/about/mission-and-values.\nBentley College Honor Code: The Bentley College Honor Code formally recognized the responsibility of students to act in an ethical manner. It expects all students to maintain academic honesty in their own work, recognizing that most students will maintain academic honesty because of their own high standards. The Honor Code expects students to promote ethical behavior throughout the Bentley community and to take responsible action when there is a reason to suspect dishonesty.\nPersonal Academic Behavior: A student acknowledges that all submitted work (e.g., examination, papers, cases homework assignments) must be his or her own. The exception is the case in which an instructor permits or encourages students to work together on some or all assignments. When a student is in doubt, he or she should consult the instructor for clarification.\nResponsible Actions: Each student, as an integral member of the academic community, is expected to make a commitment to act honestly and to reject dishonesty on the part of other students. The students as a community are responsible for maintaining an ethical environment. Policies may be found at: http://www.bentley.edu/centers/alliance/academic-integrity\n\n\nBias Incident Reporting\nThe Bias Incident Response Team (BIRT) provides students affected by bias or bias-related incidents with access to appropriate resources. Where appropriate, BIRT assists the University in its response to situations that may impact the overall campus climate related to diversity and inclusion. Working closely with appropriate students, faculty, committees, organizations, and staff, BIRT plays an educational role in fostering an inclusive campus community and supporting targeted individuals when bias or bias-related incidents occur. More information about BIRT and how to file a bias incident report can be found at: https://www.bentley.edu/offices/student-affairs/birt.\n\n\nSpecial Accommodations\nStatement of Disabilities: Bentley University abides by Section 504 of the Rehabilitation Act of 1973 and the Americans with Disabilities Act of 1990 which stipulate no student shall be denied the benefits of an education solely by reason of a disability. If you have a hidden or visible disability which may require classroom accommodations, please call (if you are a residential student or on online student) Disability Services within the first 4 weeks of the semester to schedule an appointment. Disability Services is located in the Office of Academic Services (JEN 336, 781.891.2004). Disability Services is responsible for managing accommodations and services for all students with disabilities.\n\n\nWriting Center\nThe Writing Center offers one-on-one tutoring to students of all years and skill levels. Located on the lower level of the Bentley library (room 023), the Writing Center provides a welcoming and supportive environment in which students can work on writing from any class or discipline. Writers are encouraged to visit at all stages of the writing process; they can come with a draft, an outline, or just some initial thoughts and questions.\nStaffed by highly skilled student tutors, the Writing Center is open six days a week. Most conferences will be conducted online, but limited in-person hours will be held by appointment only. Appointments can be made at bentley.mywconline.net. For specific hours and additional information, please visit the Writing Center SharePoint site.\n\n\nESOL\nThe ESOL Center offers online appointments for helping undergraduate and graduate students strengthen their writing and English language skills. Our ESOL faculty tutors specialize in working with international and multilingual students to provide one-on-one support for all courses writing at any stage in the writing process. Along with individualized help for writing, the ESOL tutors provide guidance and feedback for documenting sources, oral presentation practice, and pronunciation/fluency enrichment.\nThe ESOL Center offers real-time video appointments Monday through Friday between 7:30 a.m. and 10:00 p.m. These can be reserved through our website: https://bentleyesol.mywconline.net. The complete information about booking appointments and uploading papers is clarified on the website’s announcement page.\n\n\nCourse Style\nI want this course to be an enjoyable and engaging experience for all, so although I will have lecture slides to talk through, I will also be using this course more as a discussion about statistical topics, not a lecture about them.\nIn order to meaningfully engage in this discussion, I encourage you to read through the required readings and skim through the supplemental readings (although I think they are all interesting reads!). I understand everyone is busy, so, despite being labeleled “Required Readings”, I will not make the readings required, but to make this course useful you will need to engage with the material and come with questions!\nTo be successful in this course, you will need to:\n\nDo the required readings and skim the supplemental readings\nCome to class and bring questions\nEngage in the course discussions\nMost importantly, ASK QUESTIONS\n\n\n\nTentative Course Schedule\nNOTE: The course syllabus is a general plan for the course and as such there may be deviations throughout the semester. Supplemental readings are any readings that are italicized or hyperlinked.\n\n\n\nDate\nTopic\nReadings\n\n\n\n\n9/5\nBivariate Measures of Association & Simple Regression\n\nRAGLM – Chapter 1: Statistical Control & Linear Models\nRAGLM – Chapter 2: The Simple Regression Model\nDBEP – Chapter 4.6: Dependence, Covariance, & Correlation\nR for Data Science – Chapter 2: Data Visualization\nR for Data Science – Chapter 3: Workflow: Basics\n\n\n\n\n\n\n\n\n9/12\nMeasures of Partial Association & Multiple Regression\n\nRAGLM – Chapter 3: Partial Relationship & the Multiple Regression Model\nR for Data Science – Chapter 4: Data Transformation\nR for Data Science – Chapter 5: Workflow: Code Style\nR for Data Science – Chapter 6: Data Tidying\n\n\n\n\n\n\n\n\n9/19\nStatistical Inference for Regression\n\nRAGLM – Chapter 4: Statistical Inference in Regression\nDBEP – Chapter 9: Generalizing Results of a Regression\nModernDive - Chapter 10.1 to 10.3\n\n\n\n\n\n\n\n\n9/29\nCategorical Predictors & Model Building\nImmersion Day\n\nRAGLM – Chapter 5: Extending Regression Analysis & Principles\nRAGLM – Chapter 8: Assessing the Importance of Regressors\nRAGLM – Chapter 9: Multicategorical Regressors\nModernDive – Chapter 5: Basic Regression\nModernDive – Chapter 6: Multiple Regression\n\n\n\n\n\n\n\n\n10/10\nFall Break - No Class\n\n\n\n\n\n\n\n\n10/17\nNonlinear Effects, Interactions, & Model Building\n\nRAGLM – Chapter 12.1 & 12.2: Nonlinear Relationships\nRAGLM – Chapter 13: Linear Interaction\nR for Data Science – 10: Layers\nR for Data Science – Chapter 11: Exploratory Data Analysis\n\n\n\n\n\n\n\n\n10/24\nMore on Interactions\n\nRAGLM – Chapter 14: Probing Interactions & Various Complexities\nRAGLM – Chapter 11: Mutliple Tests\nDawson (2014). Moderation in management research: What, why, when, and how.\nHaans et al. (2016). Thinking about U: Theorizing and testing U- and inverted U-shaped relationships\n\n\n\n\n\n\n\n\n11/3\nMediation, Path Analysis, & and Diagnostics\nImmersion Day\n\nRAGLM – Chapter 15: Mediation & Path Analysis\nRAGLM – 16: Detecting & Managing Irregularities\nRAGLM – 17.3: An Assortment of Problems\nAguinis et al. (2017). Improving our understanding of moderation and mediation in strategic management research.\nVanderWeele (2016). Mediation analysis: A practitioner’s guide.\nChatterjee & Yilmaz (1992). A review of regression diagnostics for behavioral research.\n\n\n\n\n\n\n\n\n11/7\nPower, Precision, & Measurement Error\n\nRAGLM – 17.1: Power & Precision of Estimation\nRAGLM – 17.2: Measurement Error\nRee & Caretta (2006). The role of measurement error in familiar statistics.\nCortina (1993). What is coefficient alpha: An examination of theory and application\n\n\n\n\n\n\n\n\n11/14\nIntroduction to Causation\n\nDBEP – Chapter 19: A Framework for Causal Analysis\nWest & Thoemmes (2010). Campbell’s and Rubin’s perspectives on causal inference.\n\n\n\n\n\n\n\n\n11/21\nThanksgiving Break - No Class\n\n\n\n\n\n\n\n\n11/28\nExperimental Design & Analysis\n\nDBEP – Chapter 20: Designing & Analyzing Experiments\nRAGLM – Chapter 6: Statistical versus Experimental Control\nPosakoff & Podsakoff (2019). Experimental designs in management and leadership research\nEden (2017). Field experiments in organizations.\nLuca & Bazerman (2020). Want to make better decisions? Start experimenting.\n\n\n\n\n\n\n\n\n12/5\nCausal Inference with Observational Data\n\nDBEP – Chapter 21: Regression & Matching with Observational Data\nNarita et al. (2023). Causal inference with observational data: A tutorial on propensity score analysis.\nHernan (2018). The C-word: Scientific euphemisms do not improve causal inference from observational data.\n\n\n\n\n\n\n\n\n12/12\nCausal Mediation\n\nCuartas & McCoy (2021). Causal mediation in developmental science: A primer.\nNguyen et al. (2021). Clarifying causal mediation analysis for the applied researcher\nImai et al. (2010). A general approach to causal mediation analysis.\nBullock et al. (2010). Yes, but what’s the mechanism? (Don’t expect an easy answer)."
  },
  {
    "objectID": "lectures/01-lecture-page.html",
    "href": "lectures/01-lecture-page.html",
    "title": "Quantitative Analysis 1",
    "section": "",
    "text": "Next Week’s Materials »"
  },
  {
    "objectID": "lectures/01-lecture-page.html#lecture-stats-bootcamp",
    "href": "lectures/01-lecture-page.html#lecture-stats-bootcamp",
    "title": "Quantitative Analysis 1",
    "section": "Lecture: Stats Bootcamp",
    "text": "Lecture: Stats Bootcamp\n\n\nTo download a pdf version of these slides, click here.\nTo download the R script that follows the R portion of the lecture, click here."
  },
  {
    "objectID": "lectures/01-lecture-page.html#r-in-class-assignment",
    "href": "lectures/01-lecture-page.html#r-in-class-assignment",
    "title": "Quantitative Analysis 1",
    "section": "R: In-Class Assignment",
    "text": "R: In-Class Assignment\n\nx <- rnorm(500)\nmean(x)\n\n[1] -0.01222021"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#slide-1",
    "href": "lectures/01-lecture-slides.html#slide-1",
    "title": "Stats Bootcamp",
    "section": "Slide 1",
    "text": "Slide 1"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#slide-2",
    "href": "lectures/01-lecture-slides.html#slide-2",
    "title": "Stats Bootcamp",
    "section": "Slide 2",
    "text": "Slide 2"
  },
  {
    "objectID": "notes/00-notes-template.html",
    "href": "notes/00-notes-template.html",
    "title": "Notes Template",
    "section": "",
    "text": "This is a template for me to use to right my lecture notes."
  },
  {
    "objectID": "notes/00-notes-template.html#test",
    "href": "notes/00-notes-template.html#test",
    "title": "Notes Template",
    "section": "Test",
    "text": "Test"
  },
  {
    "objectID": "notes/00-notes-template.html#test-2",
    "href": "notes/00-notes-template.html#test-2",
    "title": "Notes Template",
    "section": "Test 2",
    "text": "Test 2"
  },
  {
    "objectID": "notes/00-notes-template.html#test-3",
    "href": "notes/00-notes-template.html#test-3",
    "title": "Notes Template",
    "section": "Test 3",
    "text": "Test 3\nTest.\n\nTest 4\nThis is a test."
  },
  {
    "objectID": "notes/01-lecture-notes.html#sampling-distribution",
    "href": "notes/01-lecture-notes.html#sampling-distribution",
    "title": "Lecture 1 Notes",
    "section": "Sampling Distribution",
    "text": "Sampling Distribution"
  },
  {
    "objectID": "notes/01-lecture-notes.html#confidence-interval",
    "href": "notes/01-lecture-notes.html#confidence-interval",
    "title": "Lecture 1 Notes",
    "section": "Confidence Interval",
    "text": "Confidence Interval"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#learning-goals",
    "href": "lectures/01-lecture-slides.html#learning-goals",
    "title": "An Introduction to Programming & Statistics",
    "section": "Learning Goals",
    "text": "Learning Goals\n\nData importing and transformation with R\nEstimate and interpret a statistical test"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#overview",
    "href": "lectures/01-lecture-slides.html#overview",
    "title": "An Introduction to Statistics & Programming",
    "section": "Overview",
    "text": "Overview\n\nIntroduction to Statistical Science\n\nDescriptive Statistics\nQuick look at Probability Theory\nInferential Statistics\n\nIntroduction to Programming with R\n\nBase R\nTidyverse"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#first-steps",
    "href": "lectures/01-lecture-slides.html#first-steps",
    "title": "An Introduction to Statistics & Programming",
    "section": "First Steps",
    "text": "First Steps\n\nDownload R if you haven’t already\nDownload RStudio"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#what-is-r",
    "href": "lectures/01-lecture-slides.html#what-is-r",
    "title": "An Introduction to Statistics & Programming",
    "section": "What is R?",
    "text": "What is R?\nR is a programming language that is generally used for statistical computing."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#why-learn-r",
    "href": "lectures/01-lecture-slides.html#why-learn-r",
    "title": "An Introduction to Statistics & Programming",
    "section": "Why Learn R?",
    "text": "Why Learn R?\nTo analyze the data you collect, you will need to be familiar with some kind of general programming language (R, Python, etc.) or a more specific statistical program (SPSS, SAS). I recommend and use R because it is:\n\nOpen-source (free to download, use, and improve)\nHighly flexible language\nCan estimate A LOT of different statistical models"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#rstudio-and-ides",
    "href": "lectures/01-lecture-slides.html#rstudio-and-ides",
    "title": "An Introduction to Programming & Statistics",
    "section": "RStudio and IDEs",
    "text": "RStudio and IDEs\nWe will be using RStudio to do all of our statistical computing. RStudio is an integrated development environment (IDE) initially developed for R.\nIf you have not already, please go ahead and download RStudio from: https://posit.co/downloads/."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#basics-of-the-integrated-development-environment-ide",
    "href": "lectures/01-lecture-slides.html#basics-of-the-integrated-development-environment-ide",
    "title": "An Introduction to Programming & Statistics",
    "section": "Basics of the Integrated Development Environment (IDE)",
    "text": "Basics of the Integrated Development Environment (IDE)\nAn IDE is an application that makes programming a little easier and organized. As we will see shortly, it includes all of the tools one needs to program effectively and efficiently."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#writing-scripts-in-r",
    "href": "lectures/01-lecture-slides.html#writing-scripts-in-r",
    "title": "An Introduction to Statistics & Programming",
    "section": "Writing Scripts in R",
    "text": "Writing Scripts in R\nJust because you can write R code in just about any kind of digital document (Word, Notes, Notepad) does not mean you should!\nIt is best practice to write your code in an R Script (.R) in RStudio (in my opinion at least)."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#using-comments-in-your-code",
    "href": "lectures/01-lecture-slides.html#using-comments-in-your-code",
    "title": "An Introduction to Statistics & Programming",
    "section": "Using Comments in Your Code",
    "text": "Using Comments in Your Code\nYou can write comments in your own code by beginning a line with #. R will not evaluate any text on a line that begins with #.\n\n# This is a comment. Use comments to leave yourself notes in your script."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#objects-in-r",
    "href": "lectures/01-lecture-slides.html#objects-in-r",
    "title": "An Introduction to Statistics & Programming",
    "section": "Objects in R",
    "text": "Objects in R\nEverything you do in R will involve some kind of object that you have created. Think of an object like a box that you can place data in, so that R can later access and manipulate the data. An important of the code below is the assignment operator &lt;- which is how R knows to assign value to object_name.\n\nobject_name &lt;- value"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#functions-in-r",
    "href": "lectures/01-lecture-slides.html#functions-in-r",
    "title": "An Introduction to Statistics & Programming",
    "section": "Functions in R",
    "text": "Functions in R\nFunctions are objects in R that take user inputs, apply some predefined set of operations, and return an expected output.\n\nsum(c(1, 3))\n\n[1] 4"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#writing-your-own-function",
    "href": "lectures/01-lecture-slides.html#writing-your-own-function",
    "title": "An Introduction to Programming & Statistics",
    "section": "Writing Your Own Function",
    "text": "Writing Your Own Function\nR comes with many predefined functions, but often times you will want to write your own function to accomplish some repetive task. If you find yourself copy and pasting the same lines of code, then it is time to turn those lines into a function."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#the-elements-of-a-function",
    "href": "lectures/01-lecture-slides.html#the-elements-of-a-function",
    "title": "An Introduction to Statistics & Programming",
    "section": "The Elements of a Function",
    "text": "The Elements of a Function\nR comes with a variety of predefined functions and they all follow the same structure:\n\nA name for the function.\nThe arguments that change across different function calls.\nThe body which contains the code that is repeated across different calls."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#the-elements-of-a-function-1",
    "href": "lectures/01-lecture-slides.html#the-elements-of-a-function-1",
    "title": "An Introduction to Statistics & Programming",
    "section": "The Elements of a Function",
    "text": "The Elements of a Function\n\nname &lt;- function(argument) {\n  body\n}"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#example-function",
    "href": "lectures/01-lecture-slides.html#example-function",
    "title": "An Introduction to Programming & Statistics",
    "section": "Example Function",
    "text": "Example Function\n\ncreate_email_address &lt;- function(name) {\n  email &lt;- stringr::str_to_title(name)\n  email &lt;- stringr::str_replace_all(email, \" \", \"_\")\n  email &lt;- paste0(email, \"@organization.com\")\n  return(email)\n}\n\ncreate_email_address(name = \"john doe\")\n\n[1] \"John_Doe@organization.com\"\n\ncreate_email_address(name = \"JOHN DOE\")\n\n[1] \"John_Doe@organization.com\"\n\ncreate_email_address(name = \"jOhN dOe\")\n\n[1] \"John_Doe@organization.com\""
  },
  {
    "objectID": "lectures/01-lecture-slides.html#linking-functions-together",
    "href": "lectures/01-lecture-slides.html#linking-functions-together",
    "title": "An Introduction to Statistics & Programming",
    "section": "Linking Functions Together",
    "text": "Linking Functions Together\nR lets you link any number of functions together by nesting them. R will start with the innermost function and then work its way outward.\n\nsum(abs(c(-1, -1, 1, 1)))\n\n[1] 4"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#packages-the-lifeblood-of-r",
    "href": "lectures/01-lecture-slides.html#packages-the-lifeblood-of-r",
    "title": "An Introduction to Statistics & Programming",
    "section": "Packages: The Lifeblood of R",
    "text": "Packages: The Lifeblood of R\nA lot of what makes R such an effective programming language (especially for statistics) is the sheer number of available R packages. An R package is a collection of functions that complement one another for a given task. New packages are always being developed and anyone can author one!"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#getting-help-with-r",
    "href": "lectures/01-lecture-slides.html#getting-help-with-r",
    "title": "An Introduction to Statistics & Programming",
    "section": "Getting Help with R",
    "text": "Getting Help with R\nThere are two ways to get help in R:\n\nAdd ? in front of your function, which will result in RStudio displaying the help page for that function.\nGoogle what you are trying to do. More often than not, someone else has run into your problem, found a solution, and posted it. Stand on their shoulders!\n\n\n?sum()"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#atomic-vectors",
    "href": "lectures/01-lecture-slides.html#atomic-vectors",
    "title": "An Introduction to Statistics & Programming",
    "section": "Atomic Vectors",
    "text": "Atomic Vectors\n\nAn atomic vector is just a simple vector of data.\nR recognizes six types of atomic vectors:\n\nIntegers\nDoubles (Numeric)\nCharacters\nLogicals\nComplex\nRaw"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#integer-numeric-vectors",
    "href": "lectures/01-lecture-slides.html#integer-numeric-vectors",
    "title": "An Introduction to Statistics & Programming",
    "section": "Integer & Numeric Vectors",
    "text": "Integer & Numeric Vectors\nInteger vectors contain only integers. Add L after each number so R recognizes it as an integer. Numeric (doubles) vectors contain real numbers. These are the default vectors for numbers.\n\ninteger_vec &lt;- c(1L, 2L, 50L)\nnumeric_vec &lt;- c(1, 2, 50, 45.23)"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#character-vector",
    "href": "lectures/01-lecture-slides.html#character-vector",
    "title": "An Introduction to Statistics & Programming",
    "section": "Character Vector",
    "text": "Character Vector\nCharacter vectors contain only text data also referred to as string data. Basically anything surrounded by \"\" or '' is considered string data.\n\ncharacter_vec &lt;- c(\"1\", \"abc\", \"$#2\")"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#logical-vector",
    "href": "lectures/01-lecture-slides.html#logical-vector",
    "title": "An Introduction to Statistics & Programming",
    "section": "Logical Vector",
    "text": "Logical Vector\nLogical vectors are vectors that can only contain TRUE or FALSE values also referred to as boolean values.\n\nlogical_vec &lt;- c(TRUE, FALSE)"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#applying-functions-to-vectors",
    "href": "lectures/01-lecture-slides.html#applying-functions-to-vectors",
    "title": "Stats Bootcamp",
    "section": "Applying Functions to Vectors",
    "text": "Applying Functions to Vectors"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#nesting-multiple-functions",
    "href": "lectures/01-lecture-slides.html#nesting-multiple-functions",
    "title": "Stats Bootcamp",
    "section": "Nesting Multiple Functions",
    "text": "Nesting Multiple Functions"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#using-the-pipe",
    "href": "lectures/01-lecture-slides.html#using-the-pipe",
    "title": "An Introduction to Statistics & Programming",
    "section": "Using the pipe |>",
    "text": "Using the pipe |&gt;\nThe |&gt; operator allows you to take the output of one function and feed it directly into the first argument of the next function. Using the |&gt; makes it easier to read your code, which is a good thing.\n\nc(-1, -1, 1, 1) |&gt;\n  abs() |&gt;\n  sum()\n\n[1] 4"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#adding-attributes",
    "href": "lectures/01-lecture-slides.html#adding-attributes",
    "title": "An Introduction to Statistics & Programming",
    "section": "Adding Attributes",
    "text": "Adding Attributes\nYou can think of attributes as metadata for R objects. As a user you will not need to worry too much about attributes directly, but attributes tell R how to interact with the specific object and allow the user to store information that is secondary to the analyses they are conducting."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#names-attribute",
    "href": "lectures/01-lecture-slides.html#names-attribute",
    "title": "An Introduction to Statistics & Programming",
    "section": "names Attribute",
    "text": "names Attribute\n\ndays_of_week &lt;- 1:7 \nnames(days_of_week) &lt;- c(\"mon\", \"tues\", \"wed\", \"thurs\", \"fri\", \"sat\", \"sun\")\nnames(days_of_week)\n\n[1] \"mon\"   \"tues\"  \"wed\"   \"thurs\" \"fri\"   \"sat\"   \"sun\"  \n\nattributes(days_of_week)\n\n$names\n[1] \"mon\"   \"tues\"  \"wed\"   \"thurs\" \"fri\"   \"sat\"   \"sun\""
  },
  {
    "objectID": "lectures/01-lecture-slides.html#dim-attribute",
    "href": "lectures/01-lecture-slides.html#dim-attribute",
    "title": "An Introduction to Statistics & Programming",
    "section": "dim Attribute",
    "text": "dim Attribute\n\ndays_of_week &lt;- 1:14\ndim(days_of_week) &lt;- c(2, 7) # 2 Rows, 7 Columns\nattributes(days_of_week)\n\n$dim\n[1] 2 7\n\nclass(days_of_week)\n\n[1] \"matrix\" \"array\""
  },
  {
    "objectID": "lectures/01-lecture-slides.html#creating-factors",
    "href": "lectures/01-lecture-slides.html#creating-factors",
    "title": "An Introduction to Statistics & Programming",
    "section": "Creating Factors",
    "text": "Creating Factors\nR stores categorical data using factors, which are integer vectors with two attributes: class and levels.\n\ndays_of_week &lt;- factor(c(\"mon\", \"tues\", \"wed\", \"thurs\", \"fri\", \"sat\", \"sun\"))\ntypeof(days_of_week)\n\n[1] \"integer\"\n\nattributes(days_of_week)\n\n$levels\n[1] \"fri\"   \"mon\"   \"sat\"   \"sun\"   \"thurs\" \"tues\"  \"wed\"  \n\n$class\n[1] \"factor\""
  },
  {
    "objectID": "lectures/01-lecture-slides.html#lists-a-flexible-representation-of-data",
    "href": "lectures/01-lecture-slides.html#lists-a-flexible-representation-of-data",
    "title": "An Introduction to Programming & Statistics",
    "section": "Lists: A Flexible Representation of Data",
    "text": "Lists: A Flexible Representation of Data\nLike vectors, lists are another way to structure data in R. Rather than group individual elements, however, lists group together different types of R objects including other lists.\n\nlist_1 &lt;- list(c(1, 3, 4), c(\"a\", \"b\"), list(3, 4))"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#data-frames-best-way-to-represent-data",
    "href": "lectures/01-lecture-slides.html#data-frames-best-way-to-represent-data",
    "title": "An Introduction to Statistics & Programming",
    "section": "Data Frames: Best way to Represent Data",
    "text": "Data Frames: Best way to Represent Data\nData frames are the best way to structure and store data in R. Data frames are sort of the R equivalent of an excel spreadsheet.\nEach column in a data frame is a vector, so a data frame can combine a numeric vector as one column with a character vector as another column.\n\ndata_frame_1 &lt;- data.frame(NUMERIC = c(1, 3), CHARACTER = c(\"a\", \"b\"), \n                           LOGICAL = c(TRUE, FALSE))\ndata_frame_1\n\n  NUMERIC CHARACTER LOGICAL\n1       1         a    TRUE\n2       3         b   FALSE"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#viewing-your-data",
    "href": "lectures/01-lecture-slides.html#viewing-your-data",
    "title": "An Introduction to Statistics & Programming",
    "section": "Viewing Your Data",
    "text": "Viewing Your Data\nYou can use View() to open up a spreadsheet-like view of your data.\n\nView(data_frame_1)"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#selecting-data-from-vectors-and-data-frames",
    "href": "lectures/01-lecture-slides.html#selecting-data-from-vectors-and-data-frames",
    "title": "Stats Bootcamp",
    "section": "Selecting Data from Vectors and Data Frames",
    "text": "Selecting Data from Vectors and Data Frames"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#selecting-data-from-lists",
    "href": "lectures/01-lecture-slides.html#selecting-data-from-lists",
    "title": "Stats Bootcamp",
    "section": "Selecting Data from Lists",
    "text": "Selecting Data from Lists"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#na-value-missing-data",
    "href": "lectures/01-lecture-slides.html#na-value-missing-data",
    "title": "Stats Bootcamp",
    "section": "NA Value: Missing Data",
    "text": "NA Value: Missing Data"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#reading-writing-data",
    "href": "lectures/01-lecture-slides.html#reading-writing-data",
    "title": "Stats Bootcamp",
    "section": "Reading & Writing Data",
    "text": "Reading & Writing Data"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#what-is-the-tidyverse",
    "href": "lectures/01-lecture-slides.html#what-is-the-tidyverse",
    "title": "An Introduction to Statistics & Programming",
    "section": "What is the Tidyverse?",
    "text": "What is the Tidyverse?\nThe tidyverse is a collection of R packages that “share a common philosophy of data and R programming and are designed to work together.”"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#tibble-data-frame-of-tidyverse",
    "href": "lectures/01-lecture-slides.html#tibble-data-frame-of-tidyverse",
    "title": "An Introduction to Statistics & Programming",
    "section": "tibble: Data frame of Tidyverse",
    "text": "tibble: Data frame of Tidyverse\nTibbles are the tidyverse’s version of a data.frame. They can be loaded from the tidyverse package: tibble.\n\ndata_employees_tbl &lt;- tibble::as_tibble(data_employees)\ndata_employees_tbl\n\n# A tibble: 1,470 × 36\n   employee_id active stock_opt_lvl trainings   age commute_dist ed_lvl ed_field\n         &lt;int&gt; &lt;chr&gt;          &lt;int&gt;     &lt;int&gt; &lt;int&gt;        &lt;int&gt;  &lt;int&gt; &lt;chr&gt;   \n 1        1001 No                 0         0    41            1      2 Life Sc…\n 2        1002 Yes                1         3    49            8      1 Life Sc…\n 3        1003 No                 0         3    37            2      2 Other   \n 4        1004 Yes                0         3    33            3      4 Life Sc…\n 5        1005 Yes                1         3    27            2      1 Medical \n 6        1006 Yes                0         2    32            2      2 Life Sc…\n 7        1007 Yes                3         3    59            3      3 Medical \n 8        1008 Yes                1         2    30           24      1 Life Sc…\n 9        1009 Yes                0         2    38           23      3 Life Sc…\n10        1010 Yes                2         3    36           27      3 Medical \n# ℹ 1,460 more rows\n# ℹ 28 more variables: gender &lt;chr&gt;, marital_sts &lt;chr&gt;, dept &lt;chr&gt;,\n#   engagement &lt;int&gt;, job_lvl &lt;int&gt;, job_title &lt;chr&gt;, overtime &lt;chr&gt;,\n#   business_travel &lt;chr&gt;, hourly_rate &lt;int&gt;, daily_comp &lt;int&gt;,\n#   monthly_comp &lt;int&gt;, annual_comp &lt;int&gt;, ytd_leads &lt;int&gt;, ytd_sales &lt;int&gt;,\n#   standard_hrs &lt;int&gt;, salary_hike_pct &lt;int&gt;, perf_rating &lt;int&gt;,\n#   prior_emplr_cnt &lt;int&gt;, env_sat &lt;int&gt;, job_sat &lt;int&gt;, rel_sat &lt;int&gt;, …"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#dplyr-your-data-multitool",
    "href": "lectures/01-lecture-slides.html#dplyr-your-data-multitool",
    "title": "An Introduction to Statistics & Programming",
    "section": "dplyr: Your Data Multitool",
    "text": "dplyr: Your Data Multitool\nThe package dplyr should become your go-to data manipulation and structuring tool! It contains many useful functions that make it surprisingly easy to manipulate and structure your data."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#selecting-columns-using-dplyrselect",
    "href": "lectures/01-lecture-slides.html#selecting-columns-using-dplyrselect",
    "title": "Stats Bootcamp",
    "section": "Selecting Columns using dplyr::select",
    "text": "Selecting Columns using dplyr::select\n\ndplyr::select(data_frame_1, NUMERIC, LOGICAL)\n\n  NUMERIC LOGICAL\n1       1    TRUE\n2       3   FALSE\n\n\n\ndata_frame_1 |&gt;\n  dplyr::select(NUMERIC, LOGICAL)"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#adding-new-columns-using-dplyrmutate",
    "href": "lectures/01-lecture-slides.html#adding-new-columns-using-dplyrmutate",
    "title": "Stats Bootcamp",
    "section": "Adding New Columns Using dplyr::mutate",
    "text": "Adding New Columns Using dplyr::mutate"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#filtering-rows-using-dplyrfilter",
    "href": "lectures/01-lecture-slides.html#filtering-rows-using-dplyrfilter",
    "title": "Stats Bootcamp",
    "section": "Filtering Rows Using dplyr::filter",
    "text": "Filtering Rows Using dplyr::filter"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#an-example-tibble",
    "href": "lectures/01-lecture-slides.html#an-example-tibble",
    "title": "Stats Bootcamp",
    "section": "An Example tibble",
    "text": "An Example tibble"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#using-to-bring-it-all-together",
    "href": "lectures/01-lecture-slides.html#using-to-bring-it-all-together",
    "title": "Stats Bootcamp",
    "section": "Using |> to bring it all together",
    "text": "Using |&gt; to bring it all together"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#r-resources",
    "href": "lectures/01-lecture-slides.html#r-resources",
    "title": "An Introduction to Statistics & Programming",
    "section": "R Resources",
    "text": "R Resources\nhttps://r4ds.hadley.nz/"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#what-is-statistical-science",
    "href": "lectures/01-lecture-slides.html#what-is-statistical-science",
    "title": "An Introduction to Statistics & Programming",
    "section": "What is Statistical Science?",
    "text": "What is Statistical Science?\n\nStatistical science is the science of developing and applying methods for collecting, analyzing, and interpreting data.\n\n— Agresti & Kateri, 2022"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#three-aspects-of-statistical-science",
    "href": "lectures/01-lecture-slides.html#three-aspects-of-statistical-science",
    "title": "An Introduction to Statistics & Programming",
    "section": "Three Aspects of Statistical Science",
    "text": "Three Aspects of Statistical Science\n\nDesign: Planning on how to gather relevant data.\nDescription: Summarizing the data.\nInference: Making evaluations (generalizations) based on the data."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#design-of-studies",
    "href": "lectures/01-lecture-slides.html#design-of-studies",
    "title": "An Introduction to Statistics & Programming",
    "section": "Design of Studies",
    "text": "Design of Studies\nThe design of a study focuses on planning a study so that it produces useful data. This involves:\n\nDeciding how to sample and who to sample\nConstructing surveys for observational studies\nConstructing treatments for experimental studies"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#descriptive-statistics",
    "href": "lectures/01-lecture-slides.html#descriptive-statistics",
    "title": "Stats Bootcamp",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#inferential-statistics",
    "href": "lectures/01-lecture-slides.html#inferential-statistics",
    "title": "An Introduction to Statistics & Programming",
    "section": "Inferential Statistics",
    "text": "Inferential Statistics\nUsually, when you analyze your data you want to generalize the results from your specific dataset to a broader population or more general phenomenon. This is called statistical inference. We infer something from our data about a more general phenomenon."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#populations",
    "href": "lectures/01-lecture-slides.html#populations",
    "title": "Stats Bootcamp",
    "section": "Populations",
    "text": "Populations\n\nSubject\nParameter"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#actual-vs-conceptual-populations",
    "href": "lectures/01-lecture-slides.html#actual-vs-conceptual-populations",
    "title": "An Introduction to Statistics & Programming",
    "section": "Actual vs Conceptual Populations",
    "text": "Actual vs Conceptual Populations\nDepending on the research question, the population may be real or it may be conceptual. Conceptual populations are often future populations we want to generalize to, but which we have to use data collected on current populations."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#samples-data",
    "href": "lectures/01-lecture-slides.html#samples-data",
    "title": "Stats Bootcamp",
    "section": "Samples & Data",
    "text": "Samples & Data"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#variables",
    "href": "lectures/01-lecture-slides.html#variables",
    "title": "An Introduction to Statistics & Programming",
    "section": "Variables",
    "text": "Variables\nVariables are characteristics of the sample or population that vary across subjects. Data consists of a set of variables:\n\ndata_employees &lt;- peopleanalytics::employees\ndata_employees_tbl &lt;- tibble::as_tibble(data_employees)\n\nset.seed(3)\ndata_employees_tbl |&gt;\n  dplyr::sample_n(5) |&gt;\n  dplyr::select(\n    employee_id,\n    trainings,\n    ed_field,\n    dept\n  )\n\n# A tibble: 5 × 4\n  employee_id trainings ed_field         dept                  \n        &lt;int&gt;     &lt;int&gt; &lt;chr&gt;            &lt;chr&gt;                 \n1        1773         4 Medical          Research & Development\n2        1652         2 Marketing        Sales                 \n3        1999         2 Medical          Research & Development\n4        1548         2 Medical          Research & Development\n5        1698         5 Technical Degree Sales"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#types-of-variables",
    "href": "lectures/01-lecture-slides.html#types-of-variables",
    "title": "Stats Bootcamp",
    "section": "Types of Variables",
    "text": "Types of Variables\n\nQuantitative\nCategorical"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#types-of-quantitative-variables",
    "href": "lectures/01-lecture-slides.html#types-of-quantitative-variables",
    "title": "An Introduction to Statistics & Programming",
    "section": "Types of Quantitative Variables",
    "text": "Types of Quantitative Variables\nQuantitative variables can be further classified into two groups:\n\nDiscrete: Values are distinct, separable numbers (e.g. integers)\nContinuous: Values are on an infinite continuum (e.g. real numbers)"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#types-of-categorical-variables",
    "href": "lectures/01-lecture-slides.html#types-of-categorical-variables",
    "title": "An Introduction to Statistics & Programming",
    "section": "Types of Categorical Variables",
    "text": "Types of Categorical Variables\nSimilar to quantitative variables, categorical variables can also be classified into two groups:\n\nDichotomous / Binary: Two categories\nMulticategorical: Three or more categories"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#roles-of-variables",
    "href": "lectures/01-lecture-slides.html#roles-of-variables",
    "title": "An Introduction to Statistics & Programming",
    "section": "Roles of Variables",
    "text": "Roles of Variables\nVariables can not only be categorized by the kinds and ranges of values they take on, but also by the role they take on in the analysis:\n\nResponse, Outcome, Dependent Variable, Criterion Variable\nPredictor, Independent Variable, Feature, Covariate"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#collecting-data",
    "href": "lectures/01-lecture-slides.html#collecting-data",
    "title": "Stats Bootcamp",
    "section": "Collecting Data",
    "text": "Collecting Data\n\nRandomized Experiment\nQuasi-Experiment\nObservational Studies"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#randomized-experiments",
    "href": "lectures/01-lecture-slides.html#randomized-experiments",
    "title": "Stats Bootcamp",
    "section": "Randomized Experiments",
    "text": "Randomized Experiments"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#quasi-experiments",
    "href": "lectures/01-lecture-slides.html#quasi-experiments",
    "title": "Stats Bootcamp",
    "section": "Quasi-Experiments",
    "text": "Quasi-Experiments"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#observational-studies",
    "href": "lectures/01-lecture-slides.html#observational-studies",
    "title": "An Introduction to Statistics & Programming",
    "section": "Observational Studies",
    "text": "Observational Studies\nIn observational studies, the researchers observe collect a sample of subjects and observe their outcomes across the variables of interest. One type of observational study design is a survey study.\nThe important difference between observational studies and experiments is that subjects are not randomly assigned to treatments."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#what-is-a-statistic",
    "href": "lectures/01-lecture-slides.html#what-is-a-statistic",
    "title": "An Introduction to Statistics & Programming",
    "section": "What is a Statistic?",
    "text": "What is a Statistic?\nStatistics are numbers computed from your data that provide useful numerical information about the characteristics of a variable’s distribution such as its center (mean) or spread (standard deviation).\n\nmean(data_employees_tbl$commute_dist) |&gt; round(2) # Mean\n\n[1] 9.19\n\nmedian(data_employees_tbl$commute_dist) # Median\n\n[1] 7\n\nsd(data_employees_tbl$commute_dist) |&gt; round(2) # SD\n\n[1] 8.11"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#statistic-vs-population-parameter",
    "href": "lectures/01-lecture-slides.html#statistic-vs-population-parameter",
    "title": "An Introduction to Statistics & Programming",
    "section": "Statistic vs Population Parameter",
    "text": "Statistic vs Population Parameter\nPopulation parameters (or parameters) are numerical summaries of our population.\nStatistics are estimates of these parameters calculated from data sampled from this population.\nUsually, we do not have access to our full population of interest, so we sample our data from it and learn about its characteristics (parameters) through the statistics we compute from our sampled data."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#describing-the-center-of-your-data",
    "href": "lectures/01-lecture-slides.html#describing-the-center-of-your-data",
    "title": "An Introduction to Statistics & Programming",
    "section": "Describing the Center of your Data",
    "text": "Describing the Center of your Data\nOne common way to describe your data is to compute a statistic that tells you where the center of your data is—the average or expected value of your data. There are three statistics you can compute:\n\nMean: The average value.\nMedian: The value at which 50% of your data lies below it.\nMode: The most common value."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#describing-the-spread-of-your-data",
    "href": "lectures/01-lecture-slides.html#describing-the-spread-of-your-data",
    "title": "An Introduction to Statistics & Programming",
    "section": "Describing the Spread of your Data",
    "text": "Describing the Spread of your Data\nYou can describe the spread of your data by computing statistics that tells you generally how far the observations are from the mean of your data. There are three statistics you can compute:\n\nVariance: The average squared distance your data falls from the mean.\nStandard Deviation: The average distance your data falls from the mean (square root of variance).\nRange: Maximum value minus the minimum value of your data."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#describing-the-shape-of-your-data",
    "href": "lectures/01-lecture-slides.html#describing-the-shape-of-your-data",
    "title": "An Introduction to Statistics & Programming",
    "section": "Describing the Shape of your Data",
    "text": "Describing the Shape of your Data\nOftentimes, you will also want to talk about the shape of your data. Typically, this is about how skewed or asymmetric your data is in one direction or how heavy the tails of your distribution are:\n\nSkewness: How long the tails of your distribution are in a given direction.\nKurtosis: How heavy the tails of your distribution are."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#describing-the-overall-distribution-of-your-data",
    "href": "lectures/01-lecture-slides.html#describing-the-overall-distribution-of-your-data",
    "title": "Stats Bootcamp",
    "section": "Describing the Overall Distribution of your Data",
    "text": "Describing the Overall Distribution of your Data"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#describing-the-relationship-between-two-variables",
    "href": "lectures/01-lecture-slides.html#describing-the-relationship-between-two-variables",
    "title": "An Introduction to Statistics & Programming",
    "section": "Describing the Relationship between two Variables",
    "text": "Describing the Relationship between two Variables\nWe can also describe the linear relationship between two variables by computing either the covariation or correlation between two variables. Both of these metrics tell us the extent to which two variables are linearly related to one another."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#the-importance-of-plots",
    "href": "lectures/01-lecture-slides.html#the-importance-of-plots",
    "title": "Stats Bootcamp",
    "section": "The Importance of Plots",
    "text": "The Importance of Plots"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#histograms",
    "href": "lectures/01-lecture-slides.html#histograms",
    "title": "Stats Bootcamp",
    "section": "Histograms",
    "text": "Histograms"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#box-plots",
    "href": "lectures/01-lecture-slides.html#box-plots",
    "title": "Stats Bootcamp",
    "section": "Box Plots",
    "text": "Box Plots"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#violin-plots",
    "href": "lectures/01-lecture-slides.html#violin-plots",
    "title": "Stats Bootcamp",
    "section": "Violin Plots",
    "text": "Violin Plots"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#scatter-plots",
    "href": "lectures/01-lecture-slides.html#scatter-plots",
    "title": "Stats Bootcamp",
    "section": "Scatter Plots",
    "text": "Scatter Plots"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#what-is-probability",
    "href": "lectures/01-lecture-slides.html#what-is-probability",
    "title": "An Introduction to Statistics & Programming",
    "section": "What is Probability?",
    "text": "What is Probability?\nProbability is the language of uncertainty.\nAnytime we are dealing with random events such as the outcome of a coin toss or the response to a survey question, we rely on probability to talk about these events."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#probability-as-a-long-run-frequency",
    "href": "lectures/01-lecture-slides.html#probability-as-a-long-run-frequency",
    "title": "An Introduction to Statistics & Programming",
    "section": "Probability as a Long-Run Frequency",
    "text": "Probability as a Long-Run Frequency\n\nFor an observation of a random phenomen, the probability of a particular outcome is the proportion of times that outcome would occur in an indefinitely long sequence of like observations, under the same conditions.\n\n— Agresti & Kateri, 2022"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#sample-space-events",
    "href": "lectures/01-lecture-slides.html#sample-space-events",
    "title": "Quantitative Analysis 1 - Crash Course",
    "section": "Sample Space & Events",
    "text": "Sample Space & Events\n\nprobability-space"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#rules-of-probability-theory",
    "href": "lectures/01-lecture-slides.html#rules-of-probability-theory",
    "title": "Quantitative Analysis 1 - Crash Course",
    "section": "Rules of Probability Theory",
    "text": "Rules of Probability Theory"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#joint-probability",
    "href": "lectures/01-lecture-slides.html#joint-probability",
    "title": "An Introduction to Statistics & Programming",
    "section": "Joint Probability",
    "text": "Joint Probability\nJoint probability is the probability of some event happening for two or more random variables.\nFor example, what is the probability of employment_status == inactive & job_satisfaction == satisfied?"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#conditional-probability",
    "href": "lectures/01-lecture-slides.html#conditional-probability",
    "title": "An Introduction to Statistics & Programming",
    "section": "Conditional Probability",
    "text": "Conditional Probability\nConditional probability is the probability of one event occurring given the occurrence of another event. This is written mathematically as:\n\\[P(\\text{Event 1} \\space | \\space \\text{Event 2})\\]\nwhich is read as the probability of Event 1 conditional on (or given) Event 2."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#random-variables-connecting-probability-statistics-and-data",
    "href": "lectures/01-lecture-slides.html#random-variables-connecting-probability-statistics-and-data",
    "title": "Quantitative Analysis 1 - Crash Course",
    "section": "Random Variables: Connecting Probability, Statistics, and Data",
    "text": "Random Variables: Connecting Probability, Statistics, and Data"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#types-of-random-variables-discrete-continuous",
    "href": "lectures/01-lecture-slides.html#types-of-random-variables-discrete-continuous",
    "title": "An Introduction to Statistics & Programming",
    "section": "Types of Random Variables: Discrete & Continuous",
    "text": "Types of Random Variables: Discrete & Continuous\nRandom variables, like quantitative variables, can be classified into two broad categories:\n\nDiscrete: Separate, distinct outcome values like integers\nContinuous: Infinite continuum of possible outcomes"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#expectation-of-a-random-variable",
    "href": "lectures/01-lecture-slides.html#expectation-of-a-random-variable",
    "title": "Quantitative Analysis 1 - Crash Course",
    "section": "Expectation of a Random Variable",
    "text": "Expectation of a Random Variable"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#connection-between-the-mean-and-expectation",
    "href": "lectures/01-lecture-slides.html#connection-between-the-mean-and-expectation",
    "title": "Quantitative Analysis 1 - Crash Course",
    "section": "Connection Between the Mean and Expectation",
    "text": "Connection Between the Mean and Expectation\n\n1/n @ large n"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#probability-distribution-functions",
    "href": "lectures/01-lecture-slides.html#probability-distribution-functions",
    "title": "Quantitative Analysis 1 - Crash Course",
    "section": "Probability Distribution Functions",
    "text": "Probability Distribution Functions"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#probability-mass-density-functions",
    "href": "lectures/01-lecture-slides.html#probability-mass-density-functions",
    "title": "An Introduction to Statistics & Programming",
    "section": "Probability Mass & Density Functions",
    "text": "Probability Mass & Density Functions\nProbability Mass and Density Functions (PMF & PDF, respectively) are mathematical functions that take the value of a random variable as an input and return the probability of that value occurring as an output. Every statistical model we will use will assume a certain PMF or PDF.\n\nPMF is a probability distribution function for discrete random variables\nPDF is a probability distribution function for continuous random variables"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#cumulative-distribution-function",
    "href": "lectures/01-lecture-slides.html#cumulative-distribution-function",
    "title": "An Introduction to Statistics & Programming",
    "section": "Cumulative Distribution Function",
    "text": "Cumulative Distribution Function\nClosely related to the PMF/PDF, the Cumulative Distribution Function (CDF) specifies the cumulative probability that a random variable takes a value, Y, or any value less than Y:\n\\[F(\\text{Y})=P(\\text{Y} \\leq \\text{y})\\]\nIn our example, what is the probability that involuntary_turnover takes a value of 0? What is the probability involuntary_turnover takes a value less than or equal to 1?"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#joint-probability-distributions",
    "href": "lectures/01-lecture-slides.html#joint-probability-distributions",
    "title": "Stats Bootcamp",
    "section": "Joint Probability Distributions",
    "text": "Joint Probability Distributions"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#conditional-probability-distributions",
    "href": "lectures/01-lecture-slides.html#conditional-probability-distributions",
    "title": "Stats Bootcamp",
    "section": "Conditional Probability Distributions",
    "text": "Conditional Probability Distributions"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#independent-random-variables",
    "href": "lectures/01-lecture-slides.html#independent-random-variables",
    "title": "Quantitative Analysis 1 - Crash Course",
    "section": "Independent Random Variables",
    "text": "Independent Random Variables"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#inferential-statistics-1",
    "href": "lectures/01-lecture-slides.html#inferential-statistics-1",
    "title": "Stats Bootcamp",
    "section": "Inferential Statistics",
    "text": "Inferential Statistics"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#sampling-distributions",
    "href": "lectures/01-lecture-slides.html#sampling-distributions",
    "title": "An Introduction to Statistics & Programming",
    "section": "Sampling Distributions",
    "text": "Sampling Distributions\nImagine you can draw an infinite number of random samples from a population and then for each sample you compute the sample mean. The distribution of these sample means is referred to as the sampling distribution of the mean."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#the-standard-error",
    "href": "lectures/01-lecture-slides.html#the-standard-error",
    "title": "An Introduction to Statistics & Programming",
    "section": "The Standard Error",
    "text": "The Standard Error\nThe standard deviation of a sampling distribution is known by another name: the standard error. The standard error quantifies our uncertainty in a given statistic and is fundamental to inferential statistics.\nFor the mean, the standard error can be calculated as:\n\\[\\sigma_{\\bar{Y}} = \\sqrt{\\frac{\\sigma^{2}}{n}} = \\frac{\\sigma}{\\sqrt{n}}\\]"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#sampling-distribution-of-the-mean",
    "href": "lectures/01-lecture-slides.html#sampling-distribution-of-the-mean",
    "title": "Quantitative Analysis 1 - Crash Course",
    "section": "Sampling Distribution of the Mean",
    "text": "Sampling Distribution of the Mean"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#standard-error-of-the-mean",
    "href": "lectures/01-lecture-slides.html#standard-error-of-the-mean",
    "title": "Quantitative Analysis 1 - Crash Course",
    "section": "Standard Error of the Mean",
    "text": "Standard Error of the Mean"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#central-limit-theorem",
    "href": "lectures/01-lecture-slides.html#central-limit-theorem",
    "title": "An Introduction to Statistics & Programming",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#point-estimates",
    "href": "lectures/01-lecture-slides.html#point-estimates",
    "title": "An Introduction to Programming & Statistics",
    "section": "Point Estimates",
    "text": "Point Estimates"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#law-of-large-numbers",
    "href": "lectures/01-lecture-slides.html#law-of-large-numbers",
    "title": "An Introduction to Programming & Statistics",
    "section": "Law of Large Numbers",
    "text": "Law of Large Numbers"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#estimating-the-mean-variance",
    "href": "lectures/01-lecture-slides.html#estimating-the-mean-variance",
    "title": "An Introduction to Programming & Statistics",
    "section": "Estimating the Mean & Variance",
    "text": "Estimating the Mean & Variance"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#interval-estimates-confidence-interval",
    "href": "lectures/01-lecture-slides.html#interval-estimates-confidence-interval",
    "title": "An Introduction to Programming & Statistics",
    "section": "Interval Estimates (Confidence Interval)",
    "text": "Interval Estimates (Confidence Interval)"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#building-a-confidence-interval-around-the-mean",
    "href": "lectures/01-lecture-slides.html#building-a-confidence-interval-around-the-mean",
    "title": "An Introduction to Programming & Statistics",
    "section": "Building a Confidence Interval Around the Mean",
    "text": "Building a Confidence Interval Around the Mean\nGenerally, to build a confidence interval, you need three pieces of information:\n\nPoint estimate to build the interval around\nThe probability distribution that best approximates the estimate’s sampling distribution (almost always the normal distribution)\nThe Standard Error of the estimate (the standard devation of the sampling distribution)"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#interpreting-a-confidence-interval",
    "href": "lectures/01-lecture-slides.html#interpreting-a-confidence-interval",
    "title": "An Introduction to Statistics & Programming",
    "section": "Interpreting a Confidence Interval",
    "text": "Interpreting a Confidence Interval\nConfidence intervals have a very strict (and kind of odd) interpretation:\nIf you were to randomly sample a large number of samples from a population and create a 95% confidence interval around the sample mean, then 95% of those intervals would contain the population mean."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#what-is-a-statistical-test",
    "href": "lectures/01-lecture-slides.html#what-is-a-statistical-test",
    "title": "An Introduction to Programming & Statistics",
    "section": "What is a statistical test?",
    "text": "What is a statistical test?"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#the-four-elements-of-a-statistical-test",
    "href": "lectures/01-lecture-slides.html#the-four-elements-of-a-statistical-test",
    "title": "An Introduction to Statistics & Programming",
    "section": "The Four Elements of a Statistical Test",
    "text": "The Four Elements of a Statistical Test\n\nAssumptions: Background assumptions that need to hold for our test to be valid.\nHypotheses: The \\(H_{0}\\) and \\(H_{a}\\) hypotheses, which need to be formulated before analyses happen.\nTest Statistic: Summary of how far away a statistical estimate is from the population value predicted by \\(H_{0}\\).\nP-value & Conclusion: A decision on whether to reject or not reject \\(H_{0}\\) if the probability of our data coming from the null population distribution is sufficiently low as measured by a P-value."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#assumptions",
    "href": "lectures/01-lecture-slides.html#assumptions",
    "title": "An Introduction to Programming & Statistics",
    "section": "Assumptions",
    "text": "Assumptions"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#hypotheses",
    "href": "lectures/01-lecture-slides.html#hypotheses",
    "title": "An Introduction to Programming & Statistics",
    "section": "Hypotheses",
    "text": "Hypotheses"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#test-statistic",
    "href": "lectures/01-lecture-slides.html#test-statistic",
    "title": "An Introduction to Programming & Statistics",
    "section": "Test Statistic",
    "text": "Test Statistic"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#p-value-conclusion",
    "href": "lectures/01-lecture-slides.html#p-value-conclusion",
    "title": "An Introduction to Programming & Statistics",
    "section": "P-value & Conclusion",
    "text": "P-value & Conclusion"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#comparing-two-means-t-test",
    "href": "lectures/01-lecture-slides.html#comparing-two-means-t-test",
    "title": "An Introduction to Programming & Statistics",
    "section": "Comparing Two Means (T-Test)",
    "text": "Comparing Two Means (T-Test)"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#assumptions-of-the-t-test",
    "href": "lectures/01-lecture-slides.html#assumptions-of-the-t-test",
    "title": "An Introduction to Programming & Statistics",
    "section": "Assumptions of the T-Test",
    "text": "Assumptions of the T-Test"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#hypotheses-of-the-t-test",
    "href": "lectures/01-lecture-slides.html#hypotheses-of-the-t-test",
    "title": "An Introduction to Programming & Statistics",
    "section": "Hypotheses of the T-Test",
    "text": "Hypotheses of the T-Test"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#t-test-test-statistic",
    "href": "lectures/01-lecture-slides.html#t-test-test-statistic",
    "title": "An Introduction to Programming & Statistics",
    "section": "T-Test Test Statistic",
    "text": "T-Test Test Statistic"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#pooled-standard-deviation",
    "href": "lectures/01-lecture-slides.html#pooled-standard-deviation",
    "title": "Quantitative Analysis 1 - Crash Course",
    "section": "Pooled Standard Deviation",
    "text": "Pooled Standard Deviation"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#standard-error",
    "href": "lectures/01-lecture-slides.html#standard-error",
    "title": "Quantitative Analysis 1 - Crash Course",
    "section": "Standard Error",
    "text": "Standard Error"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#calculating-the-p-value",
    "href": "lectures/01-lecture-slides.html#calculating-the-p-value",
    "title": "Quantitative Analysis 1 - Crash Course",
    "section": "Calculating the P-Value",
    "text": "Calculating the P-Value"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#building-a-confidence-interval",
    "href": "lectures/01-lecture-slides.html#building-a-confidence-interval",
    "title": "An Introduction to Statistics & Programming",
    "section": "Building a Confidence Interval",
    "text": "Building a Confidence Interval\nGenerally, to build a confidence interval, you need three pieces of information:\n\nPoint estimate to build the interval around\nThe probability distribution that best approximates the estimate’s sampling distribution (almost always the normal distribution)\nThe Standard Error of the estimate (the standard deviation of the sampling distribution)\nThe level of “confidence” (i.e. how confident you are that the population parameter is contained in the interval)"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#connection-between-p-values-confidence-intervals",
    "href": "lectures/01-lecture-slides.html#connection-between-p-values-confidence-intervals",
    "title": "Quantitative Analysis 1 - Crash Course",
    "section": "Connection Between P-Values & Confidence Intervals",
    "text": "Connection Between P-Values & Confidence Intervals"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#significance-tests-decision-errors",
    "href": "lectures/01-lecture-slides.html#significance-tests-decision-errors",
    "title": "An Introduction to Programming & Statistics",
    "section": "Significance Tests & Decision Errors",
    "text": "Significance Tests & Decision Errors"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#statistical-significance-vs-practical-significance",
    "href": "lectures/01-lecture-slides.html#statistical-significance-vs-practical-significance",
    "title": "An Introduction to Programming & Statistics",
    "section": "Statistical Significance vs Practical Significance",
    "text": "Statistical Significance vs Practical Significance"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#weaknesses-of-signficance-tests-p-values",
    "href": "lectures/01-lecture-slides.html#weaknesses-of-signficance-tests-p-values",
    "title": "An Introduction to Programming & Statistics",
    "section": "Weaknesses of Signficance Tests & P-Values",
    "text": "Weaknesses of Signficance Tests & P-Values"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#installing-loading-packages",
    "href": "lectures/01-lecture-slides.html#installing-loading-packages",
    "title": "An Introduction to Statistics & Programming",
    "section": "Installing & Loading Packages",
    "text": "Installing & Loading Packages\nYou can use install.packages to install a package once and then library to load that package and gain access to all of its functions.\n\ninstall.packages(\"package_name\")\nlibrary(package_name)"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#selecting-data-from-data-frames",
    "href": "lectures/01-lecture-slides.html#selecting-data-from-data-frames",
    "title": "An Introduction to Statistics & Programming",
    "section": "Selecting Data from Data Frames",
    "text": "Selecting Data from Data Frames\nYou will mainly select data from data frames using one of the two following methods:\n\ndata_frame_1[1, 1] # Index the row and/or column\n\n[1] 1\n\ndata_frame_1[, 1] # Leaving the column or row index blank selects the whole vector\n\n[1] 1 3\n\ndata_frame_1$NUMERIC # Use a $ operator to reference the column name\n\n[1] 1 3"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#na-values-missing-data",
    "href": "lectures/01-lecture-slides.html#na-values-missing-data",
    "title": "An Introduction to Programming & Statistics",
    "section": "NA Values: Missing Data",
    "text": "NA Values: Missing Data\nFor a number of reasons, uou will often having missing data in the datasets you create. R encodes missing data as NA and often interacts differently with vectors that contain NA values compared to vectors that do not contain any NA values.\n\nno_na_value &lt;- c(1, 3, 5)\nmean(no_na_value)\n\n[1] 3\n\nna_value &lt;- c(NA, 3, 5)\nmean(na_value)\n\n[1] NA\n\nmean(na_value, na.rm = TRUE) # Drops NA value from calculation.\n\n[1] 4"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#reading-and-writing-data",
    "href": "lectures/01-lecture-slides.html#reading-and-writing-data",
    "title": "An Introduction to Statistics & Programming",
    "section": "Reading and Writing Data",
    "text": "Reading and Writing Data\nThere are a number of different methods to read and write data into R. The two most common functions are:\n\ndata &lt;- read.csv(\"filepath/file-name.csv\")\n\nwrite.csv(data, \"filepath/file-name.csv\")"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#importing-data-from-an-r-package",
    "href": "lectures/01-lecture-slides.html#importing-data-from-an-r-package",
    "title": "An Introduction to Statistics & Programming",
    "section": "Importing Data from an R Package",
    "text": "Importing Data from an R Package\nOftentimes, R packages will come with their own datasets that we can load into R. The peopleanalytics package has many such datasets that we will use today:\n\ndata_employees &lt;- peopleanalytics::employees"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#installing-packages-from-the-tidyverse",
    "href": "lectures/01-lecture-slides.html#installing-packages-from-the-tidyverse",
    "title": "An Introduction to Statistics & Programming",
    "section": "Installing Packages from the Tidyverse",
    "text": "Installing Packages from the Tidyverse\n\ninstall.packages(\"tidyverse\")"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#the-philosophy-of-dplyr-functions",
    "href": "lectures/01-lecture-slides.html#the-philosophy-of-dplyr-functions",
    "title": "An Introduction to Statistics & Programming",
    "section": "The Philosophy of dplyr Functions",
    "text": "The Philosophy of dplyr Functions\nEvery function in dplyr follows this philosophy:\n\nFirst argument is always a data frame.\nRemaining arguments are usually names of columns on which to operate.\nThe output is always a new data frame (tibble).\n\ndplyr functions are also further grouped by whether they operate on rows, columns, groups, or tables."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#using-dplyr-to-operate-on-rows",
    "href": "lectures/01-lecture-slides.html#using-dplyr-to-operate-on-rows",
    "title": "An Introduction to Statistics & Programming",
    "section": "Using dplyr to Operate on Rows",
    "text": "Using dplyr to Operate on Rows\nThe following dplyr functions can filter, reduce, or reorder the rows of a data frame:\n\ndplyr::filter(data_employees_tbl, job_level %in% c(4, 5))\n\ndplyr::distinct(data_employees_tbl, ed_lvl, ed_field)\n\ndplyr::arrange(data_employees_tbl, work_exp)"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#using-dplyr-to-operate-on-columns",
    "href": "lectures/01-lecture-slides.html#using-dplyr-to-operate-on-columns",
    "title": "An Introduction to Statistics & Programming",
    "section": "Using dplyr to Operate on Columns",
    "text": "Using dplyr to Operate on Columns\nThe following dplyr functions can select, rename, add/change, or relocate the columns of a data frame:\n\ndplyr::select(data_employees_tbl, dept)\n\ndplyr::rename(data_employees_tbl, job_level = job_lvl)\n\ndplyr::mutate(data_employees_tbl, salary = monthly_comp * 12)\n\ndplyr::relocate(data_employees_tbl, job_lvl, .before = employee_id)"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#using-dplyr-to-operate-on-groups",
    "href": "lectures/01-lecture-slides.html#using-dplyr-to-operate-on-groups",
    "title": "An Introduction to Statistics & Programming",
    "section": "Using dplyr to Operate on Groups",
    "text": "Using dplyr to Operate on Groups\nThe following dplyr functions can group and summarize your data by a predefined group indicator:\n\ndata_employees_tbl |&gt;\n  dplyr::group_by(\n    job_lvl\n  ) |&gt;\n  dplyr::summarize(\n    annual_comp_mean = mean(annual_comp),\n    annual_comp_median = median(annual_comp)\n  )\n\nIn this code chunk, we have grouped by an employee’s job level and summarized their annual salary by job level."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#using-dplyr-to-operate-on-tables",
    "href": "lectures/01-lecture-slides.html#using-dplyr-to-operate-on-tables",
    "title": "An Introduction to Statistics & Programming",
    "section": "Using dplyr to Operate on Tables",
    "text": "Using dplyr to Operate on Tables\nThe followingdplyr functions can be used to join different tables (data frames) together by a unique identifier:\n\ndata_job &lt;- peopleanalytics::job |&gt; tibble::as_tibble()\n\ndata_payroll &lt;- peopleanalytics::payroll |&gt; tibble::as_tibble()\n\ndata_job_payroll &lt;- \n  data_job |&gt;\n  dplyr::left_join(\n    data_payroll,\n    by = \"employee_id\"\n  )"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#description",
    "href": "lectures/01-lecture-slides.html#description",
    "title": "An Introduction to Statistics & Programming",
    "section": "Description",
    "text": "Description\nDescription focuses on how to summarize the raw data without losing too much information. Descriptive statistics are statistics calculated from the raw data that summarize all (or most) of the information contained in the data:\n\nMean, median, and mode\nVariance and standard deviation\nCumulative distribution of the data"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#inference",
    "href": "lectures/01-lecture-slides.html#inference",
    "title": "An Introduction to Statistics & Programming",
    "section": "Inference",
    "text": "Inference\nInference focuses on how to make evaluations (generalizations) from the data that take into consideration the uncertainty present in the data. These data-based evaluations take the form of:\n\nPredictions\nInterval and point estimates\nProbability (P) values"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#populations-samples",
    "href": "lectures/01-lecture-slides.html#populations-samples",
    "title": "An Introduction to Statistics & Programming",
    "section": "Populations & Samples",
    "text": "Populations & Samples\nThe purpose of most analyses is to learn something about the population from the collected data or sample.\n\nA population is the collection of every unit or subject (e.g. person) that one wishes to generalize to from the results of their study.\nA sample is the actual collected data one is using to make these generalizations."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#types-of-variables-by-measurement-scale",
    "href": "lectures/01-lecture-slides.html#types-of-variables-by-measurement-scale",
    "title": "An Introduction to Statistics & Programming",
    "section": "Types of Variables by Measurement Scale",
    "text": "Types of Variables by Measurement Scale\nWe can classify variables into two broad categories based on their measurement scale–the types of values the variable can take on:\n\nQuantitative: Values are numbers\nCategorical: Values are categories"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#types-of-variables-by-measurement-scale-1",
    "href": "lectures/01-lecture-slides.html#types-of-variables-by-measurement-scale-1",
    "title": "An Introduction to Statistics & Programming",
    "section": "Types of Variables by Measurement Scale",
    "text": "Types of Variables by Measurement Scale\n\nset.seed(4)\ndata_employees_tbl |&gt; \n  dplyr::sample_n(5) |&gt;\n  dplyr::select(\n    job_tenure,\n    dept\n  )\n\n# A tibble: 5 × 2\n  job_tenure dept                  \n       &lt;int&gt; &lt;chr&gt;                 \n1          0 Research & Development\n2          2 Sales                 \n3          3 Sales                 \n4          0 Sales                 \n5          0 Sales"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#types-of-quantitative-variables-1",
    "href": "lectures/01-lecture-slides.html#types-of-quantitative-variables-1",
    "title": "An Introduction to Statistics & Programming",
    "section": "Types of Quantitative Variables",
    "text": "Types of Quantitative Variables\n\nset.seed(4)\ndata_employees_tbl |&gt;\n  dplyr::sample_n(5) |&gt;\n  dplyr::select(\n    trainings,\n    commute_dist\n  )\n\n# A tibble: 5 × 2\n  trainings commute_dist\n      &lt;int&gt;        &lt;int&gt;\n1         4            9\n2         4           20\n3         2            1\n4         2           19\n5         0           12"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#types-of-categorical-variables-1",
    "href": "lectures/01-lecture-slides.html#types-of-categorical-variables-1",
    "title": "An Introduction to Statistics & Programming",
    "section": "Types of Categorical Variables",
    "text": "Types of Categorical Variables\n\nset.seed(4)\ndata_employees_tbl |&gt;\n  dplyr::sample_n(5) |&gt;\n  dplyr::select(\n    overtime,\n    ed_field\n  )\n\n# A tibble: 5 × 2\n  overtime ed_field     \n  &lt;chr&gt;    &lt;chr&gt;        \n1 No       Life Sciences\n2 No       Life Sciences\n3 No       Life Sciences\n4 Yes      Marketing    \n5 No       Life Sciences"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#data-collection",
    "href": "lectures/01-lecture-slides.html#data-collection",
    "title": "An Introduction to Statistics & Programming",
    "section": "Data Collection",
    "text": "Data Collection\nThe strength of the inferences you can make depends on the quality of your data. The quality of your data is very dependent on the method used to collect it:\n\nExperiments\nObservational Studies"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#experiments",
    "href": "lectures/01-lecture-slides.html#experiments",
    "title": "An Introduction to Statistics & Programming",
    "section": "Experiments",
    "text": "Experiments\nIn experiments—also known as randomized control trials (RCTs)—data are collected by randomly assigning subjects to an experimental trial or condition, then collecting the subsequent outcome data.\nBy randomly assigning subjects to conditions, you are effectively ensuring that any differences in the outcome variable by condition is due solely to the condition not to any other lurking variable."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#statistic-vs-population-parameter-1",
    "href": "lectures/01-lecture-slides.html#statistic-vs-population-parameter-1",
    "title": "An Introduction to Statistics & Programming",
    "section": "Statistic vs Population Parameter",
    "text": "Statistic vs Population Parameter"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#a-reassuring-reminder",
    "href": "lectures/01-lecture-slides.html#a-reassuring-reminder",
    "title": "An Introduction to Statistics & Programming",
    "section": "A Reassuring Reminder",
    "text": "A Reassuring Reminder\n\nStatistics is hard, especially when effects are small and variable and measurements are noisy.\n\n— McShane et al. (2019)"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#describing-the-center-of-your-data-1",
    "href": "lectures/01-lecture-slides.html#describing-the-center-of-your-data-1",
    "title": "An Introduction to Statistics & Programming",
    "section": "Describing the Center of your Data",
    "text": "Describing the Center of your Data"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#describing-the-center-of-your-data-mean",
    "href": "lectures/01-lecture-slides.html#describing-the-center-of-your-data-mean",
    "title": "Stats Bootcamp",
    "section": "Describing the Center of your Data: Mean",
    "text": "Describing the Center of your Data: Mean\n\\(\\frac{\\Sigma{y_i}}{n}\\)"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#describing-the-spread-of-your-data-1",
    "href": "lectures/01-lecture-slides.html#describing-the-spread-of-your-data-1",
    "title": "An Introduction to Statistics & Programming",
    "section": "Describing the Spread of your Data",
    "text": "Describing the Spread of your Data"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#describing-the-shape-of-your-data-1",
    "href": "lectures/01-lecture-slides.html#describing-the-shape-of-your-data-1",
    "title": "An Introduction to Statistics & Programming",
    "section": "Describing the Shape of your Data",
    "text": "Describing the Shape of your Data"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#using-r-to-describe-your-data",
    "href": "lectures/01-lecture-slides.html#using-r-to-describe-your-data",
    "title": "Stats Bootcamp",
    "section": "Using R to Describe your Data",
    "text": "Using R to Describe your Data"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#correlation-coefficient",
    "href": "lectures/01-lecture-slides.html#correlation-coefficient",
    "title": "An Introduction to Statistics & Programming",
    "section": "Correlation Coefficient",
    "text": "Correlation Coefficient\nThe correlation coefficient is just a standardized version of the covariance statistic with values that range from -1 to 1.\n\nPositive Correlation: High (low) values of one variable, X, are frequently seen with high (low) values of another variable.\nNegative Correlation: High (low) values of one variable, X, are frequently seen with low (high) values of another variable.\n\n\nset.seed(324)\nx &lt;- rnorm(100, sd = 50)\ny &lt;- 1 * x + rnorm(100, sd = 100)\n\ncov(x, y) |&gt; round(2)\n\n[1] 2522.9\n\ncor(x, y) |&gt; round(2)\n\n[1] 0.45"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#plotting-the-relationship-between-two-variables",
    "href": "lectures/01-lecture-slides.html#plotting-the-relationship-between-two-variables",
    "title": "An Introduction to Statistics & Programming",
    "section": "Plotting the Relationship Between two Variables",
    "text": "Plotting the Relationship Between two Variables"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#importance-of-plots",
    "href": "lectures/01-lecture-slides.html#importance-of-plots",
    "title": "An Introduction to Statistics & Programming",
    "section": "Importance of Plots",
    "text": "Importance of Plots\nPlotting your data immediately gives you more information than looking at the raw numbers:\n\nVisual information about the center, spread, and shape of your data.\nAlert you to outlier values."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#thinking-in-disributions",
    "href": "lectures/01-lecture-slides.html#thinking-in-disributions",
    "title": "An Introduction to Statistics & Programming",
    "section": "Thinking in Disributions",
    "text": "Thinking in Disributions\nThe distribution of a given variable gives the frequency of each value of the variable. This frequency can be in either:\n\nAbsolute terms: Count of observations\nRelative terms: Proportion or percent of observations\n\nA variable’s distribution completely describes the variable."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#examples-of-plots",
    "href": "lectures/01-lecture-slides.html#examples-of-plots",
    "title": "An Introduction to Statistics & Programming",
    "section": "Examples of Plots",
    "text": "Examples of Plots\n\nset.seed(2311)\nx &lt;- rnorm(1000)\ndata &lt;- tibble::tibble(x = x)"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#probability-of-a-coin-flip",
    "href": "lectures/01-lecture-slides.html#probability-of-a-coin-flip",
    "title": "Quantitative Analysis 1 - Crash Course",
    "section": "Probability of a Coin Flip",
    "text": "Probability of a Coin Flip\nYou flip a coin 100 times. Of those 100 flips, 78 were heads and 22 were tails, so we can (not so safely) say that the probability of this coin coming up heads is 78%."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#the-three-rules-of-probability",
    "href": "lectures/01-lecture-slides.html#the-three-rules-of-probability",
    "title": "An Introduction to Statistics & Programming",
    "section": "The Three Rules of Probability",
    "text": "The Three Rules of Probability\nAll of probability theory rests on three rules:\n\n\\(P(\\text{Event}) \\geq 0\\)\n\\(P(\\text{Any Event}) = 1\\)\nIf two events are mutually exclusive, then the probability of event one or event two happening is equal to \\(P(\\text{Event 1}) + P(\\text{Event 2})\\)"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#a-concrete-example",
    "href": "lectures/01-lecture-slides.html#a-concrete-example",
    "title": "An Introduction to Statistics & Programming",
    "section": "A Concrete Example",
    "text": "A Concrete Example\nYou’re a Human Capital Analytics researcher at a large, multinational organization and you have access to all of the firm’s HR data over the past fiscal year, which includes three key variables: voluntary_turnover, job_satisfacation, and office_region.\nYour manager asks you to determine the likelihood that an employee leaves the firm. How do you approach this project?"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#probability-of-voluntary-turnover",
    "href": "lectures/01-lecture-slides.html#probability-of-voluntary-turnover",
    "title": "An Introduction to Statistics & Programming",
    "section": "Probability of Voluntary Turnover",
    "text": "Probability of Voluntary Turnover\nA quick way to determine the probability of voluntary turnover is to look at the proportion of employees who left the firm in the last year. This proportion is .16, so the \\(P(\\text{Status = Inactive})=.16\\)."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#a-concrete-example-1",
    "href": "lectures/01-lecture-slides.html#a-concrete-example-1",
    "title": "An Introduction to Statistics & Programming",
    "section": "A Concrete Example",
    "text": "A Concrete Example\n\n\n# A tibble: 20,000 × 3\n   voluntary_turnover job_satisfaction office_region\n   &lt;chr&gt;              &lt;fct&gt;            &lt;fct&gt;        \n 1 active             Neutral          China        \n 2 active             Satisfied        Latin Am.    \n 3 active             Satisfied        North America\n 4 inactive           Neutral          North America\n 5 active             Neutral          China        \n 6 active             Neutral          China        \n 7 inactive           Neutral          Latin Am.    \n 8 active             Dissatisfied     Europe       \n 9 active             Neutral          Europe       \n10 active             Dissatisfied     Europe       \n# ℹ 19,990 more rows"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#employment-status-job-satisfaction-joint-probability",
    "href": "lectures/01-lecture-slides.html#employment-status-job-satisfaction-joint-probability",
    "title": "An Introduction to Statistics & Programming",
    "section": "Employment Status & Job Satisfaction: Joint Probability",
    "text": "Employment Status & Job Satisfaction: Joint Probability\n\n\n\n\n\n\nactive\ninactive\n\n\n\n\nDissatisfied\n0.06\n0.04\n\n\nNeutral\n0.50\n0.10\n\n\nSatisfied\n0.28\n0.02"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#employment-status-job-satisfaction-joint-probability-1",
    "href": "lectures/01-lecture-slides.html#employment-status-job-satisfaction-joint-probability-1",
    "title": "Quantitative Analysis 1 - Crash Course",
    "section": "Employment Status & Job Satisfaction: Joint Probability",
    "text": "Employment Status & Job Satisfaction: Joint Probability\n\n\n\n\n\n\nactive\ninactive\n\n\n\n\nDissatisfied\n0.06\n0.04\n\n\nNeutral\n0.50\n0.10\n\n\nSatisfied\n0.28\n0.02"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#emplyment-status-given-job-satisfaction-conditional-probability",
    "href": "lectures/01-lecture-slides.html#emplyment-status-given-job-satisfaction-conditional-probability",
    "title": "Quantitative Analysis 1 - Crash Course",
    "section": "Emplyment Status Given Job Satisfaction: Conditional Probability",
    "text": "Emplyment Status Given Job Satisfaction: Conditional Probability\nConditional probability is the probability of one event occurring given the occurence of another event. This is written mathematically as:\n\\[P(\\text{Event 1} \\space | \\space \\text{Event 2})\\]\nwhich is read as the probability of Event 1 conditional on (or given) Event 2."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#employment-status-given-job-satisfaction-conditional-probability",
    "href": "lectures/01-lecture-slides.html#employment-status-given-job-satisfaction-conditional-probability",
    "title": "An Introduction to Statistics & Programming",
    "section": "Employment Status Given Job Satisfaction: Conditional Probability",
    "text": "Employment Status Given Job Satisfaction: Conditional Probability\nWhat is the probability that an employee’s status is inactive given that they had responded they were dissatisfied with their job on an an earlier attitude survey? What happens to this probability as job satisfaction moves from dissatisfied to satisfied?\n\n\n\n\n\n\nactive\ninactive\n\n\n\n\nDissatisfied\n0.63\n0.37\n\n\nNeutral\n0.84\n0.16\n\n\nSatisfied\n0.93\n0.07"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#employment-status-given-job-satisfaction-conditional-probability-1",
    "href": "lectures/01-lecture-slides.html#employment-status-given-job-satisfaction-conditional-probability-1",
    "title": "Quantitative Analysis 1 - Crash Course",
    "section": "Employment Status Given Job Satisfaction: Conditional Probability",
    "text": "Employment Status Given Job Satisfaction: Conditional Probability\nWhat is the probability that an employee’s status is inactive given that they had responded they were disstaisfied with their job on an an earlier attitude survey? What happens to this probability as job satisfaction moves from dissatisfied to satisfied?\n\n\n\n\n\n\nactive\ninactive\n\n\n\n\nDissatisfied\n0.63\n0.37\n\n\nNeutral\n0.84\n0.16\n\n\nSatisfied\n0.93\n0.07"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#independent-probabilities",
    "href": "lectures/01-lecture-slides.html#independent-probabilities",
    "title": "Quantitative Analysis 1 - Crash Course",
    "section": "Independent Probabilities",
    "text": "Independent Probabilities\nTwo events are said to be independent if the probability of one event occurring does not change given the occurence of the other event:\n\\[P(\\text{Event 1} \\space | \\space \\text{Event 2})=P(\\text{Event 1)}\\]"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#office-region-job-satisfaction-independent-probabilities",
    "href": "lectures/01-lecture-slides.html#office-region-job-satisfaction-independent-probabilities",
    "title": "Quantitative Analysis 1 - Crash Course",
    "section": "Office Region & Job Satisfaction: Independent Probabilities",
    "text": "Office Region & Job Satisfaction: Independent Probabilities\nHow does the probability of an employee’s job satisfaction response change depending on the region they’re working in? Or does it change?\n\n\n\n\n\n\nDissatisfied\nNeutral\nSatisfied\n\n\n\n\nNorth America\n0.10\n0.59\n0.31\n\n\nAsia\n0.11\n0.62\n0.27\n\n\nChina\n0.11\n0.59\n0.30\n\n\nEEMEA\n0.10\n0.59\n0.31\n\n\nEurope\n0.09\n0.61\n0.29\n\n\nLatin Am.\n0.10\n0.60\n0.29"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#independent-events",
    "href": "lectures/01-lecture-slides.html#independent-events",
    "title": "An Introduction to Statistics & Programming",
    "section": "Independent Events",
    "text": "Independent Events\nTwo events are said to be independent if the probability of one event occurring does not change given the occurrence of the other event:\n\\[P(\\text{Event 1} \\space | \\space \\text{Event 2})=P(\\text{Event 1)}\\]"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#office-region-job-satisfaction-independent-events",
    "href": "lectures/01-lecture-slides.html#office-region-job-satisfaction-independent-events",
    "title": "An Introduction to Statistics & Programming",
    "section": "Office Region & Job Satisfaction: Independent Events",
    "text": "Office Region & Job Satisfaction: Independent Events\nHow does the probability of an employee’s job satisfaction response change depending on the region they’re working in? Or does it change?\n\n\n\n\n\n\nDissatisfied\nNeutral\nSatisfied\n\n\n\n\nNorth America\n0.10\n0.59\n0.31\n\n\nAsia\n0.11\n0.62\n0.27\n\n\nChina\n0.11\n0.59\n0.30\n\n\nEEMEA\n0.10\n0.59\n0.31\n\n\nEurope\n0.09\n0.61\n0.29\n\n\nLatin Am.\n0.10\n0.60\n0.29"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#random-variables",
    "href": "lectures/01-lecture-slides.html#random-variables",
    "title": "An Introduction to Statistics & Programming",
    "section": "Random Variables",
    "text": "Random Variables\nWhether you realize it or not, we have been talking about voluntary_turnover as a random variable.\nA random variable is a function of a random phenomenon that maps an outcome of that phenomenon to a real number.\nIn our example, voluntary_turnover is a random variable that maps the outcome of an employee’s decision to leave or remain with their organization to a real number: 1 or 0."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#voluntary-turnover-as-a-random-variable",
    "href": "lectures/01-lecture-slides.html#voluntary-turnover-as-a-random-variable",
    "title": "An Introduction to Statistics & Programming",
    "section": "Voluntary Turnover as a Random Variable",
    "text": "Voluntary Turnover as a Random Variable\nAs a random variable, voluntary_turnover maps inactive to 1 and active to 0:\n\\[\\text{Y}(\\text{inactive})=1 \\\\ \\text{Y}(\\text{active})=0\\]\nBecause the random variable is a function of a random phenomenon, we can still calculate probabilities for the outcome:\n\\[P(\\text{Y}=1)=.16 \\\\ P(\\text{Y}=0) = .84\\]"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#connecting-probability-to-statistics-with-random-variables",
    "href": "lectures/01-lecture-slides.html#connecting-probability-to-statistics-with-random-variables",
    "title": "An Introduction to Statistics & Programming",
    "section": "Connecting Probability to Statistics with Random Variables",
    "text": "Connecting Probability to Statistics with Random Variables\nThe big gain from introducing random variables is that we can now apply mathematical and statistical models to the numerical values, and we can use more general probability distributions to describe the distributions of these random variables.\nFor instance, we can say voluntary_turnover can be modeled using a binomial distribution."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#probability-distributions",
    "href": "lectures/01-lecture-slides.html#probability-distributions",
    "title": "An Introduction to Statistics & Programming",
    "section": "Probability Distributions",
    "text": "Probability Distributions\nProbability distributions are mathematical models that can be used to summarize the random variation in the random variables by specifying the probabilities of all possible outcomes of the random variable."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#modeling-voluntary-turnover-with-a-binomial-distribution",
    "href": "lectures/01-lecture-slides.html#modeling-voluntary-turnover-with-a-binomial-distribution",
    "title": "An Introduction to Statistics & Programming",
    "section": "Modeling Voluntary Turnover with a Binomial Distribution",
    "text": "Modeling Voluntary Turnover with a Binomial Distribution\nThe Bernoulli distribution is a probability distribution that can be used to model a random variable that has two outcomes. It specifies the probability of the first outcome, 1, as p and the second outcome, as 1 - p:\n\\[\\begin{equation}\n    f(\\text{Employment Status};p) =\n    \\left\\{\n        \\begin{array}{cc}\n                p & \\mathrm{if\\ } status=1 \\\\\n                1-p & \\mathrm{if\\ } status=0 \\\\\n        \\end{array}\n    \\right.\n\\end{equation}\\]"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#plotting-pmfs-and-pdfs",
    "href": "lectures/01-lecture-slides.html#plotting-pmfs-and-pdfs",
    "title": "An Introduction to Statistics & Programming",
    "section": "Plotting PMFs and PDFs",
    "text": "Plotting PMFs and PDFs"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#cumulative-distribution-function-1",
    "href": "lectures/01-lecture-slides.html#cumulative-distribution-function-1",
    "title": "An Introduction to Statistics & Programming",
    "section": "Cumulative Distribution Function",
    "text": "Cumulative Distribution Function"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#example-recruiters-trust-in-ai",
    "href": "lectures/01-lecture-slides.html#example-recruiters-trust-in-ai",
    "title": "An Introduction to Statistics & Programming",
    "section": "Example: Recruiters’ trust in AI",
    "text": "Example: Recruiters’ trust in AI\nYour organization is considering adopting an AI automated resume scraper program to lessen the burden on the recruiters. Before committing to the tool, however, your manager has asked you to determine if the recruiters would trust the outcomes provided by the AI system.\nTo assess this, you administer a single survey question to three random samples of 50 recruiters:\n\nI trust the outcome provided by an artificially intelligent system."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#population-sample-response-distributions",
    "href": "lectures/01-lecture-slides.html#population-sample-response-distributions",
    "title": "Quantitative Analysis 1 - Crash Course",
    "section": "Population & Sample Response Distributions",
    "text": "Population & Sample Response Distributions"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#example-do-recruiters-trust-ai",
    "href": "lectures/01-lecture-slides.html#example-do-recruiters-trust-ai",
    "title": "An Introduction to Statistics & Programming",
    "section": "Example: Do Recruiters’ trust AI?",
    "text": "Example: Do Recruiters’ trust AI?\nTo determine how recruiters feel about AI, on average, you compute the mean of each sample and find the following:\n\n\n\n\n\nsample_1\nsample_2\nsample_3\n\n\n\n\n3.74\n3.76\n3.82\n\n\n\n\n\nThe average response is different across the three different samples. Is this expected? What should you do?"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#populations-sample-variation",
    "href": "lectures/01-lecture-slides.html#populations-sample-variation",
    "title": "An Introduction to Statistics & Programming",
    "section": "Populations & Sample Variation",
    "text": "Populations & Sample Variation"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#statistics-as-random-variables",
    "href": "lectures/01-lecture-slides.html#statistics-as-random-variables",
    "title": "An Introduction to Statistics & Programming",
    "section": "Statistics as Random Variables",
    "text": "Statistics as Random Variables\nBecause statistics like the sample mean are computed from a sample that contains random variation, we expect our statistics to behave like random variables.\nLike a random variable, we can specify a probability distribution for our statistic called a sampling distribution."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#the-mean-standard-deviation-of-a-sampling-distribution",
    "href": "lectures/01-lecture-slides.html#the-mean-standard-deviation-of-a-sampling-distribution",
    "title": "An Introduction to Statistics & Programming",
    "section": "The Mean & Standard Deviation of a Sampling Distribution",
    "text": "The Mean & Standard Deviation of a Sampling Distribution\nLike all distributions, we can compute the mean and standard deviation of a sampling distribution and obtain useful information:\n\nMean of a sampling distribution = Population Parameter\nStandard deviation of a sampling distribution = Uncertainty in our statistic"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#the-standard-error-1",
    "href": "lectures/01-lecture-slides.html#the-standard-error-1",
    "title": "An Introduction to Statistics & Programming",
    "section": "The Standard Error",
    "text": "The Standard Error\nWe can reduce the standard error, thereby reducing our uncertainty, by increasing our sample size:"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#the-norm-of-normality-the-central-limit-theorem",
    "href": "lectures/01-lecture-slides.html#the-norm-of-normality-the-central-limit-theorem",
    "title": "An Introduction to Statistics & Programming",
    "section": "The Norm of Normality: The Central Limit Theorem",
    "text": "The Norm of Normality: The Central Limit Theorem\nThe Central Limit Theorem (CLT) is a mathematical finding that tells us that the sampling distribution of a statistic like the mean starts to closely resemble a normal distribution as the sample size increases regardless of the distribution of the sample data itself!\nThe CLT plays a very important role in all of our statistical inference!"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#what-is-statistical-estimation",
    "href": "lectures/01-lecture-slides.html#what-is-statistical-estimation",
    "title": "An Introduction to Statistics & Programming",
    "section": "What is Statistical Estimation?",
    "text": "What is Statistical Estimation?\nThe goal of every data analytic project is to estimate some population parameter by computing some statistic or point estimate. This is statistical estimation."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#point-estimates-interval-estimates",
    "href": "lectures/01-lecture-slides.html#point-estimates-interval-estimates",
    "title": "An Introduction to Statistics & Programming",
    "section": "Point Estimates & Interval Estimates",
    "text": "Point Estimates & Interval Estimates\nStatistical estimates can either come as point estimates or interval estimates:\n\nPoint Estimate: A single value that estimates the population parameter.\nInterval Estimate: An interval of values centered around the point estimate."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#differences-in-recruiters-trust-in-ai-by-job-experience",
    "href": "lectures/01-lecture-slides.html#differences-in-recruiters-trust-in-ai-by-job-experience",
    "title": "An Introduction to Statistics & Programming",
    "section": "Differences in Recruiters’ trust in AI by Job Experience",
    "text": "Differences in Recruiters’ trust in AI by Job Experience\nYour manager asks you to administer the survey one more time to a larger sample of 300 recruiters, but this time they would like you to measure the amount of years the employee has been in the recruiting industry as a proxy for job experience job_exp.\nYour manager would like to know if employees’ trust differs based on their level of job experience. How should you approach this project?"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#plotting-your-data",
    "href": "lectures/01-lecture-slides.html#plotting-your-data",
    "title": "An Introduction to Statistics & Programming",
    "section": "Plotting your Data",
    "text": "Plotting your Data"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#estimating-the-mean-variance-by-job-experience",
    "href": "lectures/01-lecture-slides.html#estimating-the-mean-variance-by-job-experience",
    "title": "An Introduction to Statistics & Programming",
    "section": "Estimating the Mean & Variance by Job Experience",
    "text": "Estimating the Mean & Variance by Job Experience\nFor each job experience group, we can estimate the population means by computing the sample mean and standard deviation of their responses to the trust in AI question:\n\n\n\n\n\nJob Exp.\nMean Trust\nSD Trust\nN\n\n\n\n\nLow\n4.33\n0.90\n100\n\n\nMedium\n4.28\n0.78\n100\n\n\nHigh\n2.94\n1.02\n100"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#a-confidence-interval-estimate-for-the-mean",
    "href": "lectures/01-lecture-slides.html#a-confidence-interval-estimate-for-the-mean",
    "title": "An Introduction to Statistics & Programming",
    "section": "A Confidence (Interval) Estimate for the Mean",
    "text": "A Confidence (Interval) Estimate for the Mean\nBecause there is uncertainty in our data, we would like to move away from providing a single estimate of trust in AI for each group and provide an interval of estimates that adequately quantifies the uncertainty we have in our estimate:\n\n\n\n\n\nJob Exp.\nMean Trust\n95% Conf. Int.\nSE\nSD Trust\nN\n\n\n\n\nLow\n4.33\n4.15 - 4.51\n0.090\n0.90\n100\n\n\nMedium\n4.28\n4.13 - 4.43\n0.078\n0.78\n100\n\n\nHigh\n2.94\n2.74 - 3.14\n0.102\n1.02\n100"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#the-cdf-of-the-normal-distribution",
    "href": "lectures/01-lecture-slides.html#the-cdf-of-the-normal-distribution",
    "title": "An Introduction to Statistics & Programming",
    "section": "The CDF of the Normal Distribution",
    "text": "The CDF of the Normal Distribution\nThink about the CDF as a way to compute the percentiles of a distribution. What is the 50th percentile—the value where 50% or less of the observations fall—for the Normal CDF below?"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#a-99-confidence-interval",
    "href": "lectures/01-lecture-slides.html#a-99-confidence-interval",
    "title": "An Introduction to Programming & Statistics",
    "section": "A 99% Confidence Interval",
    "text": "A 99% Confidence Interval"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#getting-to-know-the-normal-distribution-better",
    "href": "lectures/01-lecture-slides.html#getting-to-know-the-normal-distribution-better",
    "title": "An Introduction to Statistics & Programming",
    "section": "Getting to Know the Normal Distribution Better",
    "text": "Getting to Know the Normal Distribution Better\n\nIt has two parameters: Mean & Variance.\n68% of its mass is between \\(\\pm1\\) SDs from its mean, 95% of its mass is between \\(\\pm2\\) SDs from its mean, and 99.7% of its mass is between \\(\\pm3\\) SDs from its mean."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#building-a-confidence-interval-1",
    "href": "lectures/01-lecture-slides.html#building-a-confidence-interval-1",
    "title": "An Introduction to Statistics & Programming",
    "section": "Building a Confidence Interval",
    "text": "Building a Confidence Interval\n\\[\\overline{Y} \\space \\pm \\space 1.96 \\times\\text{SE}\\]\n\n\\(\\overline{Y}\\): Point estimate (sample mean)\n\\(\\pm 1.96\\) is the value at which 95% of the mass of the standard normal distribution falls\nSE: The standard error of the estimate"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#are-the-differences-in-recruiters-trust-in-ai-by-job-experience-real",
    "href": "lectures/01-lecture-slides.html#are-the-differences-in-recruiters-trust-in-ai-by-job-experience-real",
    "title": "An Introduction to Statistics & Programming",
    "section": "Are the Differences in Recruiters’ trust in AI by Job Experience Real?",
    "text": "Are the Differences in Recruiters’ trust in AI by Job Experience Real?\nWe saw that the sample means of trust in AI differed by job experience level, but are those differences real or are they a result of random noise (sample variation)?\nWe can use a statistical test to determine if the difference we see in the sample means is indicative of true population-level differences."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#stating-a-statistical-hypothesis",
    "href": "lectures/01-lecture-slides.html#stating-a-statistical-hypothesis",
    "title": "An Introduction to Statistics & Programming",
    "section": "Stating a Statistical Hypothesis",
    "text": "Stating a Statistical Hypothesis\nIn statistics, a hypothesis is a statement about the population distribution. Researchers typically formulate two kinds of hypotheses: null hypothesis (\\(H_{0}\\)) and alternative (researcher’s) hypothesis (\\(H_{a}\\)).\n\n\\(H_{0}\\) is a statement that the population parameter takes on some value—usually 0.\n\\(H_{a}\\) is a statement that the population parameters takes on an alternative set of values that fit with the researcher’s theory."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#trust-in-ai-hypotheses",
    "href": "lectures/01-lecture-slides.html#trust-in-ai-hypotheses",
    "title": "An Introduction to Statistics & Programming",
    "section": "Trust in AI Hypotheses",
    "text": "Trust in AI Hypotheses\n\\(H_{0}\\): In this organization, there are no differences between mean-level trust in AI for employees with low job experience and mean-level trust in AI for employees with either medium or high job experience.\n\\(H_{a}\\): In this organization, the mean-level trust in AI for employees with low job experience is higher than the mean-level trust in AI for employees with medium job experience and high experience."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#what-is-a-statistical-significance-test",
    "href": "lectures/01-lecture-slides.html#what-is-a-statistical-significance-test",
    "title": "An Introduction to Statistics & Programming",
    "section": "What is a Statistical Significance Test?",
    "text": "What is a Statistical Significance Test?\nA statistical significance test, or just test, uses data to summarize the evidence about a hypothesis, usually the null, by comparing a point estimate of the parameter of interest (e.g. sample mean) to the value predicted by the hypothesis (e.g. 0 in the case of the null hypothesis)."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#a-significance-test-for-trust-in-ai",
    "href": "lectures/01-lecture-slides.html#a-significance-test-for-trust-in-ai",
    "title": "An Introduction to Statistics & Programming",
    "section": "A Significance Test for Trust in AI",
    "text": "A Significance Test for Trust in AI\nTo determine if trust in AI differs by job experience, we are going to use a z-test to test our two null hypotheses, which can be framed as hypotheses about the mean differences between trust in AI by job experience:\n\n\n\n\\(H_{o}\\):\n\\(\\mu_{\\text{AI Low Exp.}} - \\mu_{\\text{AI Med. Exp.}} = 0\\) \\(\\mu_{\\text{AI Low Exp.}} - \\mu_{\\text{AI Hifg Exp.}} = 0\\)\n\n\\(H_{a}\\):\n\\(\\mu_{\\text{AI Low Exp.}} - \\mu_{\\text{AI Med. Exp.}} &gt; 0\\) \\(\\mu_{\\text{AI Low Exp.}} - \\mu_{\\text{AI Hifg Exp.}} &gt; 0\\)"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#understanding-a-t-test",
    "href": "lectures/01-lecture-slides.html#understanding-a-t-test",
    "title": "An Introduction to Programming & Statistics",
    "section": "Understanding a T-Test",
    "text": "Understanding a T-Test"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#differnce-in-trust-in-ai-means-by-job-experience",
    "href": "lectures/01-lecture-slides.html#differnce-in-trust-in-ai-means-by-job-experience",
    "title": "An Introduction to Programming & Statistics",
    "section": "Differnce in Trust in AI Means by Job Experience",
    "text": "Differnce in Trust in AI Means by Job Experience\n\n\n\n\n\nComparison\nMean Trust\nMean Difference\n\n\n\n\nLow - Low\n4.33\n0.00\n\n\nLow - Medium\n4.28\n0.05\n\n\nLow - High\n2.94\n1.39"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#understanding-a-z-test",
    "href": "lectures/01-lecture-slides.html#understanding-a-z-test",
    "title": "An Introduction to Statistics & Programming",
    "section": "Understanding a Z-Test",
    "text": "Understanding a Z-Test\nA Z-test is a test that compares the mean of one variable to a specific population parameter specified by the null hypothesis (usually 0) or to the mean of a different variable. To conduct a Z-test you can follow these steps:\n\nEnsure the Z-test assumptions are met.\nSet the the probability threshold you need to surpass for an effect to be considered significant—your alpha level.\nCompute your test statistic and determine if it is significant at your specified alpha-level."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#z-test-assumptions",
    "href": "lectures/01-lecture-slides.html#z-test-assumptions",
    "title": "An Introduction to Statistics & Programming",
    "section": "Z-Test Assumptions",
    "text": "Z-Test Assumptions\n\nThe populations from which the samples were taken from must be normal.\nThe population SDs must be known or the sample sizes for each group must be large (~30 or more observations per group)."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#z-test-test-statistic",
    "href": "lectures/01-lecture-slides.html#z-test-test-statistic",
    "title": "An Introduction to Statistics & Programming",
    "section": "Z-Test Test Statistic",
    "text": "Z-Test Test Statistic\nWhen you are comparing two groups to one another, like we are, the test statistic, \\(Z\\), is defined as:\n\\[Z = \\frac{\\overline{Y_{1}}-\\overline{Y_{2}}}{\\sqrt{\\frac{\\sigma^{2}_{1}}{n_{1}} + \\frac{\\sigma^{2}_{2}}{n_{2}}}}\\]"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#difference-in-trust-in-ai-means-by-job-experience",
    "href": "lectures/01-lecture-slides.html#difference-in-trust-in-ai-means-by-job-experience",
    "title": "An Introduction to Statistics & Programming",
    "section": "Difference in Trust in AI Means by Job Experience",
    "text": "Difference in Trust in AI Means by Job Experience\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComp.\nLow Exp. Mean\nMean Trust\nMean Diff.\nLow Exp. Var.\nVar. Trust\nn\n\n\n\n\nLow - Medium\n4.33\n4.28\n0.05\n0.81\n0.61\n100\n\n\nLow - High\n4.33\n2.94\n1.39\n0.81\n1.04\n100\n\n\n\n\n\n\n\\[Z = \\frac{4.33 - 4.28}{\\sqrt{\\frac{.81}{100} + \\frac{.61}{100}}} = \\frac{0.05}{.12}=.42\\]"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#determining-the-p-value",
    "href": "lectures/01-lecture-slides.html#determining-the-p-value",
    "title": "An Introduction to Statistics & Programming",
    "section": "Determining the P-Value",
    "text": "Determining the P-Value\nA P-value is a tricky thing to think about, it is the probability of seeing a value greater than or equal to your test value given that the sampling distribution specified by the null hypothesis is true.\n\\[P(Z \\geq z \\space | \\space H_{0} )\\]\nIf your P-value is small (usually less than .05), then you can conclude that your test statistic is very unlikely to have come from the null distribution and thus you can reject the null hypothesis."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#visualizing-the-p-value",
    "href": "lectures/01-lecture-slides.html#visualizing-the-p-value",
    "title": "An Introduction to Statistics & Programming",
    "section": "Visualizing the P-Value",
    "text": "Visualizing the P-Value\nWe assume (somewhat safely thanks to the CLT), the our estimate has a normal sampling distribution and according to our null hypothesis of no effect the mean of the normal distribution should be 0 and the SD should be 1. We can use the CDF of the standard normal distribution to compute the P-value."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#using-r-to-conduct-a-z-test",
    "href": "lectures/01-lecture-slides.html#using-r-to-conduct-a-z-test",
    "title": "An Introduction to Programming & Statistics",
    "section": "Using R to Conduct a Z-test",
    "text": "Using R to Conduct a Z-test\n\nlow_group &lt;- data$trust_in_ai[data$job_exp == \"Low\"]\nmedium_group &lt;- data$trust_in_ai[data$job_exp == \"Medium\"]\n\nz_low_medium &lt;- BSDA::z.test(\n  x = low_group, y = medium_group, alternative = \"greater\",\n  sigma.x = sd(low_group), sigma.y = sd(medium_group) \n)\n\nz_low_medium\n\n\n    Two-sample z-Test\n\ndata:  low_group and medium_group\nz = 0.42005, p-value = 0.3372\nalternative hypothesis: true difference in means is greater than 0\n95 percent confidence interval:\n -0.1457907         NA\nsample estimates:\nmean of x mean of y \n     4.33      4.28"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#using-r-to-conduct-a-z-test-1",
    "href": "lectures/01-lecture-slides.html#using-r-to-conduct-a-z-test-1",
    "title": "An Introduction to Programming & Statistics",
    "section": "Using R to Conduct a Z-test",
    "text": "Using R to Conduct a Z-test\n\nlow_group &lt;- data$trust_in_ai[data$job_exp == \"Low\"]\nhigh_group &lt;- data$trust_in_ai[data$job_exp == \"High\"]\n\nz_low_high &lt;- BSDA::z.test(\n  x = low_group, y = high_group, alternative = \"greater\",\n  sigma.x = sd(low_group), sigma.y = sd(high_group) \n)\n\nz_low_high\n\n\n    Two-sample z-Test\n\ndata:  low_group and high_group\nz = 10.203, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means is greater than 0\n95 percent confidence interval:\n 1.16591      NA\nsample estimates:\nmean of x mean of y \n     4.33      2.94"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#visualizing-a-high-p-value",
    "href": "lectures/01-lecture-slides.html#visualizing-a-high-p-value",
    "title": "An Introduction to Programming & Statistics",
    "section": "Visualizing a High P-Value",
    "text": "Visualizing a High P-Value"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#using-r-to-conduct-a-z-test-medium-job-experience",
    "href": "lectures/01-lecture-slides.html#using-r-to-conduct-a-z-test-medium-job-experience",
    "title": "An Introduction to Statistics & Programming",
    "section": "Using R to Conduct a Z-test: Medium Job Experience",
    "text": "Using R to Conduct a Z-test: Medium Job Experience\n\nlow_group &lt;- data$trust_in_ai[data$job_exp == \"Low\"]\nmedium_group &lt;- data$trust_in_ai[data$job_exp == \"Medium\"]\n\nz_low_medium &lt;- BSDA::z.test(\n  x = low_group, y = medium_group, alternative = \"greater\",\n  sigma.x = sd(low_group), sigma.y = sd(medium_group) \n)\n\nz_low_medium\n\n\n    Two-sample z-Test\n\ndata:  low_group and medium_group\nz = 0.42005, p-value = 0.3372\nalternative hypothesis: true difference in means is greater than 0\n95 percent confidence interval:\n -0.1457907         NA\nsample estimates:\nmean of x mean of y \n     4.33      4.28"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#using-r-to-conduct-a-z-test-high-job-experience",
    "href": "lectures/01-lecture-slides.html#using-r-to-conduct-a-z-test-high-job-experience",
    "title": "An Introduction to Statistics & Programming",
    "section": "Using R to Conduct a Z-test: High Job Experience",
    "text": "Using R to Conduct a Z-test: High Job Experience\n\nlow_group &lt;- data$trust_in_ai[data$job_exp == \"Low\"]\nhigh_group &lt;- data$trust_in_ai[data$job_exp == \"High\"]\n\nz_low_high &lt;- BSDA::z.test(\n  x = low_group, y = high_group, alternative = \"greater\",\n  sigma.x = sd(low_group), sigma.y = sd(high_group) \n)\n\nz_low_high\n\n\n    Two-sample z-Test\n\ndata:  low_group and high_group\nz = 10.203, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means is greater than 0\n95 percent confidence interval:\n 1.16591      NA\nsample estimates:\nmean of x mean of y \n     4.33      2.94"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#visualizing-a-very-small-p-value",
    "href": "lectures/01-lecture-slides.html#visualizing-a-very-small-p-value",
    "title": "An Introduction to Statistics & Programming",
    "section": "Visualizing a Very Small P-Value",
    "text": "Visualizing a Very Small P-Value"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#a-glimpse-of-your-data",
    "href": "lectures/01-lecture-slides.html#a-glimpse-of-your-data",
    "title": "An Introduction to Statistics & Programming",
    "section": "A Glimpse of your Data",
    "text": "A Glimpse of your Data\n\nset.seed(435)\ndata |&gt;\n  dplyr::slice_sample(n = 10) |&gt;\n  dplyr::select(\n    job_exp,\n    trust_in_ai\n  ) |&gt;\n  dplyr::arrange(\n    job_exp\n  )\n\n# A tibble: 10 × 2\n   job_exp trust_in_ai\n   &lt;fct&gt;         &lt;dbl&gt;\n 1 Low               5\n 2 Low               3\n 3 Low               5\n 4 Low               5\n 5 High              1\n 6 High              3\n 7 High              3\n 8 High              3\n 9 Medium            5\n10 Medium            5"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#the-direction-of-a-statistical-hypothesis",
    "href": "lectures/01-lecture-slides.html#the-direction-of-a-statistical-hypothesis",
    "title": "An Introduction to Statistics & Programming",
    "section": "The Direction of a Statistical Hypothesis",
    "text": "The Direction of a Statistical Hypothesis\nHypotheses can be directional or non-directional.\nA directional hypothesis is a hypothesis that makes an explicit statement about whether one group will have a larger (or smaller) mean than another group.\nA non-directional hypothesis is a hypothesis that states that the means of the two groups differ, but does not specify which group has a larger (or smaller) mean."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#determing-your-alpha-level",
    "href": "lectures/01-lecture-slides.html#determing-your-alpha-level",
    "title": "An Introduction to Statistics & Programming",
    "section": "Determing your Alpha-Level",
    "text": "Determing your Alpha-Level\nThe \\(\\alpha\\)-level, also called the significance level, is a number \\(\\alpha\\) between 0 and 1 such that we reject \\(H_{0}\\) if the P-value of the test statistic is less than or equal to \\(\\alpha\\).\nGenerally, we set \\(\\alpha\\) to .05 or .01. To reject the \\(H_{0}\\), the P-value needs to be less than or equal to .05 or .01, respectively."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#decision-errors",
    "href": "lectures/01-lecture-slides.html#decision-errors",
    "title": "An Introduction to Statistics & Programming",
    "section": "Decision Errors",
    "text": "Decision Errors\nThe conclusion you come to thanks to a statistical test is not guaranteed to be the right one. There is always a risk of making a decision error:\n\n\n\n\nReject \\(H_{0}\\)\nDo Not Reject \\(H_{0}\\)\n\n\n\n\n\\(H_{0}\\) is true\nType 1 Error\nCorrect Decision\n\n\n\\(H_{0}\\) is false\nCorrect Decision\nType 2 Error\n\n\n\nTo protect against a Type 1 Error, we can make our \\(\\alpha\\)-level very small, which will make it very difficult to reject \\(H_{0}\\), but this increases Type 2 Error. One way to guard against Type 2 Error is by using an appropriate statistical test on a large sample of data."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#example-base-function",
    "href": "lectures/01-lecture-slides.html#example-base-function",
    "title": "An Introduction to Statistics & Programming",
    "section": "Example Base Function",
    "text": "Example Base Function\n\nx &lt;- c(1, 4, 6)\nsum(x) \n\n[1] 11\n\nmean(x)\n\n[1] 3.666667\n\nmin(x)\n\n[1] 1"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#integrated-development-environment-rstudio",
    "href": "lectures/01-lecture-slides.html#integrated-development-environment-rstudio",
    "title": "An Introduction to Statistics & Programming",
    "section": "Integrated Development Environment & RStudio",
    "text": "Integrated Development Environment & RStudio\nAn integrated development environment (IDE) is an application that makes programming a little easier and organized. It includes all of the tools one needs to program effectively and efficiently.\nRStudio is an IDE initially developed for R, but it can be used for other programming languages too.\nIf you have not already, please go ahead and download RStudio from here."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#welcome",
    "href": "lectures/01-lecture-slides.html#welcome",
    "title": "An Introduction to Statistics & Programming",
    "section": "Welcome!",
    "text": "Welcome!\nFirst off, breathe! We will all make it through this together!\nMy quick teaching philosophy:\n\nI love talking not lecturing—ask me questions!\nWe are all here because we enjoy learning, which is the goal of my course: learning. DO NOT WORRY ABOUT YOUR GRADES—take that stress off of yourself."
  },
  {
    "objectID": "lectures/01-lecture-slides.html#goals-for-today",
    "href": "lectures/01-lecture-slides.html#goals-for-today",
    "title": "An Introduction to Statistics & Programming",
    "section": "Goals for Today",
    "text": "Goals for Today\n\nRefresh yourself on statistics!\nLearn about statistical estimation and tests\nData importing and transformation with R"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#schedule-for-today",
    "href": "lectures/01-lecture-slides.html#schedule-for-today",
    "title": "An Introduction to Statistics & Programming",
    "section": "Schedule for Today",
    "text": "Schedule for Today\nIn addition to a 15 minute break at 2:30 PM, we will take two 5-ish minute breaks at:\n\n5 min break @ 1:45 PM\n5 min break @ 4:00 PM\n\nFeel free to ask me even more questions during this time!"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#types-of-decision-errors",
    "href": "lectures/01-lecture-slides.html#types-of-decision-errors",
    "title": "An Introduction to Statistics & Programming",
    "section": "Types of Decision Errors",
    "text": "Types of Decision Errors\nThe conclusion you come to thanks to a statistical test is not guaranteed to be the right one. There is always a risk of making a decision error:\n\n\n\n\nReject \\(H_{0}\\)\nDo Not Reject \\(H_{0}\\)\n\n\n\n\n\\(H_{0}\\) is true\nType 1 Error\nCorrect Decision\n\n\n\\(H_{0}\\) is false\nCorrect Decision\nType 2 Error"
  },
  {
    "objectID": "lectures/01-lecture-slides.html#protecting-against-errors",
    "href": "lectures/01-lecture-slides.html#protecting-against-errors",
    "title": "An Introduction to Statistics & Programming",
    "section": "Protecting Against Errors",
    "text": "Protecting Against Errors\nTo protect against a Type 1 Error, we can make our \\(\\alpha\\)-level very small, which will make it very difficult to reject \\(H_{0}\\), but this increases Type 2 Error.\nOne way to guard against Type 2 Error is by using an appropriate statistical test on a large sample of data."
  },
  {
    "objectID": "notes/01-lecture-notes.html#what-is-statistics",
    "href": "notes/01-lecture-notes.html#what-is-statistics",
    "title": "Lecture 1 Notes",
    "section": "What is Statistics",
    "text": "What is Statistics"
  },
  {
    "objectID": "notes/01-lecture-notes.html#probability-theory-mathematics-of-uncertainty",
    "href": "notes/01-lecture-notes.html#probability-theory-mathematics-of-uncertainty",
    "title": "Lecture 1 Notes",
    "section": "Probability Theory: Mathematics of Uncertainty",
    "text": "Probability Theory: Mathematics of Uncertainty\n\nWhat is Probability?\n\n\nFundamentals of Probability Theory\n\n\nJoint Probability: The Probability of Two or More Events\n\n\nConditional Probability: The Probability of One Event Given Another"
  },
  {
    "objectID": "notes/01-lecture-notes.html#random-variables-connecting-probability-statistics-and-data",
    "href": "notes/01-lecture-notes.html#random-variables-connecting-probability-statistics-and-data",
    "title": "Lecture 1 Notes",
    "section": "Random Variables: Connecting Probability, Statistics, and Data",
    "text": "Random Variables: Connecting Probability, Statistics, and Data\n\nTypes of Random Variables: Discrete or Continuous\n\n\nProbability Distributions\n\n\nProbability Mass Function: Describing Discrete Random Variables\n\n\nProbability Density Function: Describing Continuous Random Variables\n\n\nCumulative Distribution Function\n\n\nJoint & Conditional Distributions: Modeling Two (or More) Random Variables"
  },
  {
    "objectID": "notes/01-lecture-notes.html#descriptive-statistics-describing-your-data",
    "href": "notes/01-lecture-notes.html#descriptive-statistics-describing-your-data",
    "title": "Lecture 1 Notes",
    "section": "Descriptive Statistics: Describing Your Data",
    "text": "Descriptive Statistics: Describing Your Data\n\nPopulation vs Sample\n\n\nMeasures of Central Tendency\n\nMean (or Expected Value)\n\n\nMedian\n\n\nMode\n\n\n\nMeasures of Spread\n\nRange\n\n\nVariance\n\n\nStandard Deviation"
  },
  {
    "objectID": "notes/01-lecture-notes.html#inferential-statistics-generalizing-from-your-data",
    "href": "notes/01-lecture-notes.html#inferential-statistics-generalizing-from-your-data",
    "title": "Lecture 1 Notes",
    "section": "Inferential Statistics: Generalizing from Your Data",
    "text": "Inferential Statistics: Generalizing from Your Data"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#schedule-for-today",
    "href": "lectures/02-lecture-slides.html#schedule-for-today",
    "title": "An Introduction to Simple Regression",
    "section": "Schedule for Today",
    "text": "Schedule for Today\n\nTalk about stats for ~75 mins (5 PM - 6:15 PM ET)\nBreak for 5 minutes\nR Introduction / Hands-on coding for 40 mins (6:20 PM - 7:00 PM ET)"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#overview",
    "href": "lectures/02-lecture-slides.html#overview",
    "title": "An Introduction to Simple Regression",
    "section": "Overview",
    "text": "Overview\n\nA quick review of last week\nOverview of conditional distributions & statistics\nIntroduction to Simple Regression\nR Intro"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#goals",
    "href": "lectures/02-lecture-slides.html#goals",
    "title": "An Introduction to Simple Regression",
    "section": "Goals",
    "text": "Goals\n\nDevelop an understanding of simple regression\nWrite your first R script"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#theory-construction-in-the-managerial-sciences",
    "href": "lectures/02-lecture-slides.html#theory-construction-in-the-managerial-sciences",
    "title": "Correlation & Covariation",
    "section": "Theory Construction in the Managerial Sciences",
    "text": "Theory Construction in the Managerial Sciences"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#empirically-testing-theories",
    "href": "lectures/02-lecture-slides.html#empirically-testing-theories",
    "title": "An Introduction to Simple Regression",
    "section": "Empirically Testing Theories",
    "text": "Empirically Testing Theories\nWhat is theory without evidence? We need methods that allow us to test the relationships among variables hypothesized by our theory and we need to adjust for other variables (control)"
  },
  {
    "objectID": "lectures/02-lecture-page.html",
    "href": "lectures/02-lecture-page.html",
    "title": "Quantitative Analysis 1",
    "section": "",
    "text": "Next Week’s Materials »"
  },
  {
    "objectID": "lectures/02-lecture-page.html#lecture-stats-bootcamp",
    "href": "lectures/02-lecture-page.html#lecture-stats-bootcamp",
    "title": "Quantitative Analysis 1",
    "section": "Lecture: Stats Bootcamp",
    "text": "Lecture: Stats Bootcamp\n\n\nTo download a pdf version of these slides, click here.\nTo download the R script that follows the R portion of the lecture, click here."
  },
  {
    "objectID": "lectures/02-lecture-slides.html#why-do-re-care-about-statistical-modeling",
    "href": "lectures/02-lecture-slides.html#why-do-re-care-about-statistical-modeling",
    "title": "An Introduction to Simple Regression",
    "section": "Why Do Re Care About Statistical Modeling?",
    "text": "Why Do Re Care About Statistical Modeling?\nIn research and practice, you will likely come up with questions or hypotheses of the form:\n\n\n“Are changes in Variable X associated with changes in Variable Y?”\n\n\nWe can apply the models we will talk about in class to our data in order to empirically test our hypotheses (or answer our questions). These models allow us to make data-driven evaluations of our theories."
  },
  {
    "objectID": "lectures/02-lecture-slides.html#the-role-of-theory-in-research-practice",
    "href": "lectures/02-lecture-slides.html#the-role-of-theory-in-research-practice",
    "title": "An Introduction to Simple Regression",
    "section": "The Role of Theory in Research & Practice",
    "text": "The Role of Theory in Research & Practice\n\nA theory is a set of interrelated constructs (concepts), definitions, and propositions that present a systematic view of phenomena by specifying relations among variables with &gt; the purpose of explaining and predicting the phenomenon.\n\n— Kerlinger & Lee (2000)"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#from-theory-to-model-to-data",
    "href": "lectures/02-lecture-slides.html#from-theory-to-model-to-data",
    "title": "An Introduction to Simple Regression",
    "section": "From Theory to Model to Data",
    "text": "From Theory to Model to Data\nPath model diagrams"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#enter-general-linear-models",
    "href": "lectures/02-lecture-slides.html#enter-general-linear-models",
    "title": "Correlation, Covariation, & Simple Regression",
    "section": "Enter General Linear Models",
    "text": "Enter General Linear Models\nSpecfiically regression"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#example-theory-hypothesis",
    "href": "lectures/02-lecture-slides.html#example-theory-hypothesis",
    "title": "An Introduction to Simple Regression",
    "section": "Example Theory & Hypothesis",
    "text": "Example Theory & Hypothesis"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#visualzing-the-relationship-between-two-variables",
    "href": "lectures/02-lecture-slides.html#visualzing-the-relationship-between-two-variables",
    "title": "An Introduction to Simple Regression",
    "section": "Visualzing the Relationship Between Two Variables",
    "text": "Visualzing the Relationship Between Two Variables\nScatter Plot"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#improving-our-scatter-plot",
    "href": "lectures/02-lecture-slides.html#improving-our-scatter-plot",
    "title": "An Introduction to Simple Regression",
    "section": "Improving our Scatter Plot",
    "text": "Improving our Scatter Plot\nWe can use an R function called geom_jitter to add a bit of random noise to each of our data points, which improves the usefulness of scatter plots when working with discrete data like survey responses."
  },
  {
    "objectID": "lectures/02-lecture-slides.html#conditional-distributions",
    "href": "lectures/02-lecture-slides.html#conditional-distributions",
    "title": "An Introduction to Simple Regression",
    "section": "Conditional Distributions",
    "text": "Conditional Distributions\nConditional distributions are distributions of one variable, Y, conditional (fixed) on a value of one or more additional variables."
  },
  {
    "objectID": "lectures/02-lecture-slides.html#conditional-mean-expectation",
    "href": "lectures/02-lecture-slides.html#conditional-mean-expectation",
    "title": "An Introduction to Simple Regression",
    "section": "Conditional Mean (Expectation)",
    "text": "Conditional Mean (Expectation)\n\\[E[Y|X]\\]"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#conditional-mean-plot",
    "href": "lectures/02-lecture-slides.html#conditional-mean-plot",
    "title": "An Introduction to Simple Regression",
    "section": "Conditional Mean Plot",
    "text": "Conditional Mean Plot"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#using-covariation-correlation-to-measure-linear-dependence",
    "href": "lectures/02-lecture-slides.html#using-covariation-correlation-to-measure-linear-dependence",
    "title": "An Introduction to Simple Regression",
    "section": "Using Covariation & Correlation to Measure Linear Dependence",
    "text": "Using Covariation & Correlation to Measure Linear Dependence\nTwo of the most basic measures of linear dependence we have between two variables are the covariance and correlation between two variables.\n\\[Cov(X, Y)=\\frac{\\Sigma{(X - \\overline{X})(Y-\\overline{Y})}}{N}\\] \\[r_{XY}=\\frac{Cov(X,Y)}{S_{X}S_{Y}}\\]"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#using-the-slope-of-a-line-to-measure-dependence",
    "href": "lectures/02-lecture-slides.html#using-the-slope-of-a-line-to-measure-dependence",
    "title": "Correlation, Covariation, & Simple Regression",
    "section": "Using the Slope of a Line to Measure Dependence",
    "text": "Using the Slope of a Line to Measure Dependence"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#what-line-to-choose",
    "href": "lectures/02-lecture-slides.html#what-line-to-choose",
    "title": "Correlation, Covariation, & Simple Regression",
    "section": "What Line to Choose?",
    "text": "What Line to Choose?"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#errors-of-estimate-or-prediction-errors",
    "href": "lectures/02-lecture-slides.html#errors-of-estimate-or-prediction-errors",
    "title": "An Introduction to Simple Regression",
    "section": "Errors of Estimate (or Prediction Errors)",
    "text": "Errors of Estimate (or Prediction Errors)\nWe say that the “best” line is the line that minimizes the squared distance between our outcome variable, \\(Y\\), and what our line predicts that outcome variable to be, \\(\\hat{Y}\\), on average:\n\\[SS_{error}=\\Sigma(Y_{i}-\\hat{Y}_{i})^2=\\Sigma(e^{2}_{i})\\]\n\\[\\hat{Y}_{i}=\\beta_{0}+\\beta_{1}X_{i}\\]\nWe call this the sum of squared error."
  },
  {
    "objectID": "lectures/02-lecture-slides.html#linear-regression-the-line-that-best-fits-your-data",
    "href": "lectures/02-lecture-slides.html#linear-regression-the-line-that-best-fits-your-data",
    "title": "An Introduction to Simple Regression",
    "section": "Linear Regression: The Line that “Best” Fits your Data",
    "text": "Linear Regression: The Line that “Best” Fits your Data\nLinear regression is the statistical method that estimates the “best” fitting line by minimizing the sum of squared errors (\\(SS_{error}\\)).\nThere is no other line that will produce a smaller value of \\(SS_{error}\\)!"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#the-linear-regression-model",
    "href": "lectures/02-lecture-slides.html#the-linear-regression-model",
    "title": "An Introduction to Simple Regression",
    "section": "The Linear Regression Model",
    "text": "The Linear Regression Model"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#estimating-your-model-using-lm",
    "href": "lectures/02-lecture-slides.html#estimating-your-model-using-lm",
    "title": "An Introduction to Simple Regression",
    "section": "Estimating your Model Using lm",
    "text": "Estimating your Model Using lm\n\nmod_ai &lt;- lm(freq_use_ai ~ pos_attitude_ai, data = data_ai)\nmod_ai |&gt; summary()\n\n\nCall:\nlm(formula = freq_use_ai ~ pos_attitude_ai, data = data_ai)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.9683 -1.0050  0.0072  1.0317  3.9950 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      1.51112    0.05575   27.11   &lt;2e-16 ***\npos_attitude_ai  0.49388    0.01285   38.45   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.483 on 4998 degrees of freedom\nMultiple R-squared:  0.2282,    Adjusted R-squared:  0.2281 \nF-statistic:  1478 on 1 and 4998 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#interpreting-the-coefficients",
    "href": "lectures/02-lecture-slides.html#interpreting-the-coefficients",
    "title": "An Introduction to Simple Regression",
    "section": "Interpreting the Coefficients",
    "text": "Interpreting the Coefficients"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#residuals",
    "href": "lectures/02-lecture-slides.html#residuals",
    "title": "An Introduction to Simple Regression",
    "section": "Residuals",
    "text": "Residuals"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#residual-analysis",
    "href": "lectures/02-lecture-slides.html#residual-analysis",
    "title": "An Introduction to Simple Regression",
    "section": "Residual Analysis",
    "text": "Residual Analysis"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#variance-explained",
    "href": "lectures/02-lecture-slides.html#variance-explained",
    "title": "An Introduction to Simple Regression",
    "section": "Variance Explained",
    "text": "Variance Explained"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#r2",
    "href": "lectures/02-lecture-slides.html#r2",
    "title": "An Introduction to Simple Regression",
    "section": "R2",
    "text": "R2"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#m",
    "href": "lectures/02-lecture-slides.html#m",
    "title": "An Introduction to Simple Regression",
    "section": "M",
    "text": "M"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#using-the-slope-of-a-line-to-measure-linear-dependence",
    "href": "lectures/02-lecture-slides.html#using-the-slope-of-a-line-to-measure-linear-dependence",
    "title": "An Introduction to Simple Regression",
    "section": "Using the Slope of a Line to Measure Linear Dependence",
    "text": "Using the Slope of a Line to Measure Linear Dependence\n\\[Y = \\beta_{0}+\\beta_{1}X_{1}+\\epsilon\\]\n\\(\\beta_{0}\\): The intercept of the line\n\\(\\beta_{1}\\): The slope of the line\n\\(\\epsilon\\): Error"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#which-line-to-choose",
    "href": "lectures/02-lecture-slides.html#which-line-to-choose",
    "title": "An Introduction to Simple Regression",
    "section": "Which Line to Choose?",
    "text": "Which Line to Choose?\nWe could, however, choose many different lines. How do we determine what the “best” line is? First, we have to define exactly what we mean by “best.”"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#quick-review-of-terminology",
    "href": "lectures/02-lecture-slides.html#quick-review-of-terminology",
    "title": "An Introduction to Simple Regression",
    "section": "Quick Review of Terminology",
    "text": "Quick Review of Terminology\n\nMean of X: \\(\\overline{X}\\) or \\(E[X]\\)\nVariance of X: \\(\\sigma^{2}_{x}\\) or \\(Var(X)\\)\nStandard Deviation of X: \\(\\sigma_{x}\\) or \\(S_{X}\\)\nCovariance of X & Y: \\(Cov(XY)\\)\nCorrelation of X & Y: \\(r_{xy}\\)\nPopulation Parameter: \\(\\beta\\) or any other Greek letter\nPopulation Estimate: \\(\\hat{\\beta}\\)"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#conditional-statistics",
    "href": "lectures/02-lecture-slides.html#conditional-statistics",
    "title": "An Introduction to Simple Regression",
    "section": "Conditional Statistics",
    "text": "Conditional Statistics\nConditional statistics are statistics computed from conditional distributions. The characteristics of a distribution (mean, variance, etc.) can change based on the values of another variable.\nLinear regression models are largely concerned with two conditional statistics:\n\nConditional Mean (or Expectation): \\(E[Y|X]\\)\nConditional Variance: \\(\\sigma^2_{Y|X}\\)"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#conditional-statistics-visualized",
    "href": "lectures/02-lecture-slides.html#conditional-statistics-visualized",
    "title": "An Introduction to Simple Regression",
    "section": "Conditional Statistics Visualized",
    "text": "Conditional Statistics Visualized"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#conditional-mean-dependence",
    "href": "lectures/02-lecture-slides.html#conditional-mean-dependence",
    "title": "An Introduction to Simple Regression",
    "section": "Conditional Mean Dependence",
    "text": "Conditional Mean Dependence"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#modeling-mean-dependence",
    "href": "lectures/02-lecture-slides.html#modeling-mean-dependence",
    "title": "An Introduction to Simple Regression",
    "section": "Modeling Mean Dependence",
    "text": "Modeling Mean Dependence\nIn statistics, we are mostly interested in modeling mean dependence—this is why statistics has been referred to as “the science of averages.”\nEvery statistical model we will use in this class is ultimately trying to find a function that best describes how the mean of some variable Y changes across the values of some set of variables, X.\n\\[E[Y|X] = f(X)\\]"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#visualizing-mean-dependence",
    "href": "lectures/02-lecture-slides.html#visualizing-mean-dependence",
    "title": "An Introduction to Simple Regression",
    "section": "Visualizing Mean Dependence",
    "text": "Visualizing Mean Dependence"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#measuring-mean-dependence-with-covariance-correlation",
    "href": "lectures/02-lecture-slides.html#measuring-mean-dependence-with-covariance-correlation",
    "title": "An Introduction to Simple Regression",
    "section": "Measuring Mean Dependence with Covariance & Correlation",
    "text": "Measuring Mean Dependence with Covariance & Correlation\nTwo of the most basic measures of the linear mean dependence between two variables are the covariance and correlation:\n\\[Cov(X, Y)=\\frac{\\Sigma{(X - \\overline{X})(Y-\\overline{Y})}}{N}\\] \\[r_{XY}=\\frac{Cov(X,Y)}{S_{X}S_{Y}}\\]"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#measuring-mean-dependence-with-a-linear-function-a-line",
    "href": "lectures/02-lecture-slides.html#measuring-mean-dependence-with-a-linear-function-a-line",
    "title": "An Introduction to Simple Regression",
    "section": "Measuring Mean Dependence with a Linear Function (a Line)",
    "text": "Measuring Mean Dependence with a Linear Function (a Line)\nA different, but related way, to measure mean dependence is to choose a linear function (a line) to describe how the mean of \\(Y\\) changes across values of \\(X\\).\n\\[E[Y|X] = \\beta_{0}+\\beta_{1}X_{1}\\]\n\\(\\beta_{0}\\): The intercept of the line\n\\(\\beta_{1}\\): The slope of the line, which is a measure of linear mean dependence"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#the-simple-regression-model",
    "href": "lectures/02-lecture-slides.html#the-simple-regression-model",
    "title": "An Introduction to Simple Regression",
    "section": "The Simple Regression Model",
    "text": "The Simple Regression Model\nThe simple regression model is just a linear regression model with one independent variable.\n\\[Y=\\beta_0+\\beta_1X_1+\\epsilon\\]\n\n\\(\\beta_0\\): The expected value (mean) of Y when X = 0.\n\\(\\beta_1\\): The average change in Y for a one-unit increase in X.\n\\(\\epsilon\\): Variation in Y that is not explained by our model—unexplained variance."
  },
  {
    "objectID": "lectures/02-lecture-slides.html#estimating-the-regression-coefficients",
    "href": "lectures/02-lecture-slides.html#estimating-the-regression-coefficients",
    "title": "An Introduction to Simple Regression",
    "section": "Estimating the Regression Coefficients",
    "text": "Estimating the Regression Coefficients\nWe will never know the population values of the regression coefficients (\\(\\beta_0\\) & \\(\\beta_1\\)), but we can use our data to estimate them:\n\\[\\hat{\\beta}_1=\\frac{Cov(X,Y)}{Var(X)}\\]\n\\[\\hat{\\beta}_0=\\overline{Y}-\\hat{\\beta}_1\\overline{X}\\]"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#what-is-a-statistical-model",
    "href": "lectures/02-lecture-slides.html#what-is-a-statistical-model",
    "title": "An Introduction to Simple Regression",
    "section": "What is a Statistical Model?",
    "text": "What is a Statistical Model?\nA statistical model is an approximation of some random process that uses probability theory and other mathematical tools to describe the process.\nIn your own research, you will likely rely on theory to develop your own statistical models."
  },
  {
    "objectID": "lectures/02-lecture-slides.html#example-hypothesis",
    "href": "lectures/02-lecture-slides.html#example-hypothesis",
    "title": "An Introduction to Simple Regression",
    "section": "Example Hypothesis",
    "text": "Example Hypothesis\nYour organization has just implemented a new generative AI tool (fancy chat bot) to help improve the efficiency of the organizations sales force. Sales employees, however, have adopted the technology at different rates.\nYou have been asked to understand if an employee’s general positive attitude toward AI is related to how frequently they are using the new AI tool and you form the following hypothesis:\n\nAn employee’s general positive attitude toward AI will be positively related to the frequency with which they use the new AI tool."
  },
  {
    "objectID": "lectures/02-lecture-slides.html#view-our-dataset",
    "href": "lectures/02-lecture-slides.html#view-our-dataset",
    "title": "An Introduction to Simple Regression",
    "section": "View our Dataset",
    "text": "View our Dataset\n\ndata_ai |&gt;\n  dplyr::select(\n    pos_attitude_ai:freq_use_ai_label\n  )\n\n# A tibble: 5,000 × 4\n   pos_attitude_ai freq_use_ai pos_attitude_ai_label freq_use_ai_label\n             &lt;int&gt;       &lt;dbl&gt; &lt;chr&gt;                 &lt;chr&gt;            \n 1               4           4 Neutral               Fairly Often     \n 2               6           6 Strongly Agree        All the time     \n 3               7           3 Completely Agree      Occasionally     \n 4               7           6 Completely Agree      All the time     \n 5               4           3 Neutral               Occasionally     \n 6               5           4 Agree                 Fairly Often     \n 7               5           6 Agree                 All the time     \n 8               3           4 Disagree              Fairly Often     \n 9               3           5 Disagree              Very Often       \n10               4           4 Neutral               Fairly Often     \n# ℹ 4,990 more rows"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#visualizing-the-relationship-between-two-variables",
    "href": "lectures/02-lecture-slides.html#visualizing-the-relationship-between-two-variables",
    "title": "An Introduction to Simple Regression",
    "section": "Visualizing the Relationship Between Two Variables",
    "text": "Visualizing the Relationship Between Two Variables\nWe can use a scatter plot to visually explore the relationship between our two variables. Why does it look so odd?"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#partitioning-variance",
    "href": "lectures/02-lecture-slides.html#partitioning-variance",
    "title": "An Introduction to Simple Regression",
    "section": "Partitioning Variance",
    "text": "Partitioning Variance\n\\[(Y - \\overline{Y})^2=(Y - \\hat{Y})^2 + (\\hat{Y}-\\overline{Y})^2\\]"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#r-squared-variance-explained",
    "href": "lectures/02-lecture-slides.html#r-squared-variance-explained",
    "title": "An Introduction to Simple Regression",
    "section": "R-Squared: Variance Explained",
    "text": "R-Squared: Variance Explained"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#exploring-our-data",
    "href": "lectures/02-lecture-slides.html#exploring-our-data",
    "title": "An Introduction to Simple Regression",
    "section": "Exploring our Data",
    "text": "Exploring our Data"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#interpreting-the-coefficients-as-group-comparisons",
    "href": "lectures/02-lecture-slides.html#interpreting-the-coefficients-as-group-comparisons",
    "title": "An Introduction to Simple Regression",
    "section": "Interpreting the Coefficients as Group Comparisons",
    "text": "Interpreting the Coefficients as Group Comparisons\nThe most appropriate way to interpret the slope coefficient (\\(\\beta_1\\)) is as a comparison."
  },
  {
    "objectID": "lectures/02-lecture-slides.html#interpreting-the-coefficients-of-our-model",
    "href": "lectures/02-lecture-slides.html#interpreting-the-coefficients-of-our-model",
    "title": "An Introduction to Simple Regression",
    "section": "Interpreting the Coefficients of our Model",
    "text": "Interpreting the Coefficients of our Model\nComparing employees who differ in their attidues towards AI by one point, the average difference in the frequency with which those employees use the AI tool is 0.49 points."
  },
  {
    "objectID": "lectures/02-lecture-slides.html#interpreting-the-coefficients-as-comparisons",
    "href": "lectures/02-lecture-slides.html#interpreting-the-coefficients-as-comparisons",
    "title": "An Introduction to Simple Regression",
    "section": "Interpreting the Coefficients as Comparisons",
    "text": "Interpreting the Coefficients as Comparisons\nThe most appropriate way to interpret the slope coefficient (\\(\\beta_1\\)) is as a comparison. In our example:\nComparing employees who differ in their attitudes towards AI by one point, the average difference in the frequency with which those employees use the AI tool is 0.49 points."
  },
  {
    "objectID": "lectures/02-lecture-slides.html#visualizing-the-comparison",
    "href": "lectures/02-lecture-slides.html#visualizing-the-comparison",
    "title": "An Introduction to Simple Regression",
    "section": "Visualizing the Comparison",
    "text": "Visualizing the Comparison"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#regression-coefficients-the-strength-of-the-relationship",
    "href": "lectures/02-lecture-slides.html#regression-coefficients-the-strength-of-the-relationship",
    "title": "An Introduction to Simple Regression",
    "section": "Regression Coefficients & the Strength of the Relationship",
    "text": "Regression Coefficients & the Strength of the Relationship\nIt is very difficult to determine the strength of the relationship between a dependent variable, Y, and an independent variable, X, using just the regression slope, \\(\\beta_1\\).\nIt is difficult because the magnitude of \\(\\beta_1\\) depends on the scale of both the dependent and independent variable, so you can artificially change the magnitude of the slope by changing the scale of your variables."
  },
  {
    "objectID": "lectures/02-lecture-slides.html#the-connection-between-a-correlation-and-a-regression-coefficient",
    "href": "lectures/02-lecture-slides.html#the-connection-between-a-correlation-and-a-regression-coefficient",
    "title": "An Introduction to Simple Regression",
    "section": "The Connection between a Correlation and a Regression Coefficient",
    "text": "The Connection between a Correlation and a Regression Coefficient\nYou can use the correlation between the independent and dependent variable to estimate how strongly related the two variables are.\nIn simple linear regression there is a straightforward relationship between a correlation and a regression coefficient:\n\\[r_{XY}=\\beta_1\\times\\frac{SD_X}{SD_Y}\\]"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#scaling-your-dependent-and-independent-variable",
    "href": "lectures/02-lecture-slides.html#scaling-your-dependent-and-independent-variable",
    "title": "An Introduction to Simple Regression",
    "section": "Scaling your Dependent and Independent Variable",
    "text": "Scaling your Dependent and Independent Variable\nAnother way to determine the strength of the relationship between a dependent and an indpendent variable is to scale (standardize) each variable, so that it has a mean of 0 and a SD of 1:\n\\[Z_X=\\frac{X-\\overline{X}}{SD_X}\\]\nYou can then estimate a new regression model using the scaled variables and use the magnitude of \\(\\beta_1\\) to judge the strength of the relationship between the independent and dependent variable."
  },
  {
    "objectID": "lectures/02-lecture-slides.html#example-of-scaling-variables",
    "href": "lectures/02-lecture-slides.html#example-of-scaling-variables",
    "title": "An Introduction to Simple Regression",
    "section": "Example of Scaling Variables",
    "text": "Example of Scaling Variables\n\ndata_ai &lt;-\n  data_ai |&gt;\n  dplyr::mutate(\n    pos_attitude_ai_scale = (pos_attitude_ai - mean(pos_attitude_ai)) / sd(pos_attitude_ai),\n    freq_use_ai_scale = scale(freq_use_ai)[,1]\n  )\n\nmod_scale &lt;- lm(freq_use_ai_scale ~ pos_attitude_ai_scale, data_ai)\nmod_scale$coefficients |&gt; round(2)\n\n          (Intercept) pos_attitude_ai_scale \n                 0.00                  0.48 \n\ncor(data_ai$pos_attitude_ai, data_ai$freq_use_ai) |&gt; round(2)\n\n[1] 0.48"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#measuring-strength-with-correlation-coefficient",
    "href": "lectures/02-lecture-slides.html#measuring-strength-with-correlation-coefficient",
    "title": "An Introduction to Simple Regression",
    "section": "Measuring Strength with Correlation Coefficient",
    "text": "Measuring Strength with Correlation Coefficient\nYou can use the correlation between the independent and dependent variable to estimate how strongly related the two variables are.\nIn simple linear regression there is a straightforward relationship between a correlation and a regression coefficient:\n\\[r_{XY}=\\beta_1\\times\\frac{SD_X}{SD_Y}\\]"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#measuring-strength-with-scaled-variables",
    "href": "lectures/02-lecture-slides.html#measuring-strength-with-scaled-variables",
    "title": "An Introduction to Simple Regression",
    "section": "Measuring Strength with Scaled Variables",
    "text": "Measuring Strength with Scaled Variables\nIf you can estimate a regression model using the scaled (standardized) independent and dependent variables, then you can use the magnitude of \\(\\beta_1\\) to judge the strength of the relationship between the independent and dependent variable.\nScaling or standardizing a variable transforms the mean of the variable to 0 and its variance and SD to 1:\n\\[\\text{Scaled X = }Z_X=\\frac{X-\\overline{X}}{SD_X}\\]"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#the-two-approaches-will-lead-to-the-same-answer",
    "href": "lectures/02-lecture-slides.html#the-two-approaches-will-lead-to-the-same-answer",
    "title": "An Introduction to Simple Regression",
    "section": "The Two Approaches will Lead to the Same Answer",
    "text": "The Two Approaches will Lead to the Same Answer\n\ndata_ai &lt;-\n  data_ai |&gt;\n  dplyr::mutate(\n    pos_attitude_ai_scale = (pos_attitude_ai - mean(pos_attitude_ai)) / sd(pos_attitude_ai),\n    freq_use_ai_scale = scale(freq_use_ai)[,1]\n  )\n\nmod_scale &lt;- lm(freq_use_ai_scale ~ pos_attitude_ai_scale, data_ai)\nreg_coef &lt;- mod_scale$coefficients[2] |&gt; round(2)\ncor_coef &lt;- cor(data_ai$pos_attitude_ai, data_ai$freq_use_ai) |&gt; round(2)\ntibble::tibble(`Reg. Coef.` = reg_coef, `Corr. Coef` = cor_coef)\n\n# A tibble: 1 × 2\n  `Reg. Coef.` `Corr. Coef`\n         &lt;dbl&gt;        &lt;dbl&gt;\n1         0.48         0.48"
  },
  {
    "objectID": "lectures/02-lecture-slides.html",
    "href": "lectures/02-lecture-slides.html",
    "title": "An Introduction to Simple Regression",
    "section": "",
    "text": "Talk about stats for ~75 mins (5 PM - 6:15 PM ET)\nBreak for 5 minutes\nR Introduction / Hands-on coding for 40 mins (6:20 PM - 7:00 PM ET)"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#how-well-does-your-model-fit",
    "href": "lectures/02-lecture-slides.html#how-well-does-your-model-fit",
    "title": "An Introduction to Simple Regression",
    "section": "How Well Does Your Model Fit?",
    "text": "How Well Does Your Model Fit?\nYou will often want to determine how well your model fits your data—how well does your model predict your observed outcome, Y.\nTo determine this, we will partition our observed outcome into three additive pieces:\n\\[Y_{i} = \\underbrace{\\overline{Y}}_\\text{Mean Component}+\\underbrace{(\\hat{Y}_{i}-\\overline{Y})}_\\text{Model Component}+\\underbrace{(Y_{i} - \\hat{Y}_{i})}_\\text{Error Component}\\]"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#partitioning-our-observed-score-variance",
    "href": "lectures/02-lecture-slides.html#partitioning-our-observed-score-variance",
    "title": "An Introduction to Simple Regression",
    "section": "Partitioning our Observed Score Variance",
    "text": "Partitioning our Observed Score Variance\nWe can use the model and error components to summarize the variability in our outcome by partitioning it into variability because of our model and variability because of error and other unexplained causes.\n\\[\\underbrace{\\Sigma(Y_{i}-\\overline{Y})^2}_\\text{Total SS}=\\underbrace{\\Sigma(\\hat{Y}_i-\\overline{Y})^2}_\\text{SS Model}+\\underbrace{\\Sigma(Y_i-\\hat{Y}_i)}_\\text{SS Error}\\]\n\\[\\hat{\\sigma}^2_Y=\\hat{\\sigma}^2_{model}+\\hat{\\sigma}^2_{error}\\]"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#measuring-model-fit-with-r-squared",
    "href": "lectures/02-lecture-slides.html#measuring-model-fit-with-r-squared",
    "title": "An Introduction to Simple Regression",
    "section": "Measuring Model Fit with R-Squared",
    "text": "Measuring Model Fit with R-Squared\nUsing partitioned variance we can compute a statistic, \\(R^2\\), that tells us how well your model fits your data overall.\n\\[R^2 = \\frac{\\hat{\\sigma}^2_{model}}{\\hat{\\sigma}^2_{Y}}=1-\\frac{\\hat{\\sigma}^2_{error}}{\\hat{\\sigma}^2_Y} \\]\nYou can interpret \\(R^2\\) as the proportion of variance in your observed outcome that is explained by your model."
  },
  {
    "objectID": "lectures/02-lecture-slides.html#r-squared-in-our-example",
    "href": "lectures/02-lecture-slides.html#r-squared-in-our-example",
    "title": "An Introduction to Simple Regression",
    "section": "R-Squared In Our Example",
    "text": "R-Squared In Our Example\n23% of the variance in the frequency with which the sales representatives use the new AI tool can be explained by their general attitudes toward AI.\n\nsummary(mod_ai)\n\n\nCall:\nlm(formula = freq_use_ai ~ pos_attitude_ai, data = data_ai)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.9683 -1.0050  0.0072  1.0317  3.9950 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      1.51112    0.05575   27.11   &lt;2e-16 ***\npos_attitude_ai  0.49388    0.01285   38.45   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.483 on 4998 degrees of freedom\nMultiple R-squared:  0.2282,    Adjusted R-squared:  0.2281 \nF-statistic:  1478 on 1 and 4998 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#looking-at-the-model-component",
    "href": "lectures/02-lecture-slides.html#looking-at-the-model-component",
    "title": "An Introduction to Simple Regression",
    "section": "Looking at the Model Component",
    "text": "Looking at the Model Component\nThe model component tells us if our model is better able to predict our outcome than its own mean.\nIf we take a closer look at this component, what happens to it as \\(\\hat{\\beta}_1\\) becomes a perfect predictor of Y? Decreases to 0?\n\\[\\hat{Y}_{i}-\\overline{Y}\\\\=\\hat{\\beta}_0+\\hat{\\beta}_1X_{i}-\\overline{Y}\\\\=\\overline{Y}-\\hat{\\beta}_1\\overline{X}+\\hat{\\beta}_{1}X_{i}-\\overline{Y}\\\\=\\hat{\\beta}_{1}(X_{i}-\\overline{X})\\]"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#looking-at-the-error-component",
    "href": "lectures/02-lecture-slides.html#looking-at-the-error-component",
    "title": "An Introduction to Simple Regression",
    "section": "Looking at the Error Component",
    "text": "Looking at the Error Component\nThe error component or residual plays an important role in linear regression. It tell us the extent to which our model is able to predict our outcome.\nIf we take a closer look at this component, what happens to it as \\(\\hat{\\beta}_1\\) becomes a perfect predictor of Y? Decreases to 0?\n\\[Y_{i}-\\hat{Y}_i\\\\=Y_{i}-\\hat{\\beta}_0-\\hat{\\beta}_1X_{i}\\\\=Y_{i}-\\overline{Y}-\\hat{\\beta}_{1}(X_{i}-\\overline{X})\\]"
  },
  {
    "objectID": "lectures/02-lecture-slides.html#is-your-effect-significant",
    "href": "lectures/02-lecture-slides.html#is-your-effect-significant",
    "title": "An Introduction to Simple Regression",
    "section": "Is Your Effect Significant?",
    "text": "Is Your Effect Significant?\nOften researchers determine the significance of their coefficients by comparing the coefficients to a null distribution with a mean of 0.\nThe p-value tells you the probability of seeing an estimate equal to or greater than the absolute value of your estimate given that the true effect is 0.\n\n\n# A tibble: 2 × 5\n  term            estimate std.error statistic   p.value\n  &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)         1.51      0.06      27.1 4.39e-151\n2 pos_attitude_ai     0.49      0.01      38.4 1.56e-283"
  },
  {
    "objectID": "lectures/02-lecture-page.html#lecture-introduction-to-simple-regression",
    "href": "lectures/02-lecture-page.html#lecture-introduction-to-simple-regression",
    "title": "Quantitative Analysis 1",
    "section": "Lecture: Introduction to Simple Regression",
    "text": "Lecture: Introduction to Simple Regression\n\n\nTo download a pdf version of these slides, click here.\nTo download the R script that follows the R portion of the lecture, click here."
  },
  {
    "objectID": "lectures/02-lecture-slides.html#why-do-we-care-about-statistical-modeling",
    "href": "lectures/02-lecture-slides.html#why-do-we-care-about-statistical-modeling",
    "title": "An Introduction to Simple Regression",
    "section": "Why Do We Care About Statistical Modeling?",
    "text": "Why Do We Care About Statistical Modeling?\nIn research and practice, you will likely come up with questions or hypotheses of the form:\n\n\n“Are changes in Variable X associated with changes in Variable Y?”\n\n\nWe can apply the models we will talk about in class to our data in order to empirically test our hypotheses (or answer our questions). These models allow us to make data-driven evaluations of our theories."
  },
  {
    "objectID": "lectures/02-lecture-slides.html#our-measures",
    "href": "lectures/02-lecture-slides.html#our-measures",
    "title": "An Introduction to Simple Regression",
    "section": "Our Measures",
    "text": "Our Measures\nYou measure positive attitudes toward AI and frequency of tool use with the following questions:\n\nMuch of society will benefit from a future full of Artificial Intelligence. [Positive Attiude]\nIn the past two months, how frequently have you used the organization’s new AI powered chat bot? [Frequency of Use]"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#schedule-for-today",
    "href": "lectures/03-lecture-slides.html#schedule-for-today",
    "title": "An Introduction to Multiple Regression",
    "section": "Schedule for Today",
    "text": "Schedule for Today\n\nTalk about stats for ~75 mins (5 PM - 6:15 PM ET)\nBreak for 5 minutes\nFinish up stats / R for 40 mins (6:20 PM - 7:00 PM ET)"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#overview",
    "href": "lectures/03-lecture-slides.html#overview",
    "title": "An Introduction to Multiple Regression",
    "section": "Overview",
    "text": "Overview\n\nA quick review of last week\nSetup of our working example\nIntroduction to multiple regression"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#goals",
    "href": "lectures/03-lecture-slides.html#goals",
    "title": "An Introduction to Multiple Regression",
    "section": "Goals",
    "text": "Goals\n\nDevelop an understanding of multiple regression\nWrite your first R script"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#example",
    "href": "lectures/03-lecture-slides.html#example",
    "title": "An Introduction to Multiple Regression",
    "section": "Example",
    "text": "Example\nYour organization has just implemented a new generative AI tool (fancy chat bot) to help improve the efficiency of the organizations sales force. Sales employees, however, have adopted the technology at different rates.\nYou have been asked to determine why employees differ in their rates of adoption. Using the Unified Theory of Acceptance and Use of Technology, you developed a model that states:\n\n\nThe frequency of technology use is related to an employee’s positive attitudes towards AI, their anxiety around technology use, and their intentions to use the new technology."
  },
  {
    "objectID": "lectures/03-lecture-slides.html#measures",
    "href": "lectures/03-lecture-slides.html#measures",
    "title": "An Introduction to Multiple Regression",
    "section": "Measures",
    "text": "Measures\nYou measure these variables with the following questions:\n\nUsing technology such as chatbots makes me anxious. [Technology Anxiety]\n\n1: Completely Disagree to 7: Completely Agree\n\nI intend to use the chatbot to help me with my sales. [Behavioral Intentions]\n\n1: Completely Disagree to 7: Completely Agree\n\nIn the past two months, how frequently have you used the organization’s new AI powered chat bot? [Frequency of Use]\n\n1: Never to 6: All the Time"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#what-is-linear-regression",
    "href": "lectures/03-lecture-slides.html#what-is-linear-regression",
    "title": "An Introduction to Multiple Regression",
    "section": "What is Linear Regression?",
    "text": "What is Linear Regression?\nLinear regression is a statistical model that allows you to test whether a change in a predictor variable, like positive attitudes towards AI, is linearly related to change in an outcome variable, like frequency of AI tool use."
  },
  {
    "objectID": "lectures/03-lecture-slides.html#review-of-simple-regression",
    "href": "lectures/03-lecture-slides.html#review-of-simple-regression",
    "title": "An Introduction to Multiple Regression",
    "section": "Review of Simple Regression",
    "text": "Review of Simple Regression\nThe simple regression model is just a linear regression model with one independent variable.\n\\[Y=\\beta_0+\\beta_1X_1+\\epsilon\\]\n\n\\(\\beta_0\\): The expected value (mean) of Y when X = 0.\n\\(\\beta_1\\): The average change in Y for a one-unit increase in X.\n\\(\\epsilon\\): Variation in Y that is not explained by our model—unexplained variance."
  },
  {
    "objectID": "lectures/03-lecture-slides.html#interpreting-the-regression-slope",
    "href": "lectures/03-lecture-slides.html#interpreting-the-regression-slope",
    "title": "An Introduction to Multiple Regression",
    "section": "Interpreting the Regression Slope",
    "text": "Interpreting the Regression Slope\nThe most appropriate way to interpret the slope coefficient (\\(\\beta_1\\)) is as a comparison:\n\nThe average difference in the frequency with which employees use the AI tool, when comparing two people who differ in their positive attitudes towards AI by one point (e.g. 5: Agree vs 6: Strongly Agree), is equal to the value of \\(\\beta_1\\)."
  },
  {
    "objectID": "lectures/03-lecture-slides.html#ai-adoption-example-problem-statement",
    "href": "lectures/03-lecture-slides.html#ai-adoption-example-problem-statement",
    "title": "An Introduction to Multiple Regression",
    "section": "AI Adoption Example: Problem Statement",
    "text": "AI Adoption Example: Problem Statement\nYour organization has just implemented a new generative AI tool (fancy chat bot) to help improve the efficiency of the organizations sales force. Sales employees, however, have started using the technology at different rates.\nYou have been asked to determine why employees differ in their usage rates."
  },
  {
    "objectID": "lectures/03-lecture-slides.html#ai-adoption-example-hypothesis",
    "href": "lectures/03-lecture-slides.html#ai-adoption-example-hypothesis",
    "title": "An Introduction to Multiple Regression",
    "section": "AI Adoption Example: Hypothesis",
    "text": "AI Adoption Example: Hypothesis\nUsing the Unified Theory of Acceptance and Use of Technology, you develop the following hypotheses:\n\nAn employee’s technology anxiety is negatively related to the frequency with which they use the AI tool.\nAn employee’s intention to use the AI tool is positively related to the frequency with which they use the AI tool."
  },
  {
    "objectID": "lectures/03-lecture-slides.html#determining-the-strength-of-the-regression-slope",
    "href": "lectures/03-lecture-slides.html#determining-the-strength-of-the-regression-slope",
    "title": "An Introduction to Multiple Regression",
    "section": "Determining the Strength of the Regression Slope",
    "text": "Determining the Strength of the Regression Slope\nWe should be careful about using the magnitude of the regression slope to make inferences about the strength of the relationship between the predictor variable and the outcome variable unless we have transformed both our predictor and outcome so that their units are in standard deviations.\nThe magnitude of the regression coefficient is directly related to the scale of the predictor variable, so permissible changes to the scale (e.g. converting hours to minutes or Lbs to Kilograms) will change the magnitude of the regression coefficient."
  },
  {
    "objectID": "lectures/03-lecture-slides.html#understanding-the-overall-fit-of-our-model",
    "href": "lectures/03-lecture-slides.html#understanding-the-overall-fit-of-our-model",
    "title": "An Introduction to Multiple Regression",
    "section": "Understanding the Overall Fit of our Model",
    "text": "Understanding the Overall Fit of our Model\nWe can separate the variance of our outcome into two additive pieces: Variance due to things we have modeled & Variance due to things we have not modeled:\n\\[\\sigma^2_Y=\\sigma^2_{model} + \\sigma^2_{error}\\]\nThe ratio of \\(\\sigma^2_{model}\\) to \\(\\sigma^2_{Y}\\) (\\(\\frac{\\sigma^2_{model}}{\\sigma^2_{Y}}\\)) is equal to the model’s \\(R^2\\), which tells us the proportion of variance in our outcome that is due to our model. The larger the \\(R^2\\), the better our model fits the data."
  },
  {
    "objectID": "lectures/03-lecture-slides.html#the-multiple-regression-model",
    "href": "lectures/03-lecture-slides.html#the-multiple-regression-model",
    "title": "An Introduction to Multiple Regression",
    "section": "The Multiple Regression Model",
    "text": "The Multiple Regression Model\nThe multiple regression model is just a linear regression model with more than one predictor variable.\n\\[Y = \\beta_0 + \\beta_1X_1+\\beta_2X_2+...+\\beta_pX_p+\\epsilon\\]\n\n\\(\\beta_0\\) or Intercept: The expected value (mean) of \\(Y\\) when all \\(X\\) = 0.\n\\(\\beta_p\\) or Partial Regression Coefficient: The average change in \\(Y\\) for a one-unit increase in \\(X_p\\), holding all other \\(Xs\\) constant.\n\\(\\epsilon\\) or Residual: The part of Y that is not explained by our model—unexplained variance."
  },
  {
    "objectID": "lectures/03-lecture-slides.html#why-do-we-use-multiple-regression",
    "href": "lectures/03-lecture-slides.html#why-do-we-use-multiple-regression",
    "title": "An Introduction to Multiple Regression",
    "section": "Why Do We Use Multiple Regression?",
    "text": "Why Do We Use Multiple Regression?\nWe use multiple regression because it allows us to:\n\nEstimate the effect of a predictor variable while controlling for the effects of the other predictor variables in the model.\nTo estimate and test the effects of multiple predictors in a single model.\nAllows us to understand how much of the total variance in our outcome variable we can explain with our predictor variables."
  },
  {
    "objectID": "lectures/03-lecture-slides.html#what-do-we-mean-by-control",
    "href": "lectures/03-lecture-slides.html#what-do-we-mean-by-control",
    "title": "An Introduction to Multiple Regression",
    "section": "What Do We Mean by Control?",
    "text": "What Do We Mean by Control?"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#intepreting-a-partial-regression-coefficient",
    "href": "lectures/03-lecture-slides.html#intepreting-a-partial-regression-coefficient",
    "title": "An Introduction to Multiple Regression",
    "section": "Intepreting a Partial Regression Coefficient",
    "text": "Intepreting a Partial Regression Coefficient\nThe average difference in frequency of AI tool use is 0.5671689 when comparing two employees who have equal levels of technological anxiety but differ in their intention to use the AI tool by one point (unit)."
  },
  {
    "objectID": "lectures/03-lecture-slides.html#visualizing-a-partial-regression-coefficient",
    "href": "lectures/03-lecture-slides.html#visualizing-a-partial-regression-coefficient",
    "title": "An Introduction to Multiple Regression",
    "section": "Visualizing a Partial Regression Coefficient",
    "text": "Visualizing a Partial Regression Coefficient"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#what-happens-if-we-fit-several-simple-regression-models",
    "href": "lectures/03-lecture-slides.html#what-happens-if-we-fit-several-simple-regression-models",
    "title": "An Introduction to Multiple Regression",
    "section": "What Happens if We Fit Several Simple Regression Models?",
    "text": "What Happens if We Fit Several Simple Regression Models?\nAs an example of how our understanding of the relationship between a predictor variable and an outcome variable changes as we include additional predictor variables, let’s look at how the relationships between technological anxiety, intentions to use the AI tool, and frequency of AI tool use change as we move from simple regression to multiple regression."
  },
  {
    "objectID": "lectures/03-lecture-slides.html#model-1-positive-attitudes-prediciting-ai-use",
    "href": "lectures/03-lecture-slides.html#model-1-positive-attitudes-prediciting-ai-use",
    "title": "An Introduction to Multiple Regression",
    "section": "Model 1: Positive Attitudes Prediciting AI Use",
    "text": "Model 1: Positive Attitudes Prediciting AI Use\n\nmodel_1 &lt;- lm(freq_use_ai ~ pos_attitude_ai, data = data_ai)\nsummary(model_1)\n\n\nCall:\nlm(formula = freq_use_ai ~ pos_attitude_ai, data = data_ai)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.9942 -0.7443  0.2557  0.8391  3.5057 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      2.07760    0.04603   45.13   &lt;2e-16 ***\npos_attitude_ai  0.41666    0.01061   39.28   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.224 on 4998 degrees of freedom\nMultiple R-squared:  0.2359,    Adjusted R-squared:  0.2358 \nF-statistic:  1543 on 1 and 4998 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#model-2-technological-anxiety-predicting-ai-use",
    "href": "lectures/03-lecture-slides.html#model-2-technological-anxiety-predicting-ai-use",
    "title": "An Introduction to Multiple Regression",
    "section": "Model 2: Technological Anxiety Predicting AI Use",
    "text": "Model 2: Technological Anxiety Predicting AI Use\n\nmodel_2 &lt;- lm(freq_use_ai ~ tech_anx, data = data_ai)\nsummary(model_2)\n\n\nCall:\nlm(formula = freq_use_ai ~ tech_anx, data = data_ai)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.4698 -0.7707  0.2293  0.9284  2.9284 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  4.81931    0.04424  108.94   &lt;2e-16 ***\ntech_anx    -0.34954    0.01317  -26.55   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.311 on 4998 degrees of freedom\nMultiple R-squared:  0.1236,    Adjusted R-squared:  0.1234 \nF-statistic: 704.9 on 1 and 4998 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#model-3-attitudes-anxiety-predicting-ai-use",
    "href": "lectures/03-lecture-slides.html#model-3-attitudes-anxiety-predicting-ai-use",
    "title": "An Introduction to Multiple Regression",
    "section": "Model 3: Attitudes & Anxiety Predicting AI Use",
    "text": "Model 3: Attitudes & Anxiety Predicting AI Use\n\nmodel_3 &lt;- lm(freq_use_ai ~ pos_attitude_ai + tech_anx, data = data_ai)\nsummary(model_3)\n\n\nCall:\nlm(formula = freq_use_ai ~ pos_attitude_ai + tech_anx, data = data_ai)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.8202 -0.7524  0.2476  0.8917  3.1796 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      2.73606    0.08277  33.057   &lt;2e-16 ***\npos_attitude_ai  0.35595    0.01229  28.961   &lt;2e-16 ***\ntech_anx        -0.13582    0.01424  -9.535   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.214 on 4997 degrees of freedom\nMultiple R-squared:  0.2496,    Adjusted R-squared:  0.2493 \nF-statistic: 830.9 on 2 and 4997 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#omitted-variable-bias-what-happens-when-we-leave-out-a-variable",
    "href": "lectures/03-lecture-slides.html#omitted-variable-bias-what-happens-when-we-leave-out-a-variable",
    "title": "An Introduction to Multiple Regression",
    "section": "Omitted Variable Bias: What Happens When We Leave Out a Variable",
    "text": "Omitted Variable Bias: What Happens When We Leave Out a Variable\nIf we leave out a predictor variable from our model that is related to both the predictor variable we included in our model and our outcome variable, then we should almost always find a way to include the left out variable.\nIf we do not include this variable, we commit the omitted variable bias, where the effect of the omitted predictor variable on the outcome variable gets mixed together with the effect of the included predictor variable."
  },
  {
    "objectID": "lectures/03-lecture-slides.html#algebra-behind-ommitted-variable-bias",
    "href": "lectures/03-lecture-slides.html#algebra-behind-ommitted-variable-bias",
    "title": "An Introduction to Multiple Regression",
    "section": "Algebra Behind Ommitted Variable Bias",
    "text": "Algebra Behind Ommitted Variable Bias\n\\[X_{\\text{Tech Anxiey}}=\\alpha_0 + \\alpha_1X_{\\text{AI Attitude}}\\]\n\\[Y_{\\text{AI Use}}=\\beta_0 + \\beta_1X_{AI Attitude}+\\beta_2X_{\\text{Tech Anxiety}}\\]\n\\[Y_{\\text{AI Use}}=\\beta_0 + \\beta_1X_{AI Attitude}+\\beta_2(\\alpha_0 + \\alpha_1X_{\\text{AI Attitude}})\\]\n\\[Y_{\\text{AI Use}}=(\\beta_0 + \\beta_2*\\alpha_0) + (\\beta_1+\\beta_2*\\alpha_1)X_{AI Attitude}\\]"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#prediction-error-in-multiple-regression",
    "href": "lectures/03-lecture-slides.html#prediction-error-in-multiple-regression",
    "title": "An Introduction to Multiple Regression",
    "section": "Prediction & Error in Multiple Regression",
    "text": "Prediction & Error in Multiple Regression"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#judging-the-strength-of-a-partial-regression-coefficient",
    "href": "lectures/03-lecture-slides.html#judging-the-strength-of-a-partial-regression-coefficient",
    "title": "An Introduction to Multiple Regression",
    "section": "Judging the Strength of a Partial Regression Coefficient",
    "text": "Judging the Strength of a Partial Regression Coefficient"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#estimating-our-full-model",
    "href": "lectures/03-lecture-slides.html#estimating-our-full-model",
    "title": "An Introduction to Multiple Regression",
    "section": "Estimating Our Full Model",
    "text": "Estimating Our Full Model\n\nmodel_full &lt;- lm(freq_use_ai ~ tech_anx + beh_intent_ai,\n                 data = data_ai)\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n1.342\n0.070\n19.090\n0.000\n\n\ntech_anx\n-0.014\n0.012\n-1.132\n0.258\n\n\nbeh_intent_ai\n0.567\n0.010\n56.349\n0.000"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#model-1-technological-anxiety-prediciting-ai-use",
    "href": "lectures/03-lecture-slides.html#model-1-technological-anxiety-prediciting-ai-use",
    "title": "An Introduction to Multiple Regression",
    "section": "Model 1: Technological Anxiety Prediciting AI Use",
    "text": "Model 1: Technological Anxiety Prediciting AI Use\nShould we conclude that technological anxiety is significantly and negatively related to frequency of AI tool use?\n\nmodel_1 &lt;- lm(freq_use_ai ~ tech_anx, data = data_ai)\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n4.740\n0.046\n102.682\n0\n\n\ntech_anx\n-0.322\n0.014\n-23.144\n0"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#model-2-behavioral-intentions-predicting-ai-use",
    "href": "lectures/03-lecture-slides.html#model-2-behavioral-intentions-predicting-ai-use",
    "title": "An Introduction to Multiple Regression",
    "section": "Model 2: Behavioral Intentions Predicting AI Use",
    "text": "Model 2: Behavioral Intentions Predicting AI Use\nShould we conclude that intention to use the AI tool is significantly and positively related to frequency of AI tool use?\n\nmodel_2 &lt;- lm(freq_use_ai ~ beh_intent_ai, data = data_ai)\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n1.278\n0.042\n30.665\n0\n\n\nbeh_intent_ai\n0.572\n0.009\n63.636\n0"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#model-3-anxiety-intentions-predicting-ai-use",
    "href": "lectures/03-lecture-slides.html#model-3-anxiety-intentions-predicting-ai-use",
    "title": "An Introduction to Multiple Regression",
    "section": "Model 3: Anxiety & Intentions Predicting AI Use",
    "text": "Model 3: Anxiety & Intentions Predicting AI Use\nHow do our previous conclusions change? What happened to technological anxiety?\n\nmodel_3 &lt;- lm(freq_use_ai ~ tech_anx + beh_intent_ai, data = data_ai)\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n1.342\n0.070\n19.090\n0.000\n\n\ntech_anx\n-0.014\n0.012\n-1.132\n0.258\n\n\nbeh_intent_ai\n0.567\n0.010\n56.349\n0.000"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#algebra-behind-the-ommitted-variable-bias",
    "href": "lectures/03-lecture-slides.html#algebra-behind-the-ommitted-variable-bias",
    "title": "An Introduction to Multiple Regression",
    "section": "Algebra Behind the Ommitted Variable Bias",
    "text": "Algebra Behind the Ommitted Variable Bias\n\\[X_{\\text{Behavioral Intent.}}=\\alpha_0 + \\alpha_1X_{\\text{Tech. Anxiety}}\\]"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#multicollinearity-problems-when-our-predictors-are-correlated",
    "href": "lectures/03-lecture-slides.html#multicollinearity-problems-when-our-predictors-are-correlated",
    "title": "An Introduction to Multiple Regression",
    "section": "Multicollinearity: Problems when our Predictors are Correlated",
    "text": "Multicollinearity: Problems when our Predictors are Correlated"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#algebra-behind-the-ommitted-variable-bias-1",
    "href": "lectures/03-lecture-slides.html#algebra-behind-the-ommitted-variable-bias-1",
    "title": "An Introduction to Multiple Regression",
    "section": "Algebra Behind the Ommitted Variable Bias",
    "text": "Algebra Behind the Ommitted Variable Bias\n\\[X_{\\text{Behavioral Intent.}}=\\alpha_0 + \\alpha_1X_{\\text{Tech. Anxiety}}\\]\n\\[Y_{\\text{AI Use}}=\\beta_0 + \\beta_1X_{\\text{Tech. Anxiety}}+\\beta_2X_{\\text{Behavioral Intent.}}\\]"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#algebra-behind-the-ommitted-variable-bias-2",
    "href": "lectures/03-lecture-slides.html#algebra-behind-the-ommitted-variable-bias-2",
    "title": "An Introduction to Multiple Regression",
    "section": "Algebra Behind the Ommitted Variable Bias",
    "text": "Algebra Behind the Ommitted Variable Bias\n\\[X_{\\text{Behavioral Intent.}}=\\alpha_0 + \\alpha_1X_{\\text{Tech. Anxiety}}\\]\n\\[Y_{\\text{AI Use}}=\\beta_0 + \\beta_1X_{\\text{Tech. Anxiety}}+\\beta_2X_{\\text{Behavioral Intent.}}\\]\n\\[Y_{\\text{AI Use}}=\\beta_0 + \\beta_1X_{\\text{Tech. Anxiety}}+\\beta_2(\\alpha_0 + \\alpha_1X_{\\text{Tech. Anxiety}})\\]"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#algebra-behind-the-ommitted-variable-bias-3",
    "href": "lectures/03-lecture-slides.html#algebra-behind-the-ommitted-variable-bias-3",
    "title": "An Introduction to Multiple Regression",
    "section": "Algebra Behind the Ommitted Variable Bias",
    "text": "Algebra Behind the Ommitted Variable Bias\n\\[X_{\\text{Behavioral Intent.}}=\\alpha_0 + \\alpha_1X_{\\text{Tech. Anxiety}}\\]\n\\[Y_{\\text{AI Use}}=\\beta_0 + \\beta_1X_{\\text{Tech. Anxiety}}+\\beta_2X_{\\text{Behavioral Intent.}}\\]\n\\[Y_{\\text{AI Use}}=\\beta_0 + \\beta_1X_{\\text{Tech. Anxiety}}+\\beta_2(\\alpha_0 + \\alpha_1X_{\\text{Tech. Anxiety}})\\]\n\\[Y_{\\text{AI Use}}=(\\beta_0 + \\beta_2*\\alpha_0) + (\\beta_1+\\beta_2*\\alpha_1)X_{\\text{Tech. Anxiety}}\\]"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#what-do-we-mean-by-statistical-control",
    "href": "lectures/03-lecture-slides.html#what-do-we-mean-by-statistical-control",
    "title": "An Introduction to Multiple Regression",
    "section": "What Do We Mean by Statistical Control?",
    "text": "What Do We Mean by Statistical Control?\nStatistical control is a fancy way of saying “what is the effect of a predictor variable on an outcome variable for people (or units) who have the same measurements on all of the other predictor variables in the model?”\n\nWhat effect does an individual’s intention to use the AI tool have on the frequency they use an AI tool, if we hold their level of technological anxiety constant?"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#an-example-of-statistical-control",
    "href": "lectures/03-lecture-slides.html#an-example-of-statistical-control",
    "title": "An Introduction to Multiple Regression",
    "section": "An Example of Statistical Control",
    "text": "An Example of Statistical Control\nConceptually, if we wanted to estimate the relationship that intention to use the AI tool has on the frequency of AI tool use, while controlling (adjusting) for the level of technological anxiety, we could create multiple new datasets from our original dataset where each new dataset had a fixed level of technological anxiety (e.g. in the first dataset, we would only look at observtations where tech_anxiety == 1).\nThen we could estimate a simple regression model that esimates the relationship between intentions to use the AI tool (beh_intent_ai) and frequency of tool use (freq_use_ai) to each dataset. The average of those seven regression slopes (one for each dataset) would be nearly equivalent to the partial coefficient estimated from a multiple regression model that included beh_intent_ai and tech_anxiety."
  },
  {
    "objectID": "lectures/03-lecture-slides.html#why-do-we-control",
    "href": "lectures/03-lecture-slides.html#why-do-we-control",
    "title": "An Introduction to Multiple Regression",
    "section": "Why Do We Control",
    "text": "Why Do We Control"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#understanding-prediction-in-multiple-regression",
    "href": "lectures/03-lecture-slides.html#understanding-prediction-in-multiple-regression",
    "title": "An Introduction to Multiple Regression",
    "section": "Understanding Prediction in Multiple Regression",
    "text": "Understanding Prediction in Multiple Regression\nSimilar to simple regression, we can decompose the observed value of the outcome variable, \\(Y\\), into a model component and an error component. For multiple regression, the prediction is a linear combination of all \\(p\\) predictor variables.\n\\[\\hat{Y}_i=\\hat{\\beta}_0+\\hat{\\beta}_1X_{1}+\\hat{\\beta}_2X_{2}+...+\\hat{\\beta}_{p}\\]"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#understanding-error-in-multiple-regression",
    "href": "lectures/03-lecture-slides.html#understanding-error-in-multiple-regression",
    "title": "An Introduction to Multiple Regression",
    "section": "Understanding Error in Multiple Regression",
    "text": "Understanding Error in Multiple Regression\nRemember to think of error as anything that is not predicted (or modeled) by our model. This could be random noise as well as other predictor variables that we did not measure.\n\\[\\hat{e}=Y_i-\\hat{Y}_i\\] \\[\\hat{e}=\\text{Observed} - \\text{Predicted}\\]"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#judging-overall-model-strength-the-multiple-r-and-r-squared",
    "href": "lectures/03-lecture-slides.html#judging-overall-model-strength-the-multiple-r-and-r-squared",
    "title": "An Introduction to Multiple Regression",
    "section": "Judging Overall Model Strength: The Multiple R and R-Squared",
    "text": "Judging Overall Model Strength: The Multiple R and R-Squared"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#standardized-partial-regression-coefficient",
    "href": "lectures/03-lecture-slides.html#standardized-partial-regression-coefficient",
    "title": "An Introduction to Multiple Regression",
    "section": "Standardized Partial Regression Coefficient",
    "text": "Standardized Partial Regression Coefficient\nIf we put all of our predictor variables on the same scale, then we can make relative comparisons. One way to do this is by standardizing all of our predictor variables:\n\\[Z_{p}=\\frac{X_p-\\overline{X}_p}{SD(X_p)}\\]\nThe scale for each predictor variable would then be in standard deviation units."
  },
  {
    "objectID": "lectures/03-lecture-slides.html#semipartial-correlation",
    "href": "lectures/03-lecture-slides.html#semipartial-correlation",
    "title": "An Introduction to Multiple Regression",
    "section": "Semipartial Correlation",
    "text": "Semipartial Correlation"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#understanding-the-semipartial-correlation",
    "href": "lectures/03-lecture-slides.html#understanding-the-semipartial-correlation",
    "title": "An Introduction to Multiple Regression",
    "section": "Understanding the Semipartial Correlation",
    "text": "Understanding the Semipartial Correlation"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#should-you-use-the-standardized-partial-coefficeint-or-semipartial-correlation",
    "href": "lectures/03-lecture-slides.html#should-you-use-the-standardized-partial-coefficeint-or-semipartial-correlation",
    "title": "An Introduction to Multiple Regression",
    "section": "Should You Use the Standardized Partial Coefficeint or Semipartial Correlation?",
    "text": "Should You Use the Standardized Partial Coefficeint or Semipartial Correlation?"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#why-do-we-inlcude-multiple-predictors",
    "href": "lectures/03-lecture-slides.html#why-do-we-inlcude-multiple-predictors",
    "title": "An Introduction to Multiple Regression",
    "section": "Why Do We Inlcude Multiple Predictors?",
    "text": "Why Do We Inlcude Multiple Predictors?"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#visualize-control",
    "href": "lectures/03-lecture-slides.html#visualize-control",
    "title": "An Introduction to Multiple Regression",
    "section": "Visualize Control",
    "text": "Visualize Control"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#visualize-control-1",
    "href": "lectures/03-lecture-slides.html#visualize-control-1",
    "title": "An Introduction to Multiple Regression",
    "section": "Visualize Control",
    "text": "Visualize Control"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#an-example-of-statistical-control-example-dataset-1",
    "href": "lectures/03-lecture-slides.html#an-example-of-statistical-control-example-dataset-1",
    "title": "An Introduction to Multiple Regression",
    "section": "An Example of Statistical Control: Example Dataset 1",
    "text": "An Example of Statistical Control: Example Dataset 1\n\ndata_subset_1 &lt;- data_ai |&gt; dplyr::filter(tech_anx == 1)\n\n\n\n# A tibble: 721 × 4\n   employee_id freq_use_ai tech_anx beh_intent_ai\n   &lt;fct&gt;             &lt;dbl&gt;    &lt;dbl&gt;         &lt;dbl&gt;\n 1 760406                3        1             5\n 2 778236                5        1             4\n 3 339526                4        1             7\n 4 161547                4        1             5\n 5 708206                4        1             3\n 6 900585                5        1             5\n 7 558405                5        1             7\n 8 869289                5        1             5\n 9 739811                5        1             5\n10 584814                3        1             5\n# ℹ 711 more rows"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#visualizing-statistical-control",
    "href": "lectures/03-lecture-slides.html#visualizing-statistical-control",
    "title": "An Introduction to Multiple Regression",
    "section": "Visualizing Statistical Control",
    "text": "Visualizing Statistical Control\nThe plot below shows seven different regression lines each of which estimate the effect that one’s intention to use an AI tool has on the frequency with which they actually use the AI tool at a fixed level of technological anxiety (e.g. tech_anx == 1 or tech_anx == 2)."
  },
  {
    "objectID": "lectures/03-lecture-slides.html#visualizing-statistical-control-1",
    "href": "lectures/03-lecture-slides.html#visualizing-statistical-control-1",
    "title": "An Introduction to Multiple Regression",
    "section": "Visualizing Statistical Control",
    "text": "Visualizing Statistical Control\nThe plot below shows the same seven simple regression lines, but now the slope of the partial regression coefficient for the effect of intention to use the AI tool on frequency of tool use controlling for technological anxiety is laid over the simple regression lines."
  },
  {
    "objectID": "lectures/03-lecture-slides.html#why-do-we-include-multiple-predictor-variables",
    "href": "lectures/03-lecture-slides.html#why-do-we-include-multiple-predictor-variables",
    "title": "An Introduction to Multiple Regression",
    "section": "Why Do We Include Multiple Predictor Variables?",
    "text": "Why Do We Include Multiple Predictor Variables?\nThere are two main reasons to include additional predictor variables in your regression model:\n\nThe effect of a predictor variable on an outcome variable in a simple regression model may be incorrectly estimated if we do not take into consideration other predictor variables.\nBy adding additional predictor variables into our model, we reduce the error component of our model, which makes it more likely we will find a true significant relationship between our predictor variables and outcome variable."
  },
  {
    "objectID": "lectures/03-lecture-slides.html#omitted-variable-bias-in-our-data",
    "href": "lectures/03-lecture-slides.html#omitted-variable-bias-in-our-data",
    "title": "An Introduction to Multiple Regression",
    "section": "Omitted Variable Bias in Our Data",
    "text": "Omitted Variable Bias in Our Data\nThis is what happens with tech_anxiety and beh_intent_ai. When we leave out beh_intent_ai in our model, part of its effect on freq_use_ai gets mixed into the effect that tech_anx has on freq_use_ai.\n\n\n\nlm(beh_intent_ai ~ tech_anx, data_ai)\n\n\n\n\n\n\nterm\nestimate\np.value\n\n\n\n\n(Intercept)\n5.99\n0\n\n\ntech_anx\n-0.54\n0\n\n\n\n\n\n\nRight column\n\nRight column"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#interpreting-a-partial-regression-coefficient",
    "href": "lectures/03-lecture-slides.html#interpreting-a-partial-regression-coefficient",
    "title": "An Introduction to Multiple Regression",
    "section": "Interpreting a Partial Regression Coefficient",
    "text": "Interpreting a Partial Regression Coefficient\nLike with simple regression, the most appropriate interpretation of a partial regression coefficient is as a mean comparison between two groups that differ on their predictor variable by one point, but have identical values for the other predictor variables."
  },
  {
    "objectID": "lectures/03-lecture-slides.html#interpreting-the-partial-coefficient-for-behavioral-intentions",
    "href": "lectures/03-lecture-slides.html#interpreting-the-partial-coefficient-for-behavioral-intentions",
    "title": "An Introduction to Multiple Regression",
    "section": "Interpreting the Partial Coefficient for Behavioral Intentions",
    "text": "Interpreting the Partial Coefficient for Behavioral Intentions\nThe average difference in frequency of AI tool use is 0.57 points when comparing two employees who have equal levels of technological anxiety but differ in their intention to use the AI tool by one point (unit)."
  },
  {
    "objectID": "lectures/03-lecture-slides.html#predicting-frequency-of-ai-tool-use",
    "href": "lectures/03-lecture-slides.html#predicting-frequency-of-ai-tool-use",
    "title": "An Introduction to Multiple Regression",
    "section": "Predicting Frequency of AI Tool Use",
    "text": "Predicting Frequency of AI Tool Use\nOur best prediction for an employee who is highly anxious about technology (tech_anx == 7) and does not intend to use the AI tool (beh_intent_ai == 1):\n\\[1.84 = 1.34 + -.01_{\\text{Tech. Anx.}}*7+.57_{\\text{Beh. Int.}}*1\\]\nOur best prediction for an employee who is not anxious about technology (tech_anx == 1) and intends to use the AI tool (beh_intent_ai == 7):\n\\[5.32 = 1.34 + -.01_{\\text{Tech. Anx.}}*1+.57_{\\text{Beh. Int.}}*7\\]"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#error-in-our-ai-model",
    "href": "lectures/03-lecture-slides.html#error-in-our-ai-model",
    "title": "An Introduction to Multiple Regression",
    "section": "Error in Our AI Model",
    "text": "Error in Our AI Model\nBelow is a data frame that contains a sample of some of the errors our model made. How would you interpret a negative error? A positive error?\n\n\n# A tibble: 10 × 6\n   employee_id tech_anx beh_intent_ai freq_use_ai predicted_freq_use_ai error\n   &lt;fct&gt;          &lt;dbl&gt;         &lt;dbl&gt;       &lt;dbl&gt;                 &lt;dbl&gt; &lt;dbl&gt;\n 1 381064             1             7           4                  5.3  -1.3 \n 2 338015             7             1           1                  1.81 -0.81\n 3 703375             1             7           6                  5.3   0.7 \n 4 611741             1             7           6                  5.3   0.7 \n 5 812441             1             7           5                  5.3  -0.3 \n 6 475459             1             7           5                  5.3  -0.3 \n 7 921092             1             7           5                  5.3  -0.3 \n 8 567786             1             7           5                  5.3  -0.3 \n 9 680605             1             7           5                  5.3  -0.3 \n10 239815             7             1           2                  1.81  0.19"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#judging-overall-model-strength-the-multiple-correlation",
    "href": "lectures/03-lecture-slides.html#judging-overall-model-strength-the-multiple-correlation",
    "title": "An Introduction to Multiple Regression",
    "section": "Judging Overall Model Strength: The Multiple Correlation",
    "text": "Judging Overall Model Strength: The Multiple Correlation\nThe multiple correlation coefficient, \\(R\\), is the correlation between our model prediction, \\(\\hat{Y}\\) and our observed outcome variable, \\(Y\\):\n\\[\\text{Multiple Corelation = }r_{Y,\\hat{Y}}\\]"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#the-muliple-correlation-in-our-ai-data",
    "href": "lectures/03-lecture-slides.html#the-muliple-correlation-in-our-ai-data",
    "title": "An Introduction to Multiple Regression",
    "section": "The Muliple Correlation in Our AI Data",
    "text": "The Muliple Correlation in Our AI Data\n\ndata_ai_pred &lt;-\n  data_ai |&gt;\n  dplyr::mutate(\n    pred_y = predict(model_3)\n  )\n\n\n\nHere is a look at our data frame which contains the actual and predicted values of freq_use_ai:\n\n\n# A tibble: 5,000 × 2\n   freq_use_ai pred_y\n         &lt;dbl&gt;  &lt;dbl&gt;\n 1           5   4.69\n 2           4   4.11\n 3           4   3.57\n 4           4   5.27\n 5           1   1.87\n 6           4   3.57\n 7           5   2.42\n 8           3   4.16\n 9           5   3.60\n10           5   3.57\n# ℹ 4,990 more rows\n\n\n\nHere is the correlation between our actual and predicted values:\n\n\n            freq_use_ai pred_y\nfreq_use_ai       1.000  0.669\npred_y            0.669  1.000"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#judging-overall-model-strength-r-squared",
    "href": "lectures/03-lecture-slides.html#judging-overall-model-strength-r-squared",
    "title": "An Introduction to Multiple Regression",
    "section": "Judging Overall Model Strength: R-Squared",
    "text": "Judging Overall Model Strength: R-Squared\nSimilar to simple regression, we can calculate an \\(R^2\\) value which will tell us the proportion of variance in our outcome variable that is explained by all of our predictor variables:\n\n\\(R^2\\) is the square of the multiple correlation coefficient.\n\\(R^2\\) falls between 0 (no prediction) and 1 (perfect prediction).\nThe larger the value of \\(R^2\\), the better the set of predictor variables collectively predicts \\(Y\\).\n\\(R^2\\) will equal 0 only when all of the partial regression coefficients equal 0.\n\\(R^2\\) will never decrease when a new predictor variable is added to a model."
  },
  {
    "objectID": "lectures/03-lecture-slides.html#the-r-squared-in-our-ai-data",
    "href": "lectures/03-lecture-slides.html#the-r-squared-in-our-ai-data",
    "title": "An Introduction to Multiple Regression",
    "section": "The R-Squared in Our AI Data",
    "text": "The R-Squared in Our AI Data\nThe multiple correlation is equal to 0.669, which is equal to 0.4477 when squared.\n\n\n\nCall:\nlm(formula = freq_use_ai ~ tech_anx + beh_intent_ai, data = data_ai)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.7311 -0.6897 -0.1087  0.8499  3.1599 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    1.34183    0.07029  19.090   &lt;2e-16 ***\ntech_anx      -0.01379    0.01218  -1.132    0.258    \nbeh_intent_ai  0.56717    0.01007  56.349   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.029 on 4997 degrees of freedom\nMultiple R-squared:  0.4477,    Adjusted R-squared:  0.4475 \nF-statistic:  2025 on 2 and 4997 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#judging-the-relative-strength-of-a-partial-regression-coefficient",
    "href": "lectures/03-lecture-slides.html#judging-the-relative-strength-of-a-partial-regression-coefficient",
    "title": "An Introduction to Multiple Regression",
    "section": "Judging the Relative Strength of a Partial Regression Coefficient",
    "text": "Judging the Relative Strength of a Partial Regression Coefficient\nBecause the magnitude of the partial regression coefficient depends on the scale of its predictor variable, we cannot compare the partial regression coefficient of one predictor variable to the partial regression of another predictor variable unless both predictor variables have identical scales."
  },
  {
    "objectID": "lectures/03-lecture-slides.html#comparing-the-effects-of-tech-anxiety-behavioral-intentions",
    "href": "lectures/03-lecture-slides.html#comparing-the-effects-of-tech-anxiety-behavioral-intentions",
    "title": "An Introduction to Multiple Regression",
    "section": "Comparing the Effects of Tech Anxiety & Behavioral Intentions",
    "text": "Comparing the Effects of Tech Anxiety & Behavioral Intentions\nIf we rescale the technological anxiety scale so that instead of going from 1 to 7 by 1 unit intervals it goes from 0 to .06 by .01 intervals, then we get the following results:\n\n\n\n# A tibble: 7 × 2\n  tech_anx tech_anx_rescale\n     &lt;dbl&gt;            &lt;dbl&gt;\n1        1             0   \n2        2             0.01\n3        3             0.02\n4        4             0.03\n5        5             0.04\n6        6             0.05\n7        7             0.06\n\n\n\n\n\nOriginal tech_anx scale:\n\n\n# A tibble: 3 × 5\n  term          estimate    se statistic p.value\n  &lt;chr&gt;            &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)       1.34 0.07      19.1    0    \n2 tech_anx         -0.01 0.012     -1.13   0.258\n3 beh_intent_ai     0.57 0.01      56.3    0    \n\n\n\nRescaled tech_anx scale:\n\n\n# A tibble: 3 × 5\n  term             estimate    se statistic p.value\n  &lt;chr&gt;               &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)          1.33 0.061     21.8    0    \n2 tech_anx_rescale    -1.38 1.22      -1.13   0.258\n3 beh_intent_ai        0.57 0.01      56.3    0"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#rescale-technological-anxiety",
    "href": "lectures/03-lecture-slides.html#rescale-technological-anxiety",
    "title": "An Introduction to Multiple Regression",
    "section": "Rescale Technological Anxiety",
    "text": "Rescale Technological Anxiety\nIf we rescale the technological anxiety scale so that instead of going from 1 to 7 it goes from 0 to .06 by .01, then we get the following results:\n\n\n# A tibble: 7 × 2\n  tech_anx tech_anx_rescale\n     &lt;dbl&gt;            &lt;dbl&gt;\n1        1             0   \n2        2             0.01\n3        3             0.02\n4        4             0.03\n5        5             0.04\n6        6             0.05\n7        7             0.06\n\n\n\n\nOriginal tech_anx scale:\n\n\n\n\n\nterm\nestimate\nse\nt.stat\np.value\n\n\n\n\n(Intercept)\n1.34\n0.07\n19.09\n0.00\n\n\ntech_anx\n-0.01\n0.01\n-1.13\n0.26\n\n\nbeh_intent_ai\n0.57\n0.01\n56.35\n0.00\n\n\n\n\n\n\nRescaled tech_anx scale:\n\n\n\n\n\nterm\nestimate\nse\nt.stat\np.value\n\n\n\n\n(Intercept)\n1.33\n0.06\n21.80\n0.00\n\n\ntech_anx_rescale\n-1.38\n1.22\n-1.13\n0.26\n\n\nbeh_intent_ai\n0.57\n0.01\n56.35\n0.00"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#standardizing-our-predictor-variables",
    "href": "lectures/03-lecture-slides.html#standardizing-our-predictor-variables",
    "title": "An Introduction to Multiple Regression",
    "section": "Standardizing Our Predictor Variables",
    "text": "Standardizing Our Predictor Variables\nIf we standardize our predictor variables and our outcome variable, we get the following results:\n\n\n\n\n# A tibble: 7 × 3\n  original_scale tech_anx_scale beh_intent_ai_scale\n           &lt;dbl&gt;          &lt;dbl&gt;               &lt;dbl&gt;\n1              1        -1.52                -2.06 \n2              2        -0.774               -1.45 \n3              3        -0.0263              -0.829\n4              4         0.722               -0.211\n5              5         1.47                 0.407\n6              6         2.22                 1.02 \n7              7         2.97                 1.64 \n\n\n\n\n\n\n\n\nterm\nestimate\nse\np.value\n\n\n\n\n(Intercept)\n0.00\n0.01\n1.00\n\n\ntech_anx_scale\n-0.01\n0.01\n0.26\n\n\nbeh_intent_ai_scale\n0.66\n0.01\n0.00"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#understanding-the-squared-semipartial-correlation",
    "href": "lectures/03-lecture-slides.html#understanding-the-squared-semipartial-correlation",
    "title": "An Introduction to Multiple Regression",
    "section": "Understanding the Squared Semipartial Correlation",
    "text": "Understanding the Squared Semipartial Correlation\nAnother way to compare relative effects is the squared semipartial correlation: \\(sr_{j}^2\\).\nThe squared semipartial correlation tells us how much unique variance a predictor variable explains in an outcome variable when controlling for all the other predictor variables.\n\\[sr^2_p=R^2_{Y. X_1...X_p}-R^2_{Y.X_1...X_{p-1}}\\]"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#semipartial-correlation-with-our-ai-data",
    "href": "lectures/03-lecture-slides.html#semipartial-correlation-with-our-ai-data",
    "title": "An Introduction to Multiple Regression",
    "section": "Semipartial Correlation with our AI Data",
    "text": "Semipartial Correlation with our AI Data\n\n\n\n\n\nMetric\nValue\nR-Squared % Change\n\n\n\n\nOverall R-Squared\n0.450\n0\n\n\nSemipartial r-squared: Behvioral Intent.\n0.351\n355\n\n\nSemipartial r-squared: Tech. Anx.\n0.000\n0"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#understanding-statistical-control",
    "href": "lectures/03-lecture-slides.html#understanding-statistical-control",
    "title": "An Introduction to Multiple Regression",
    "section": "Understanding Statistical Control",
    "text": "Understanding Statistical Control\nIf we wanted to estimate the relationship that intention to use the AI tool has on the frequency of AI tool use, while controlling (adjusting) for the level of technological anxiety, we could create multiple new data sets from our original dataset where each new dataset had a fixed level of technological anxiety (e.g. one dataset, where tech_anxiety == 1).\nWe could then estimate the relationship between intentions to use the AI tool and frequency of tool use to each dataset. The average of those estimated slopes (relationships) would be nearly equivalent to the partial coefficient estimated from a multiple regression model that included beh_intent_ai and tech_anxiety."
  },
  {
    "objectID": "lectures/03-lecture-slides.html#should-you-use-the-standardized-partial-coefficient-or-squared-semipartial-correlation",
    "href": "lectures/03-lecture-slides.html#should-you-use-the-standardized-partial-coefficient-or-squared-semipartial-correlation",
    "title": "An Introduction to Multiple Regression",
    "section": "Should You Use the Standardized Partial Coefficient or Squared Semipartial Correlation?",
    "text": "Should You Use the Standardized Partial Coefficient or Squared Semipartial Correlation?\nPersonally, I do not think you can go wrong either way, but the best practice would be to use the squared semipartial correlation to compare the relative importance of predictor variables."
  },
  {
    "objectID": "lectures/03-lecture-page.html",
    "href": "lectures/03-lecture-page.html",
    "title": "Quantitative Analysis 1",
    "section": "",
    "text": "Next Week’s Materials »"
  },
  {
    "objectID": "lectures/03-lecture-page.html#lecture-introduction-to-simple-regression",
    "href": "lectures/03-lecture-page.html#lecture-introduction-to-simple-regression",
    "title": "Quantitative Analysis 1",
    "section": "Lecture: Introduction to Simple Regression",
    "text": "Lecture: Introduction to Simple Regression\n\n\nTo download a pdf version of these slides, click here.\nTo download the R script that follows the R portion of the lecture, click here."
  },
  {
    "objectID": "lectures/03-lecture-page.html#lecture-introduction-to-multiple-regression",
    "href": "lectures/03-lecture-page.html#lecture-introduction-to-multiple-regression",
    "title": "Quantitative Analysis 1",
    "section": "Lecture: Introduction to Multiple Regression",
    "text": "Lecture: Introduction to Multiple Regression\n\n\nTo download a pdf version of these slides, click here.\nTo download the R script that follows the R portion of the lecture, click here."
  },
  {
    "objectID": "lectures/03-lecture-slides.html#model-1-technological-anxiety-and-ai-use",
    "href": "lectures/03-lecture-slides.html#model-1-technological-anxiety-and-ai-use",
    "title": "An Introduction to Multiple Regression",
    "section": "Model 1: Technological Anxiety and AI Use",
    "text": "Model 1: Technological Anxiety and AI Use\nShould we conclude that technological anxiety is significantly and negatively related to frequency of AI tool use?\n\nmodel_1 &lt;- lm(freq_use_ai ~ tech_anx, data = data_ai)\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n4.740\n0.046\n102.682\n0\n\n\ntech_anx\n-0.322\n0.014\n-23.144\n0"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#model-2-behavioral-intentions-and-ai-use",
    "href": "lectures/03-lecture-slides.html#model-2-behavioral-intentions-and-ai-use",
    "title": "An Introduction to Multiple Regression",
    "section": "Model 2: Behavioral Intentions and AI Use",
    "text": "Model 2: Behavioral Intentions and AI Use\nShould we conclude that intention to use the AI tool is significantly and positively related to frequency of AI tool use?\n\nmodel_2 &lt;- lm(freq_use_ai ~ beh_intent_ai, data = data_ai)\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n1.278\n0.042\n30.665\n0\n\n\nbeh_intent_ai\n0.572\n0.009\n63.636\n0"
  },
  {
    "objectID": "lectures/03-lecture-slides.html#model-3-anxiety-intentions-and-ai-use",
    "href": "lectures/03-lecture-slides.html#model-3-anxiety-intentions-and-ai-use",
    "title": "An Introduction to Multiple Regression",
    "section": "Model 3: Anxiety & Intentions and AI Use",
    "text": "Model 3: Anxiety & Intentions and AI Use\nHow do our previous conclusions change? What happened to technological anxiety?\n\nmodel_3 &lt;- lm(freq_use_ai ~ tech_anx + beh_intent_ai, data = data_ai)\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n1.342\n0.070\n19.090\n0.000\n\n\ntech_anx\n-0.014\n0.012\n-1.132\n0.258\n\n\nbeh_intent_ai\n0.567\n0.010\n56.349\n0.000"
  },
  {
    "objectID": "assignments/01-assignment-page.html",
    "href": "assignments/01-assignment-page.html",
    "title": "\nAssignment 1: Simple & Multiple Regression in R\n",
    "section": "",
    "text": "Assignment 1: Simple & Multiple Regression in R\n\n\nInstructions\n\nIn this assignment you will practice some of the statistical methods we have talked about so far: bivariate correlation, simple regression, and multiple regression. This assignment will be due on or before 09-29-2023.\nFor R users:\n\nDownload the assignment R script, which can be found below.\nOpen the R script in RStudio.\nBegin the assignment.\nFor the questions that require a written answer, save your answer either as a comment directly in the R script or in a separate Word document (or something similar).\nUpload your completed R script and any other files you created, such as a Word document, into the class Brightspace.\n\nFor non-R users:\n\nDownload the assignment R script, which can be found below.\nDownload the assignment dataset.\nOpen the R script in a generic text editor as it contains the assignment questions.\nFor the questions that require a written answer, save your answers in a separate Word document (or something similar).\nSave the script you used to complete the assignment and its subsequent output as a Word or text file.\nUpload your script, output, and written answers into the class Brightspace.\n\nIf you run into any issues, please email me.\n\nMaterials\n\nAssignment R Script: Download\nAssignment Data: Download\n\nData Description\n\nYour organization is planning a wide-scale release of a new generative AI chatbot designed to support your organization’s sales force. Among other features the AI chatbot can find answers to the customer’s question, identifying new promotions tailored to the customer, and generally help the sales representative meet the customer’s demands.\nYour organization, however, is not sure if the sales representatives will begin using the new AI technology, so they have asked you design a survey study on a sample of the organization’s sales representatives to understand what predicts a sale’s representative’s intentions to use the new AI tool. You ultimately decide to measure three variables with your survey: The AI tool’s perceived ease of use (perceived_ease_use), the AI tool’s perceived usefulness for the sales job (perceived_useful), and the sales rep’s intentions to use the new AI tool (behavioral_intention).\nThe dataset for this assignment contains the following variables:\n\nemployee_id: A unique 6-digit employee identifier.\nperceived_ease_use: Responses to the question: “I find the AI tool easy to use.”\nperceived_useful: Responses to the question: “I believe the AI tool is useful for my job.”\nbehavioral_intention: Responses to the question: “I plan to use the system in the next month.”\n\nAll three survey questions, perceived_ease_use, perceived_useful, and behavioral_intention, are on a seven point agreement response scale:\n\nCompletely Disagree\nStrongly Disagree\nDisagree\nNeither Disagree, Nor Agree\nAgree\nStrongly Agree\nCompletely Agree"
  },
  {
    "objectID": "lectures/04-lecture-page.html",
    "href": "lectures/04-lecture-page.html",
    "title": "Quantitative Analysis 1",
    "section": "",
    "text": "Next Week’s Materials »"
  },
  {
    "objectID": "lectures/04-lecture-page.html#lecture-inference-for-linear-regression",
    "href": "lectures/04-lecture-page.html#lecture-inference-for-linear-regression",
    "title": "Quantitative Analysis 1",
    "section": "Lecture: Inference for Linear Regression",
    "text": "Lecture: Inference for Linear Regression\n\n\nTo download a pdf version of these slides, click here.\nTo download the R script that follows the R portion of the lecture, click here."
  },
  {
    "objectID": "lectures/04-lecture-slides.html",
    "href": "lectures/04-lecture-slides.html",
    "title": "Review of Statistical Concepts",
    "section": "",
    "text": "Review (5 PM - 6:15 PM ET)\nBreak for 10 minutes\nR Coding (6:25 PM - 7:00 PM ET)"
  },
  {
    "objectID": "lectures/04-lecture-slides.html#schedule-for-today",
    "href": "lectures/04-lecture-slides.html#schedule-for-today",
    "title": "Review of Statistical Concepts",
    "section": "Schedule for Today",
    "text": "Schedule for Today\n\nReview (5 PM - 6:15 PM ET)\nBreak for 10 minutes\nR Coding (6:25 PM - 7:00 PM ET)"
  },
  {
    "objectID": "lectures/04-lecture-slides.html#overview",
    "href": "lectures/04-lecture-slides.html#overview",
    "title": "Review of Statistical Concepts",
    "section": "Overview",
    "text": "Overview\n\nA review of what we have covered to date\nLive R coding"
  },
  {
    "objectID": "lectures/04-lecture-slides.html#goals",
    "href": "lectures/04-lecture-slides.html#goals",
    "title": "Review of Statistical Concepts",
    "section": "Goals",
    "text": "Goals\n\nStrengthen your understanding of the concepts we have covered up until now\nGet some experience coding in R"
  },
  {
    "objectID": "lectures/04-lecture-slides.html#data-generating-process",
    "href": "lectures/04-lecture-slides.html#data-generating-process",
    "title": "Inference for Linear Regression",
    "section": "Data Generating Process",
    "text": "Data Generating Process"
  },
  {
    "objectID": "lectures/04-lecture-slides.html#inferential-statistics",
    "href": "lectures/04-lecture-slides.html#inferential-statistics",
    "title": "Inference for Linear Regression",
    "section": "Inferential Statistics",
    "text": "Inferential Statistics\nUsually, when you analyze your data you want to generalize the results from your specific dataset to a broader population or more general process. This is called statistical inference."
  },
  {
    "objectID": "lectures/04-lecture-slides.html#statistic-vs-population-parameter",
    "href": "lectures/04-lecture-slides.html#statistic-vs-population-parameter",
    "title": "Inference for Linear Regression",
    "section": "Statistic vs Population Parameter",
    "text": "Statistic vs Population Parameter\nPopulation parameters (or parameters) are numerical summaries of our population.\nStatistics are estimates of these parameters calculated from data sampled from this population.\nUsually, we do not have access to our full population of interest, so we sample our data from it and learn about its characteristics (parameters) through the statistics we compute from our sampled data."
  },
  {
    "objectID": "lectures/04-lecture-slides.html#statistic-vs-population-parameter-1",
    "href": "lectures/04-lecture-slides.html#statistic-vs-population-parameter-1",
    "title": "Inference for Linear Regression",
    "section": "Statistic vs Population Parameter",
    "text": "Statistic vs Population Parameter"
  },
  {
    "objectID": "lectures/04-lecture-slides.html#population-regression-model-dgp",
    "href": "lectures/04-lecture-slides.html#population-regression-model-dgp",
    "title": "Inference for Linear Regression",
    "section": "Population Regression Model: DGP",
    "text": "Population Regression Model: DGP\nThe \\(\\beta\\) coefficients and \\(\\epsilon\\) below are population parameters or just parameters. We will never truly know their value, but we can estimate them.\n\\[Y = \\beta_0+\\beta_1X_{1}+\\beta_2X_2+...+\\beta_pX_p + \\epsilon\\]"
  },
  {
    "objectID": "lectures/04-lecture-slides.html#estimated-regression-model",
    "href": "lectures/04-lecture-slides.html#estimated-regression-model",
    "title": "Inference for Linear Regression",
    "section": "Estimated Regression Model",
    "text": "Estimated Regression Model\nWe can estimate the regression model using our data. The estimated regression coefficients (slopes and intercepts) are statistics.\nIn simple regression, this looks like:\n\\[\\hat{\\beta}_1=\\frac{\\Sigma(X - \\overline{X})(Y-\\overline{Y})}{\\Sigma(X-\\overline{X})^2}\\]\n\\[\\hat{\\beta}_0=\\overline{Y}-\\hat{\\beta}_1\\overline{X}\\]"
  },
  {
    "objectID": "lectures/04-lecture-slides.html#using-our-estimated-model-to-infer-the-dgp",
    "href": "lectures/04-lecture-slides.html#using-our-estimated-model-to-infer-the-dgp",
    "title": "Inference for Linear Regression",
    "section": "Using Our Estimated Model to Infer the DGP",
    "text": "Using Our Estimated Model to Infer the DGP\n\nSignificance Tests\nCIs"
  },
  {
    "objectID": "lectures/04-lecture-slides.html#four-components-of-a-sig-test-ci",
    "href": "lectures/04-lecture-slides.html#four-components-of-a-sig-test-ci",
    "title": "Inference for Linear Regression",
    "section": "Four Components of a Sig Test & CI",
    "text": "Four Components of a Sig Test & CI\n\nAssumptions\nHypotheses (directional or non-directional)\nTest Statistic\nP-Value (for significance tests)"
  },
  {
    "objectID": "lectures/04-lecture-slides.html#assumptions",
    "href": "lectures/04-lecture-slides.html#assumptions",
    "title": "Inference for Linear Regression",
    "section": "Assumptions",
    "text": "Assumptions\n\nValidity\nRepresentativeness\nLinearity\nIndependence\nHeteroscedasticity\nNormality"
  },
  {
    "objectID": "lectures/04-lecture-slides.html#validity",
    "href": "lectures/04-lecture-slides.html#validity",
    "title": "Inference for Linear Regression",
    "section": "Validity",
    "text": "Validity"
  },
  {
    "objectID": "lectures/04-lecture-slides.html#representativeness",
    "href": "lectures/04-lecture-slides.html#representativeness",
    "title": "Inference for Linear Regression",
    "section": "Representativeness",
    "text": "Representativeness"
  },
  {
    "objectID": "lectures/04-lecture-slides.html#linearity",
    "href": "lectures/04-lecture-slides.html#linearity",
    "title": "Inference for Linear Regression",
    "section": "Linearity",
    "text": "Linearity"
  },
  {
    "objectID": "lectures/04-lecture-slides.html#independence",
    "href": "lectures/04-lecture-slides.html#independence",
    "title": "Inference for Linear Regression",
    "section": "Independence",
    "text": "Independence"
  },
  {
    "objectID": "lectures/04-lecture-slides.html#heteroscedasticity",
    "href": "lectures/04-lecture-slides.html#heteroscedasticity",
    "title": "Inference for Linear Regression",
    "section": "Heteroscedasticity",
    "text": "Heteroscedasticity"
  },
  {
    "objectID": "lectures/04-lecture-slides.html#normality",
    "href": "lectures/04-lecture-slides.html#normality",
    "title": "Inference for Linear Regression",
    "section": "Normality",
    "text": "Normality"
  },
  {
    "objectID": "lectures/04-lecture-slides.html#setting-up-hypotheses",
    "href": "lectures/04-lecture-slides.html#setting-up-hypotheses",
    "title": "Inference for Linear Regression",
    "section": "Setting Up Hypotheses",
    "text": "Setting Up Hypotheses"
  },
  {
    "objectID": "lectures/04-lecture-slides.html#what-is-a-test-statistic",
    "href": "lectures/04-lecture-slides.html#what-is-a-test-statistic",
    "title": "Inference for Linear Regression",
    "section": "What is a Test Statistic?",
    "text": "What is a Test Statistic?\nThe test statistic is a number that tells us how many standard errors our estimate falls from the parameter value specified by the null hypothesis. Generally, a test statistic has three pieces:\n\nThe estimate being tested (our estimated regression coefficient)\nThe value specified by the null hypothesis (almost always 0)\nThe standard error of the estimate"
  },
  {
    "objectID": "lectures/04-lecture-slides.html#sampling-variance-standard-error",
    "href": "lectures/04-lecture-slides.html#sampling-variance-standard-error",
    "title": "Inference for Linear Regression",
    "section": "Sampling Variance & Standard Error",
    "text": "Sampling Variance & Standard Error"
  },
  {
    "objectID": "lectures/04-lecture-slides.html#standard-error-of-a-partial-regression-coefficient",
    "href": "lectures/04-lecture-slides.html#standard-error-of-a-partial-regression-coefficient",
    "title": "Inference for Linear Regression",
    "section": "Standard Error of a Partial Regression Coefficient",
    "text": "Standard Error of a Partial Regression Coefficient\nThe standard error of a partial regression coefficient tells how close our estimated coefficient is to the true value of that coefficient. The SE is affected by four different components:\n\nAn estimate of the overall amount of modeling error: \\(\\hat{\\sigma}^2_{error}\\)\nThe variance of the predictor variable: \\(Var(X_j)\\)\nThe overall sample size: \\(N\\)\nThe proportion of variance in the predictor variable that is explained by the other predictor variables: \\(R^2_j\\)"
  },
  {
    "objectID": "lectures/04-lecture-slides.html#testing-a-t-statistic",
    "href": "lectures/04-lecture-slides.html#testing-a-t-statistic",
    "title": "Inference for Linear Regression",
    "section": "Testing a T Statistic",
    "text": "Testing a T Statistic"
  },
  {
    "objectID": "lectures/04-lecture-slides.html#degrees-of-freedom",
    "href": "lectures/04-lecture-slides.html#degrees-of-freedom",
    "title": "Inference for Linear Regression",
    "section": "Degrees of Freedom",
    "text": "Degrees of Freedom"
  },
  {
    "objectID": "lectures/04-lecture-slides.html#students-t-distribution-degrees-of-freedom",
    "href": "lectures/04-lecture-slides.html#students-t-distribution-degrees-of-freedom",
    "title": "Inference for Linear Regression",
    "section": "Student’s T Distribution & Degrees of Freedom",
    "text": "Student’s T Distribution & Degrees of Freedom"
  },
  {
    "objectID": "lectures/04-lecture-slides.html#p-value-for-a-t-statistic",
    "href": "lectures/04-lecture-slides.html#p-value-for-a-t-statistic",
    "title": "Inference for Linear Regression",
    "section": "P-Value for a T-Statistic",
    "text": "P-Value for a T-Statistic"
  },
  {
    "objectID": "lectures/04-lecture-slides.html#testing-a-set-of-regression-coefficients",
    "href": "lectures/04-lecture-slides.html#testing-a-set-of-regression-coefficients",
    "title": "Inference for Linear Regression",
    "section": "Testing a Set of Regression Coefficients",
    "text": "Testing a Set of Regression Coefficients\nF test"
  },
  {
    "objectID": "lectures/04-lecture-slides.html#anova-table",
    "href": "lectures/04-lecture-slides.html#anova-table",
    "title": "Inference for Linear Regression",
    "section": "ANOVA Table",
    "text": "ANOVA Table"
  },
  {
    "objectID": "lectures/04-lecture-slides.html#f-distribution",
    "href": "lectures/04-lecture-slides.html#f-distribution",
    "title": "Inference for Linear Regression",
    "section": "F Distribution",
    "text": "F Distribution"
  },
  {
    "objectID": "lectures/04-lecture-slides.html#p-value",
    "href": "lectures/04-lecture-slides.html#p-value",
    "title": "Inference for Linear Regression",
    "section": "P-value",
    "text": "P-value"
  },
  {
    "objectID": "lectures/04-lecture-slides.html#inference-about-population-or-process",
    "href": "lectures/04-lecture-slides.html#inference-about-population-or-process",
    "title": "Inference for Linear Regression",
    "section": "Inference about Population or Process?",
    "text": "Inference about Population or Process?\nWhen we make statistical inferences we are generally making inferences to one of two “things”:\n\nPopulation such as all eligible US voters\nData Generating Process which is the population model you believe generated your data"
  },
  {
    "objectID": "lectures/04-lecture-slides.html#data-generating-process-your-collected-data",
    "href": "lectures/04-lecture-slides.html#data-generating-process-your-collected-data",
    "title": "Inference for Linear Regression",
    "section": "Data Generating Process & Your Collected Data",
    "text": "Data Generating Process & Your Collected Data\nIn social scientific and behavioral research, it is usually assumed that there is a data generating process that produces the data you have collected. The specific data you collected is useful because it allows us to make an inference about the underlying data genearting process."
  },
  {
    "objectID": "lectures/04-lecture-slides.html#our-data-generating-process-the-regression-model",
    "href": "lectures/04-lecture-slides.html#our-data-generating-process-the-regression-model",
    "title": "Inference for Linear Regression",
    "section": "Our Data Generating Process: The Regression Model",
    "text": "Our Data Generating Process: The Regression Model\nIn our course examples, we have been assuming that our data is produced by a process that we can represent as a population linear regression model:\n\\[Y = \\beta_0+\\beta_1X_{1}+\\beta_2X_2+...+\\beta_pX_p + \\epsilon\\]\nThe \\(\\beta\\) coefficients and \\(\\epsilon\\) below are the population parameters or just parameters if our data generating process."
  },
  {
    "objectID": "lectures/04-lecture-slides.html#estimating-our-data-generating-process-with-data",
    "href": "lectures/04-lecture-slides.html#estimating-our-data-generating-process-with-data",
    "title": "Inference for Linear Regression",
    "section": "Estimating Our Data Generating Process with Data",
    "text": "Estimating Our Data Generating Process with Data\nWe can use our data to estimate the data generating process (population regression model in this case):\n\\[Y=\\hat{\\beta}_0+\\hat{\\beta_1}X_1+\\hat{\\beta_2}X_2+...\\hat{\\beta_p}X_p+\\hat{\\epsilon}\\]"
  },
  {
    "objectID": "lectures/04-lecture-slides.html#an-example-of-estimating-simple-regression-coefficients",
    "href": "lectures/04-lecture-slides.html#an-example-of-estimating-simple-regression-coefficients",
    "title": "Inference for Linear Regression",
    "section": "An Example of Estimating Simple Regression Coefficients",
    "text": "An Example of Estimating Simple Regression Coefficients\nIt is easiest to illustrate what we mean when we say “estimating our data generating process” by looking at the equations used to estimate the coefficients in a simple regression model:\n\\[\\hat{\\beta}_1=\\frac{\\Sigma(X - \\overline{X})(Y-\\overline{Y})}{\\Sigma(X-\\overline{X})^2}\\]\n\\[\\hat{\\beta}_0=\\overline{Y}-\\hat{\\beta}_1\\overline{X}\\]\nBoth equations, only use the collected data."
  },
  {
    "objectID": "lectures/04-lecture-slides.html#the-tools-of-statistical-inference",
    "href": "lectures/04-lecture-slides.html#the-tools-of-statistical-inference",
    "title": "Inference for Linear Regression",
    "section": "The Tools of Statistical Inference",
    "text": "The Tools of Statistical Inference\nWe have two inferential tools to help us determine if our data supports our data generating process:\n\nStatistical Significance Tests\nConfidence Intervals"
  },
  {
    "objectID": "lectures/04-lecture-slides.html#the-four-components-of-a-significance-test-confidence-interval",
    "href": "lectures/04-lecture-slides.html#the-four-components-of-a-significance-test-confidence-interval",
    "title": "Inference for Linear Regression",
    "section": "The Four Components of a Significance Test & Confidence Interval",
    "text": "The Four Components of a Significance Test & Confidence Interval\nIn order to ensure our inferential tools work, we need to assess the quality of their four different components:\n\nStatistical Model Assumptions\nHypotheses (directional or non-directional)\nTest Statistic\nP-Value (for significance tests)"
  },
  {
    "objectID": "lectures/04-lecture-slides.html#assumptions-of-linear-regression-analysis",
    "href": "lectures/04-lecture-slides.html#assumptions-of-linear-regression-analysis",
    "title": "Inference for Linear Regression",
    "section": "Assumptions of Linear Regression Analysis",
    "text": "Assumptions of Linear Regression Analysis\nTo ensure that inferences made about our regression coefficients are apporpriate, we have to make six assumptions:\n\nValidity of our Data\nRepresentativeness of our Data\nAdditivity & Linearity\nIndependence of Errors\nEqual Variance of Errors (Homoscedasticity)\nNormality of Errors"
  },
  {
    "objectID": "lectures/04-lecture-slides.html#validity-of-our-data",
    "href": "lectures/04-lecture-slides.html#validity-of-our-data",
    "title": "Inference for Linear Regression",
    "section": "Validity of Our Data",
    "text": "Validity of Our Data\nOne of the most important assumptions we need to make is that our data maps to our research questions. We want to make sure that the data we have collected is a valid representation of the phenomenon in which we are interested!\nThere is a whole field called Psychometrics that is dedicated to issues around the validity of measurements!"
  },
  {
    "objectID": "lectures/04-lecture-slides.html#representativeness-of-our-data",
    "href": "lectures/04-lecture-slides.html#representativeness-of-our-data",
    "title": "Inference for Linear Regression",
    "section": "Representativeness of Our Data",
    "text": "Representativeness of Our Data\nThe data you are estimating your regression model from should be represntative of the population you are making inferences about. This applies even when we are making inferences about a data generating process."
  },
  {
    "objectID": "lectures/04-lecture-slides.html#additivity-linearity",
    "href": "lectures/04-lecture-slides.html#additivity-linearity",
    "title": "Inference for Linear Regression",
    "section": "Additivity & Linearity",
    "text": "Additivity & Linearity\nAdditivity and Linearity is the assumption that the conditional mean of the outcome variable is a linear function of the diffent predictor variables:\n\\[E[Y_{\\text{AI Freq.}}|X_{\\text{Beh. Intent.}}] = \\beta_0 + \\beta_1X_{\\text{Beh. Intent.}}\\]\nVilolations of this assumption happen when there is a nonlinear relationship between your outcome variable and at least one of your predictor variables."
  },
  {
    "objectID": "lectures/04-lecture-slides.html#independence-of-errors",
    "href": "lectures/04-lecture-slides.html#independence-of-errors",
    "title": "Inference for Linear Regression",
    "section": "Independence of Errors",
    "text": "Independence of Errors\nThe assumption of independent errors (homoscedasticity) means that once we control for all the predictor variables, then there should be no relationship among our model residuals (the part of our outcome variable that is not captured by our model)."
  },
  {
    "objectID": "lectures/04-lecture-slides.html#equal-variance-of-errors",
    "href": "lectures/04-lecture-slides.html#equal-variance-of-errors",
    "title": "Inference for Linear Regression",
    "section": "Equal Variance of Errors",
    "text": "Equal Variance of Errors"
  },
  {
    "objectID": "lectures/04-lecture-slides.html#normality-of-errors",
    "href": "lectures/04-lecture-slides.html#normality-of-errors",
    "title": "Inference for Linear Regression",
    "section": "Normality of Errors",
    "text": "Normality of Errors\nThe normality of errors assumption means that our errors follow a normal probability distribution with a mean of zero and a variance equal to \\(\\sigma^2_{e}\\).\nAnother way to state this assumption is that the distribution of your outcome variable when conditioned on all of your predictor variables follows normal probability distribution.\nIt is important to note that the distribution of your outcome variable without conditioning on your predictor variables need not follow a normal probability distribution!"
  },
  {
    "objectID": "lectures/04-lecture-slides.html#a-focus-on-errors",
    "href": "lectures/04-lecture-slides.html#a-focus-on-errors",
    "title": "Inference for Linear Regression",
    "section": "A Focus on Errors",
    "text": "A Focus on Errors"
  },
  {
    "objectID": "lectures/04-lecture-slides.html#additivity-linearity-a-graphical-view",
    "href": "lectures/04-lecture-slides.html#additivity-linearity-a-graphical-view",
    "title": "Inference for Linear Regression",
    "section": "Additivity & Linearity: A Graphical View",
    "text": "Additivity & Linearity: A Graphical View"
  },
  {
    "objectID": "lectures/04-lecture-slides.html#independence-of-errors-a-graphical-view",
    "href": "lectures/04-lecture-slides.html#independence-of-errors-a-graphical-view",
    "title": "Inference for Linear Regression",
    "section": "Independence of Errors: A Graphical View",
    "text": "Independence of Errors: A Graphical View"
  },
  {
    "objectID": "lectures/04-lecture-slides.html#equal-variance-of-errors-homoscedasticity",
    "href": "lectures/04-lecture-slides.html#equal-variance-of-errors-homoscedasticity",
    "title": "Inference for Linear Regression",
    "section": "Equal Variance of Errors (Homoscedasticity)",
    "text": "Equal Variance of Errors (Homoscedasticity)\nThe assumption of equal error variances means that the variance of our errors (residuals) is constant across all values of our predictor variables. A different way to say this is that our errors (residuals) are unrelated to to our predictor variables."
  },
  {
    "objectID": "lectures/04-lecture-slides.html#equal-variance-of-errors-a-graphical-view",
    "href": "lectures/04-lecture-slides.html#equal-variance-of-errors-a-graphical-view",
    "title": "Inference for Linear Regression",
    "section": "Equal Variance of Errors: A Graphical View",
    "text": "Equal Variance of Errors: A Graphical View"
  },
  {
    "objectID": "lectures/04-lecture-slides.html#the-probability-model-implied-by-our-assumptions",
    "href": "lectures/04-lecture-slides.html#the-probability-model-implied-by-our-assumptions",
    "title": "Inference for Linear Regression",
    "section": "The Probability Model Implied By Our Assumptions",
    "text": "The Probability Model Implied By Our Assumptions\nWhen taken all together, our linear regression assumptions imply the following probabilistic model for the conditional distribution of our outcome variable:\n\\[Y_{i}|X_{i} \\sim N(\\beta_0 + \\beta_1X_{\\text{Beh. Intent.}},\\sigma^2_e )\\]\n\\[\\epsilon_i\\sim N(0, \\sigma^2_e)\\]"
  },
  {
    "objectID": "lectures/04-lecture-slides.html#normality-of-errors-a-graphical-view",
    "href": "lectures/04-lecture-slides.html#normality-of-errors-a-graphical-view",
    "title": "Inference for Linear Regression",
    "section": "Normality of Errors: A Graphical View",
    "text": "Normality of Errors: A Graphical View"
  },
  {
    "objectID": "lectures/04-lecture-slides.html#hypotheses-and-the-data-generating-process",
    "href": "lectures/04-lecture-slides.html#hypotheses-and-the-data-generating-process",
    "title": "Inference for Linear Regression",
    "section": "Hypotheses and the Data Generating Process",
    "text": "Hypotheses and the Data Generating Process\nYou can think of a hypothesis as a statement about the data generating process. This statement needs to be made before your analyses!\nTo setup a significance test, you will need to develop two kinds of hypotheses:\n\nNull Hypothesis (\\(H_0\\)): Specifies that a parameter in your DGP takes on a specific value (usually 0).\nAlternative Hypothesis (\\(H_a\\)): Specifies that a parameter takes on some alternative range of values (e.g. all positive values)."
  },
  {
    "objectID": "lectures/04-lecture-slides.html#our-working-example-for-today",
    "href": "lectures/04-lecture-slides.html#our-working-example-for-today",
    "title": "Inference for Linear Regression",
    "section": "Our Working Example for Today",
    "text": "Our Working Example for Today\nTo keep things simple, our assumed data generating process will be:\n\\[Y_{\\text{Freq. AI Use}}=\\beta_0+\\beta_1X_{\\text{Beh. Intent.}} + \\epsilon\\]"
  },
  {
    "objectID": "lectures/04-lecture-slides.html#estimating-the-regression-coefficients-of-our-dgp",
    "href": "lectures/04-lecture-slides.html#estimating-the-regression-coefficients-of-our-dgp",
    "title": "Inference for Linear Regression",
    "section": "Estimating the Regression Coefficients of our DGP",
    "text": "Estimating the Regression Coefficients of our DGP\nIt is easiest to illustrate what we mean when we say “estimating our data generating process” by looking at the equations used to estimate the coefficients in a simple regression model:\n\\[\\hat{\\beta}_1=\\frac{\\Sigma(X - \\overline{X})(Y-\\overline{Y})}{\\Sigma(X-\\overline{X})^2}\\]\n\\[\\hat{\\beta}_0=\\overline{Y}-\\hat{\\beta}_1\\overline{X}\\]\nBoth equations, only use the collected data."
  },
  {
    "objectID": "lectures/04-lecture-slides.html#computing-model-errors",
    "href": "lectures/04-lecture-slides.html#computing-model-errors",
    "title": "Inference for Linear Regression",
    "section": "Computing Model Errors",
    "text": "Computing Model Errors\nWe will never observe the true errors (residuals) of our regression model, \\(\\epsilon\\). For each observation in our data, we have to estimate \\(\\epsilon\\) by computing:\n\\[Y_{\\text{Freq. AI Use}}-(\\hat{\\beta}_0+\\hat{\\beta}_1X_{\\text{Beh. Intent.}})\\]\nThese observed errors are commonly referred to as residuals. The bulk of the linear regression assumptions are about the distribution of the unobserved error!"
  },
  {
    "objectID": "lectures/04-lecture-slides.html#estimating-the-error-variance",
    "href": "lectures/04-lecture-slides.html#estimating-the-error-variance",
    "title": "Inference for Linear Regression",
    "section": "Estimating the Error Variance",
    "text": "Estimating the Error Variance\nBecause we will never know the true model errors, we have to use the errors we computed from our regression model to estimate things like the variance and standard deviation of error:\n\\[Var(Y - \\hat{Y})=\\sigma^2_{error}\\]"
  },
  {
    "objectID": "lectures/04-lecture-slides.html#setting-up-the-null-hypothesis",
    "href": "lectures/04-lecture-slides.html#setting-up-the-null-hypothesis",
    "title": "Inference for Linear Regression",
    "section": "Setting Up the Null Hypothesis",
    "text": "Setting Up the Null Hypothesis\nThe null hypothesis is usually specifies that the parameter you DGP takes on is 0:\n\n\\(H_0\\): There is no relationship between behavioral intentions to use the AI tool and the frequency with which the tool is used (\\(\\beta_1=0\\)).\n\nAlthough it is incredibly rare, the null hypothesis can specify values other than 0:\n\n\\(H_0\\): The relationship between behavioral intentions to use the AI tool and the frequency with which the tool is used is equal to -.50 (\\(\\beta_1=-.50\\))."
  },
  {
    "objectID": "lectures/04-lecture-slides.html#setting-up-the-alternative-hypothesis",
    "href": "lectures/04-lecture-slides.html#setting-up-the-alternative-hypothesis",
    "title": "Inference for Linear Regression",
    "section": "Setting Up the Alternative Hypothesis",
    "text": "Setting Up the Alternative Hypothesis\nThe alternative hypothesis can be a directional or non-directional statement:\n\nDirectional \\(H_a\\): The relationship between behavioral intentions to use the AI tool and the frequency with which the tool is used is positive (\\(\\beta_1 &gt; 0\\)).\nNon-Directional \\(H_a\\): There is a relationship between behavioral intentions to use the AI tool and the frequecny with which the tool is used (\\(\\beta_1 \\neq 0\\)).\n\nTo balance the risk from making a more specific statement (all positive numbers), the directional hypothesis has more statistical power than a non-directional hypothesis. That is, a directional hypothesis is more likely to determine that a weak effect is significant than a non-directional hypothesis."
  },
  {
    "objectID": "lectures/04-lecture-slides.html#t-test-the-test-statistic-for-linear-regression",
    "href": "lectures/04-lecture-slides.html#t-test-the-test-statistic-for-linear-regression",
    "title": "Inference for Linear Regression",
    "section": "T-Test: The Test Statistic for Linear Regression",
    "text": "T-Test: The Test Statistic for Linear Regression\nWe use a T-test to test the significance of any single regression coefficient:\n\\[t = \\frac{\\hat{\\beta}_1-\\beta_{\\text{Null Hyp}}}{SE(\\hat{\\beta}_1)}\\]"
  },
  {
    "objectID": "lectures/04-lecture-slides.html#sampling-distribution-standard-error-a-redux-from-lecture-1",
    "href": "lectures/04-lecture-slides.html#sampling-distribution-standard-error-a-redux-from-lecture-1",
    "title": "Inference for Linear Regression",
    "section": "Sampling Distribution & Standard Error: A Redux from Lecture 1",
    "text": "Sampling Distribution & Standard Error: A Redux from Lecture 1\nRemember the sampling distribution is a hypothetical distribution of all our possible \\(\\hat{\\beta}_1\\) estimates and the standard error is the standard deviation of that distribution.\nImagine collecting 1,000,000 independent, representative samples where we measure behavioral intentions to use the AI tool and frequency of tool use. Then we fit 1,000,000 simple regression models and save the estimated value of \\(\\hat{\\beta}_1\\) from each model. The standard error of \\(\\hat{\\beta}_1\\) is equal to the standard devation of those 1,000,000 \\(\\hat{\\beta}_1\\) values."
  },
  {
    "objectID": "lectures/04-lecture-slides.html#estimating-model-errors",
    "href": "lectures/04-lecture-slides.html#estimating-model-errors",
    "title": "Inference for Linear Regression",
    "section": "Estimating Model Errors",
    "text": "Estimating Model Errors\nWe will never observe the true errors (residuals) of our regression model, \\(\\epsilon\\). For each observation in our data, we have to estimate \\(\\epsilon\\) by computing:\n\\[Y_{\\text{Freq. AI Use}}-(\\hat{\\beta}_0+\\hat{\\beta}_1X_{\\text{Beh. Intent.}})\\]\nThese observed errors are commonly referred to as residuals. The bulk of the linear regression assumptions are about the distribution of the unobserved error!"
  },
  {
    "objectID": "lectures/04-lecture-slides.html#sampling-distribution-standard-error",
    "href": "lectures/04-lecture-slides.html#sampling-distribution-standard-error",
    "title": "Inference for Linear Regression",
    "section": "Sampling Distribution & Standard Error",
    "text": "Sampling Distribution & Standard Error\nRemember the sampling distribution is a hypothetical distribution of all our possible \\(\\hat{\\beta}_1\\) estimates and the standard error is the standard deviation of that distribution."
  },
  {
    "objectID": "lectures/04-lecture-slides.html#formula-for-the-partial-regression-coefficient-se",
    "href": "lectures/04-lecture-slides.html#formula-for-the-partial-regression-coefficient-se",
    "title": "Inference for Linear Regression",
    "section": "Formula for the Partial Regression Coefficient SE",
    "text": "Formula for the Partial Regression Coefficient SE\n\\[SE(\\hat{\\beta}_j)=\\sqrt{\\frac{1}{1-R^2_j}}\\times\\sqrt{\\frac{\\hat{\\sigma}^2_{error}}{N \\times Var(X_j)}}\\]"
  },
  {
    "objectID": "lectures/04-lecture-slides.html#calculating-the-p-value-for-a-t-statistic",
    "href": "lectures/04-lecture-slides.html#calculating-the-p-value-for-a-t-statistic",
    "title": "Inference for Linear Regression",
    "section": "Calculating the P-Value for a t-Statistic",
    "text": "Calculating the P-Value for a t-Statistic\nTo test the significance of a t-statistic, we have to use a probability distribution called the Student’s t distribution.\nWe have to use the Student’s t distribution for one very specific reason: we are using an estimate of the model error variance (\\(\\sigma^2_e\\)) because we do not know the true value. If we knew the true value, we could use that in the SE formula and then calculate the significance of the test statistic using a normal distribution."
  },
  {
    "objectID": "lectures/04-lecture-slides.html#students-t-distribution",
    "href": "lectures/04-lecture-slides.html#students-t-distribution",
    "title": "Inference for Linear Regression",
    "section": "Student’s T Distribution",
    "text": "Student’s T Distribution"
  },
  {
    "objectID": "lectures/05-lecture-slides.html#schedule-for-today",
    "href": "lectures/05-lecture-slides.html#schedule-for-today",
    "title": "Inference for Linear Regression",
    "section": "Schedule for Today",
    "text": "Schedule for Today\n\nTalk about stats for ~75 mins (5 PM - 6:15 PM ET)\nBreak for 10 minutes\nFinish up stats for 35 mins (6:25 PM - 7:00 PM ET)"
  },
  {
    "objectID": "lectures/05-lecture-slides.html#overview",
    "href": "lectures/05-lecture-slides.html#overview",
    "title": "Inference for Linear Regression",
    "section": "Overview",
    "text": "Overview\n\nA review of statistical inference\nSetup of our working example\nIntroduction to inference for linear regression"
  },
  {
    "objectID": "lectures/05-lecture-slides.html#goals",
    "href": "lectures/05-lecture-slides.html#goals",
    "title": "Inference for Linear Regression",
    "section": "Goals",
    "text": "Goals\n\nImprove your understanding of statistical inference\nUnderstand the assumptions underlying linear regression and their importance\nUnderstand hypothesis testing under linear regression"
  },
  {
    "objectID": "lectures/05-lecture-slides.html#inferential-statistics",
    "href": "lectures/05-lecture-slides.html#inferential-statistics",
    "title": "Inference for Linear Regression",
    "section": "Inferential Statistics",
    "text": "Inferential Statistics\nUsually, when you analyze your data you want to generalize the results from your specific dataset to a broader population or more general process. This is called statistical inference."
  },
  {
    "objectID": "lectures/05-lecture-slides.html#inference-about-population-or-process",
    "href": "lectures/05-lecture-slides.html#inference-about-population-or-process",
    "title": "Inference for Linear Regression",
    "section": "Inference about Population or Process?",
    "text": "Inference about Population or Process?\nWhen we make statistical inferences we are generally making inferences to one of two “things”:\n\nPopulation such as all eligible US voters\nData Generating Process which is the population model you believe generated your data"
  },
  {
    "objectID": "lectures/05-lecture-slides.html#data-generating-process-your-collected-data",
    "href": "lectures/05-lecture-slides.html#data-generating-process-your-collected-data",
    "title": "Inference for Linear Regression",
    "section": "Data Generating Process & Your Collected Data",
    "text": "Data Generating Process & Your Collected Data\nIn social scientific and behavioral research, it is usually assumed that there is a data generating process that produces the data you have collected. The specific data you collected is useful because it allows us to make an inference about the underlying data generating process."
  },
  {
    "objectID": "lectures/05-lecture-slides.html#our-data-generating-process-the-regression-model",
    "href": "lectures/05-lecture-slides.html#our-data-generating-process-the-regression-model",
    "title": "Inference for Linear Regression",
    "section": "Our Data Generating Process: The Regression Model",
    "text": "Our Data Generating Process: The Regression Model\nIn our course examples, we have been assuming that our data is produced by a process that we can represent as a population linear regression model:\n\\[Y = \\beta_0+\\beta_1X_{1}+\\beta_2X_2+...+\\beta_pX_p + \\epsilon\\]\nThe \\(\\beta\\) coefficients and \\(\\epsilon\\) below are the population parameters or just parameters if our data generating process."
  },
  {
    "objectID": "lectures/05-lecture-slides.html#estimating-our-data-generating-process-with-data",
    "href": "lectures/05-lecture-slides.html#estimating-our-data-generating-process-with-data",
    "title": "Inference for Linear Regression",
    "section": "Estimating Our Data Generating Process with Data",
    "text": "Estimating Our Data Generating Process with Data\nWe can use our data to estimate the data generating process (population regression model in this case):\n\\[Y=\\hat{\\beta}_0+\\hat{\\beta_1}X_1+\\hat{\\beta_2}X_2+...\\hat{\\beta_p}X_p+\\hat{\\epsilon}\\]"
  },
  {
    "objectID": "lectures/05-lecture-slides.html#our-working-example-for-today",
    "href": "lectures/05-lecture-slides.html#our-working-example-for-today",
    "title": "Inference for Linear Regression",
    "section": "Our Working Example for Today",
    "text": "Our Working Example for Today\nTo keep things simple, our assumed data generating process will be:\n\\[Y_{\\text{Freq. AI Use}}=\\beta_0+\\beta_1X_{\\text{Beh. Intent.}} + \\epsilon\\]"
  },
  {
    "objectID": "lectures/05-lecture-slides.html#estimating-the-regression-coefficients-of-our-dgp",
    "href": "lectures/05-lecture-slides.html#estimating-the-regression-coefficients-of-our-dgp",
    "title": "Inference for Linear Regression",
    "section": "Estimating the Regression Coefficients of our DGP",
    "text": "Estimating the Regression Coefficients of our DGP\nIt is easiest to illustrate what we mean when we say “estimating our data generating process” by looking at the equations used to estimate the coefficients in a simple regression model:\n\\[\\hat{\\beta}_1=\\frac{\\Sigma(X - \\overline{X})(Y-\\overline{Y})}{\\Sigma(X-\\overline{X})^2}\\]\n\\[\\hat{\\beta}_0=\\overline{Y}-\\hat{\\beta}_1\\overline{X}\\]\nBoth equations, only use the collected data."
  },
  {
    "objectID": "lectures/05-lecture-slides.html#estimating-model-errors",
    "href": "lectures/05-lecture-slides.html#estimating-model-errors",
    "title": "Inference for Linear Regression",
    "section": "Estimating Model Errors",
    "text": "Estimating Model Errors\nWe will never observe the true errors (residuals) of our regression model, \\(\\epsilon\\). For each observation in our data, we have to estimate \\(\\epsilon\\) by computing:\n\\[Y_{\\text{Freq. AI Use}}-(\\hat{\\beta}_0+\\hat{\\beta}_1X_{\\text{Beh. Intent.}})\\]\nThese observed errors are commonly referred to as residuals. The bulk of the linear regression assumptions are about the distribution of the unobserved error!"
  },
  {
    "objectID": "lectures/05-lecture-slides.html#estimating-the-error-variance",
    "href": "lectures/05-lecture-slides.html#estimating-the-error-variance",
    "title": "Inference for Linear Regression",
    "section": "Estimating the Error Variance",
    "text": "Estimating the Error Variance\nBecause we will never know the true model errors, we have to use the errors we computed from our regression model to estimate things like the variance and standard deviation of error:\n\\[Var(Y - \\hat{Y})=\\sigma^2_{error}\\]"
  },
  {
    "objectID": "lectures/05-lecture-slides.html#the-tools-of-statistical-inference",
    "href": "lectures/05-lecture-slides.html#the-tools-of-statistical-inference",
    "title": "Inference for Linear Regression",
    "section": "The Tools of Statistical Inference",
    "text": "The Tools of Statistical Inference\nWe have two inferential tools to help us determine if our data supports our data generating process:\n\nStatistical Significance Tests\nConfidence Intervals"
  },
  {
    "objectID": "lectures/05-lecture-slides.html#the-four-components-of-a-significance-test-confidence-interval",
    "href": "lectures/05-lecture-slides.html#the-four-components-of-a-significance-test-confidence-interval",
    "title": "Inference for Linear Regression",
    "section": "The Four Components of a Significance Test & Confidence Interval",
    "text": "The Four Components of a Significance Test & Confidence Interval\nIn order to ensure our inferential tools work, we need to assess the quality of their four different components:\n\nStatistical Model Assumptions\nHypotheses (directional or non-directional)\nTest Statistic\nP-Value (for significance tests)"
  },
  {
    "objectID": "lectures/05-lecture-slides.html#assumptions-of-linear-regression-analysis",
    "href": "lectures/05-lecture-slides.html#assumptions-of-linear-regression-analysis",
    "title": "Inference for Linear Regression",
    "section": "Assumptions of Linear Regression Analysis",
    "text": "Assumptions of Linear Regression Analysis\nTo ensure that inferences made about our regression coefficients are appropriate, we have to make six assumptions:\n\nValidity of our Data\nRepresentativeness of our Data\nAdditivity & Linearity\nIndependence of Errors\nEqual Variance of Errors (Homoscedasticity)\nNormality of Errors"
  },
  {
    "objectID": "lectures/05-lecture-slides.html#validity-of-our-data",
    "href": "lectures/05-lecture-slides.html#validity-of-our-data",
    "title": "Inference for Linear Regression",
    "section": "Validity of Our Data",
    "text": "Validity of Our Data\nOne of the most important assumptions we need to make is that our data maps to our research questions. We want to make sure that the data we have collected is a valid representation of the phenomenon in which we are interested!\nThere is a whole field called Psychometrics that is dedicated to issues around the validity of measurements!"
  },
  {
    "objectID": "lectures/05-lecture-slides.html#representativeness-of-our-data",
    "href": "lectures/05-lecture-slides.html#representativeness-of-our-data",
    "title": "Inference for Linear Regression",
    "section": "Representativeness of Our Data",
    "text": "Representativeness of Our Data\nThe data you are estimating your regression model from should be representative of the population you are making inferences about. This applies even when we are making inferences about a data generating process."
  },
  {
    "objectID": "lectures/05-lecture-slides.html#additivity-linearity",
    "href": "lectures/05-lecture-slides.html#additivity-linearity",
    "title": "Inference for Linear Regression",
    "section": "Additivity & Linearity",
    "text": "Additivity & Linearity\nAdditivity and Linearity is the assumption that the conditional mean of the outcome variable is a linear function of the different predictor variables:\n\\[E[Y_{\\text{AI Freq.}}|X_{\\text{Beh. Intent.}}] = \\beta_0 + \\beta_1X_{\\text{Beh. Intent.}}\\]\nViolations of this assumption happen when there is a nonlinear relationship between your outcome variable and at least one of your predictor variables."
  },
  {
    "objectID": "lectures/05-lecture-slides.html#additivity-linearity-a-graphical-view",
    "href": "lectures/05-lecture-slides.html#additivity-linearity-a-graphical-view",
    "title": "Inference for Linear Regression",
    "section": "Additivity & Linearity: A Graphical View",
    "text": "Additivity & Linearity: A Graphical View"
  },
  {
    "objectID": "lectures/05-lecture-slides.html#independence-of-errors",
    "href": "lectures/05-lecture-slides.html#independence-of-errors",
    "title": "Inference for Linear Regression",
    "section": "Independence of Errors",
    "text": "Independence of Errors\nThe assumption of independent errors (homoscedasticity) means that once we control for all the predictor variables, then there should be no relationship among our model residuals (the part of our outcome variable that is not captured by our model)."
  },
  {
    "objectID": "lectures/05-lecture-slides.html#independence-of-errors-a-graphical-view",
    "href": "lectures/05-lecture-slides.html#independence-of-errors-a-graphical-view",
    "title": "Inference for Linear Regression",
    "section": "Independence of Errors: A Graphical View",
    "text": "Independence of Errors: A Graphical View"
  },
  {
    "objectID": "lectures/05-lecture-slides.html#equal-variance-of-errors-homoscedasticity",
    "href": "lectures/05-lecture-slides.html#equal-variance-of-errors-homoscedasticity",
    "title": "Inference for Linear Regression",
    "section": "Equal Variance of Errors (Homoscedasticity)",
    "text": "Equal Variance of Errors (Homoscedasticity)\nThe assumption of equal error variances means that the variance of our errors (residuals) is constant across all values of our predictor variables. A different way to say this is that our errors (residuals) are unrelated to to our predictor variables."
  },
  {
    "objectID": "lectures/05-lecture-slides.html#equal-variance-of-errors-a-graphical-view",
    "href": "lectures/05-lecture-slides.html#equal-variance-of-errors-a-graphical-view",
    "title": "Inference for Linear Regression",
    "section": "Equal Variance of Errors: A Graphical View",
    "text": "Equal Variance of Errors: A Graphical View"
  },
  {
    "objectID": "lectures/05-lecture-slides.html#normality-of-errors",
    "href": "lectures/05-lecture-slides.html#normality-of-errors",
    "title": "Inference for Linear Regression",
    "section": "Normality of Errors",
    "text": "Normality of Errors\nThe normality of errors assumption means that our errors follow a normal probability distribution with a mean of zero and a variance equal to \\(\\sigma^2_{e}\\).\nAnother way to state this assumption is that the distribution of your outcome variable when conditioned on all of your predictor variables follows normal probability distribution.\nIt is important to note that the distribution of your outcome variable without conditioning on your predictor variables need not follow a normal probability distribution!"
  },
  {
    "objectID": "lectures/05-lecture-slides.html#normality-of-errors-a-graphical-view",
    "href": "lectures/05-lecture-slides.html#normality-of-errors-a-graphical-view",
    "title": "Inference for Linear Regression",
    "section": "Normality of Errors: A Graphical View",
    "text": "Normality of Errors: A Graphical View"
  },
  {
    "objectID": "lectures/05-lecture-slides.html#the-probability-model-implied-by-our-assumptions",
    "href": "lectures/05-lecture-slides.html#the-probability-model-implied-by-our-assumptions",
    "title": "Inference for Linear Regression",
    "section": "The Probability Model Implied By Our Assumptions",
    "text": "The Probability Model Implied By Our Assumptions\nWhen taken all together, our linear regression assumptions imply the following probabilistic model for the conditional distribution of our outcome variable:\n\\[Y_{i}|X_{i} \\sim N(\\beta_0 + \\beta_1X_{\\text{Beh. Intent.}},\\sigma^2_e )\\]\n\\[\\epsilon_i\\sim N(0, \\sigma^2_e)\\]"
  },
  {
    "objectID": "lectures/05-lecture-slides.html#hypotheses-and-the-data-generating-process",
    "href": "lectures/05-lecture-slides.html#hypotheses-and-the-data-generating-process",
    "title": "Inference for Linear Regression",
    "section": "Hypotheses and the Data Generating Process",
    "text": "Hypotheses and the Data Generating Process\nYou can think of a hypothesis as a statement about the data generating process. This statement needs to be made before your analyses!\nTo setup a significance test, you will need to develop two kinds of hypotheses:\n\nNull Hypothesis (\\(H_0\\)): Specifies that a parameter in your DGP takes on a specific value (usually 0).\nAlternative Hypothesis (\\(H_a\\)): Specifies that a parameter takes on some alternative range of values (e.g. all positive values)."
  },
  {
    "objectID": "lectures/05-lecture-slides.html#setting-up-the-null-hypothesis",
    "href": "lectures/05-lecture-slides.html#setting-up-the-null-hypothesis",
    "title": "Inference for Linear Regression",
    "section": "Setting Up the Null Hypothesis",
    "text": "Setting Up the Null Hypothesis\nThe null hypothesis is usually specifies that the parameter you DGP takes on is 0:\n\n\\(H_0\\): There is no relationship between behavioral intentions to use the AI tool and the frequency with which the tool is used (\\(\\beta_1=0\\)).\n\nAlthough it is incredibly rare, the null hypothesis can specify values other than 0:\n\n\\(H_0\\): The relationship between behavioral intentions to use the AI tool and the frequency with which the tool is used is equal to -.50 (\\(\\beta_1=-.50\\))."
  },
  {
    "objectID": "lectures/05-lecture-slides.html#setting-up-the-alternative-hypothesis",
    "href": "lectures/05-lecture-slides.html#setting-up-the-alternative-hypothesis",
    "title": "Inference for Linear Regression",
    "section": "Setting Up the Alternative Hypothesis",
    "text": "Setting Up the Alternative Hypothesis\nThe alternative hypothesis can be a directional or non-directional statement:\n\nDirectional \\(H_a\\): The relationship between behavioral intentions to use the AI tool and the frequency with which the tool is used is positive (\\(\\beta_1 &gt; 0\\)).\nNon-Directional \\(H_a\\): There is a relationship between behavioral intentions to use the AI tool and the frequency with which the tool is used (\\(\\beta_1 \\neq 0\\))."
  },
  {
    "objectID": "lectures/05-lecture-slides.html#what-is-a-test-statistic",
    "href": "lectures/05-lecture-slides.html#what-is-a-test-statistic",
    "title": "Inference for Linear Regression",
    "section": "What is a Test Statistic?",
    "text": "What is a Test Statistic?\nThe test statistic is a number that tells us how many standard errors our estimate falls from the parameter value specified by the null hypothesis. Generally, a test statistic has three pieces:\n\nThe estimate being tested (our estimated regression coefficient)\nThe value specified by the null hypothesis (almost always 0)\nThe standard error of the estimate"
  },
  {
    "objectID": "lectures/05-lecture-slides.html#t-test-the-test-statistic-for-linear-regression",
    "href": "lectures/05-lecture-slides.html#t-test-the-test-statistic-for-linear-regression",
    "title": "Inference for Linear Regression",
    "section": "T-Test: The Test Statistic for Linear Regression",
    "text": "T-Test: The Test Statistic for Linear Regression\nWe use a T-test to test the significance of any single regression coefficient:\n\\[t = \\frac{\\hat{\\beta}_1-\\beta_{\\text{Null Hyp}}}{SE(\\hat{\\beta}_1)}\\]"
  },
  {
    "objectID": "lectures/05-lecture-slides.html#sampling-distribution-standard-error",
    "href": "lectures/05-lecture-slides.html#sampling-distribution-standard-error",
    "title": "Inference for Linear Regression",
    "section": "Sampling Distribution & Standard Error",
    "text": "Sampling Distribution & Standard Error\nRemember the sampling distribution is a hypothetical distribution of all our possible \\(\\hat{\\beta}_1\\) estimates and the standard error is the standard deviation of that distribution."
  },
  {
    "objectID": "lectures/05-lecture-slides.html#standard-error-of-a-partial-regression-coefficient",
    "href": "lectures/05-lecture-slides.html#standard-error-of-a-partial-regression-coefficient",
    "title": "Inference for Linear Regression",
    "section": "Standard Error of a Partial Regression Coefficient",
    "text": "Standard Error of a Partial Regression Coefficient\nThe standard error of a partial regression coefficient tells how close our estimated coefficient is to the true value of that coefficient. The SE is affected by four different components:\n\nAn estimate of the overall amount of modeling error: \\(\\hat{\\sigma}^2_{error}\\)\nThe variance of the predictor variable: \\(Var(X_j)\\)\nThe overall sample size: \\(N\\)\nThe proportion of variance in the predictor variable that is explained by the other predictor variables: \\(R^2_j\\)"
  },
  {
    "objectID": "lectures/05-lecture-slides.html#formula-for-the-partial-regression-coefficient-se",
    "href": "lectures/05-lecture-slides.html#formula-for-the-partial-regression-coefficient-se",
    "title": "Inference for Linear Regression",
    "section": "Formula for the Partial Regression Coefficient SE",
    "text": "Formula for the Partial Regression Coefficient SE\n\\[SE(\\hat{\\beta}_j)=\\sqrt{\\frac{1}{1-R^2_j}}\\times\\sqrt{\\frac{\\hat{\\sigma}^2_{error}}{N \\times Var(X_j)}}\\]"
  },
  {
    "objectID": "lectures/05-lecture-slides.html#calculating-the-p-value-for-a-t-statistic",
    "href": "lectures/05-lecture-slides.html#calculating-the-p-value-for-a-t-statistic",
    "title": "Inference for Linear Regression",
    "section": "Calculating the P-Value for a t-Statistic",
    "text": "Calculating the P-Value for a t-Statistic\nTo test the significance of a t-statistic, we have to use a probability distribution called the Student’s t distribution.\nWe have to use the Student’s t distribution for one very specific reason: we are using an estimate of the model error variance (\\(\\sigma^2_e\\)) because we do not know the true value. If we knew the true value, we could use that in the SE formula and then calculate the significance of the test statistic using the Normal distribution."
  },
  {
    "objectID": "lectures/05-lecture-slides.html#students-t-distribution",
    "href": "lectures/05-lecture-slides.html#students-t-distribution",
    "title": "Inference for Linear Regression",
    "section": "Student’s t Distribution",
    "text": "Student’s t Distribution\nThe Student’s t Distribution or t distribution is a probability distribution that in many ways is like the Normal distribution (symmetrical around its mean of 0). However, the t distribution allows for more extreme observations compared to the Normal distribution—the t distribution tails are fatter than the Normal distribution tails."
  },
  {
    "objectID": "lectures/05-lecture-slides.html#students-t-distribution-degrees-of-freedom",
    "href": "lectures/05-lecture-slides.html#students-t-distribution-degrees-of-freedom",
    "title": "Inference for Linear Regression",
    "section": "Student’s T Distribution & Degrees of Freedom",
    "text": "Student’s T Distribution & Degrees of Freedom\nThe shape of the t distribution is controlled by something called the degrees of freedom. The larger the degrees of freedom, the more similar the t distribution is to the Normal distribution."
  },
  {
    "objectID": "lectures/05-lecture-slides.html#p-value-for-a-t-statistic",
    "href": "lectures/05-lecture-slides.html#p-value-for-a-t-statistic",
    "title": "Inference for Linear Regression",
    "section": "P-Value for a T-Statistic",
    "text": "P-Value for a T-Statistic\nTo calculate a p-value for our t-statistic, we need three things:\n\nT-Statistic\n\\(df_{Residual}\\): This tells us which t distribution to use\nThe direction of the hypothesis (directional or non-directional)\n\nThe p-value will tell us the probability of seeing a value as large or larger than our t-statistic, if we believe the true t-statistic should be 0.” Then, if the p-value is very small (less than or equal to .05), we say it’s pretty unlikely, so we can reject our null hypothesis."
  },
  {
    "objectID": "lectures/05-lecture-slides.html#testing-a-set-of-regression-coefficients",
    "href": "lectures/05-lecture-slides.html#testing-a-set-of-regression-coefficients",
    "title": "Inference for Linear Regression",
    "section": "Testing a Set of Regression Coefficients",
    "text": "Testing a Set of Regression Coefficients\nF test"
  },
  {
    "objectID": "lectures/05-lecture-slides.html#anova-table",
    "href": "lectures/05-lecture-slides.html#anova-table",
    "title": "Inference for Linear Regression",
    "section": "ANOVA Table",
    "text": "ANOVA Table"
  },
  {
    "objectID": "lectures/05-lecture-slides.html#f-distribution",
    "href": "lectures/05-lecture-slides.html#f-distribution",
    "title": "Inference for Linear Regression",
    "section": "F Distribution",
    "text": "F Distribution"
  },
  {
    "objectID": "lectures/05-lecture-slides.html#p-value",
    "href": "lectures/05-lecture-slides.html#p-value",
    "title": "Inference for Linear Regression",
    "section": "P-value",
    "text": "P-value"
  },
  {
    "objectID": "lectures/04-lecture-slides.html#broad-topics-we-have-covered",
    "href": "lectures/04-lecture-slides.html#broad-topics-we-have-covered",
    "title": "Review of Statistical Concepts",
    "section": "Broad Topics We Have Covered",
    "text": "Broad Topics We Have Covered\n\nDescriptive Statistics\n\nMean, Variance, Covariance, Correlation\n\nInferential Statistics\n\nProbability Distributions, Sampling Distributions, Significance Tests\n\nSimple Linear Regression\n\nModel Estimation, Interpretation, Fit Assessment\n\nMultiple Linear Regression\n\nModel Estimation, Interpretation, Fit Assessment"
  },
  {
    "objectID": "lectures/04-lecture-slides.html#lets-answer-some-questions",
    "href": "lectures/04-lecture-slides.html#lets-answer-some-questions",
    "title": "Review of Statistical Concepts",
    "section": "Let’s Answer Some Questions!",
    "text": "Let’s Answer Some Questions!\nTake ~5 minutes to write out your questions:\n\nSend me a message on Zoom (Not anonymous to me)\nRespond to the poll (Anonymous)"
  },
  {
    "objectID": "lectures/04-lecture-slides.html#concepts-in-descriptive-statistics-with-one-variable-univariate-statistics",
    "href": "lectures/04-lecture-slides.html#concepts-in-descriptive-statistics-with-one-variable-univariate-statistics",
    "title": "Review of Statistical Concepts",
    "section": "Concepts in Descriptive Statistics with One Variable (Univariate Statistics)",
    "text": "Concepts in Descriptive Statistics with One Variable (Univariate Statistics)\n\nMeasures of Central Tendency\n\nMean\nMedian\nMode\n\nMeasures of Spread\n\nVariance\nStandard Deviation"
  },
  {
    "objectID": "lectures/04-lecture-slides.html#concepts-in-descriptive-statistics-with-two-variables-bivariate-statistics",
    "href": "lectures/04-lecture-slides.html#concepts-in-descriptive-statistics-with-two-variables-bivariate-statistics",
    "title": "Review of Statistical Concepts",
    "section": "Concepts in Descriptive Statistics with Two Variables (Bivariate Statistics)",
    "text": "Concepts in Descriptive Statistics with Two Variables (Bivariate Statistics)\n\nMeasures of a Linear Relationship Between Two Variables\n\nCovariance\nCorrelation"
  },
  {
    "objectID": "lectures/04-lecture-slides.html#concepts-in-inferential-statistics",
    "href": "lectures/04-lecture-slides.html#concepts-in-inferential-statistics",
    "title": "Review of Statistical Concepts",
    "section": "Concepts in Inferential Statistics",
    "text": "Concepts in Inferential Statistics\n\nProbability Distributions\n\nNormal Distribution\n\nSampling Distributions\n\nSampling Variance & Standard Error\n\nSignificance Test\n\nHypotheses\nP-Value"
  },
  {
    "objectID": "lectures/04-lecture-slides.html#concepts-in-simple-regression",
    "href": "lectures/04-lecture-slides.html#concepts-in-simple-regression",
    "title": "Review of Statistical Concepts",
    "section": "Concepts in Simple Regression",
    "text": "Concepts in Simple Regression\n\nSimple Regression Model\nEstimating the Regression Coefficients\n\nIntercept & Slope\n\nInterpreting the Regression Coefficients\nAssessing the Overall fit of the Regression Model\n\n\\(R^2\\)\nError Variance"
  },
  {
    "objectID": "lectures/04-lecture-slides.html#concepts-in-multiple-regression",
    "href": "lectures/04-lecture-slides.html#concepts-in-multiple-regression",
    "title": "Review of Statistical Concepts",
    "section": "Concepts in Multiple Regression",
    "text": "Concepts in Multiple Regression\n\nMultiple Regression Model\nStatistical Control\nInterpreting the Regression Coefficients\n\nPartial Regression Coefficient\n\nAssessing the Overall fit of the Regression Model\n\n\\(R\\) & \\(R^2\\)\nError Variance"
  },
  {
    "objectID": "assignments/01-assignment-answers.html#assignment-setup",
    "href": "assignments/01-assignment-answers.html#assignment-setup",
    "title": "Answers to Assignment 1",
    "section": "",
    "text": "# Load required R packages\nlibrary(tibble)\nlibrary(readr)\n\n# Read in the data from the class site\ndata_ai &lt;- readr::read_csv(\"https://alopilato88.github.io/quantitative-analysis-1/assignments/01-assignment-data.csv\")\n\n# View our data frame \nhead(data_ai)\n\nThe code chunk above tells R to do three, broad tasks:\n\nLoad the packages we need for the code in the code chunks below using the library function (e.g. library(tibble))\nRead a .csv file using its URL using the read_csv function and save the .csv file in an object named data_ai\nUse the head function to print out the first 6 rows of our new object: data_ai (head(data_ai))\n\nWe told R to use a function to accomplish those three tasks. A function is a set of code that does one specific thing. For example, read_csv is a function that only reads .csv files. It does nothing else.\nNext, a package such as readr is a collection of functions that all share a similar goal. The package readr contains different functions such as read_csv, read_file, and write_csv, which are all functions that can be used to read or write different kinds of files."
  },
  {
    "objectID": "assignments/01-assignment-answers.html#question-1.",
    "href": "assignments/01-assignment-answers.html#question-1.",
    "title": "Answers to Assignment 1",
    "section": "Question 1.",
    "text": "Question 1.\n\n# 1. What is the sample size of your dataset (hint: It's the number of rows)? \n\nnrow(data_ai)\n\n[1] 500\n\n\nThe sample size of our data is the number of observations in our dataset. This is equivalent to the number of rows in our dataset—one row for each observation. So we can use the R function nrow to tell us the number of row. In this case there are 500 rows in our dataset, which means our sample size is 500."
  },
  {
    "objectID": "assignments/01-assignment-answers.html#question-2.",
    "href": "assignments/01-assignment-answers.html#question-2.",
    "title": "Answers to Assignment 1",
    "section": "Question 2.",
    "text": "Question 2.\n\n# 2. How many variables are in your dataset (hint: It's the number of columns)? \n\nncol(data_ai)\n\n[1] 4\n\n\nIn general, we will store our data in a .csv file where each column is a different variable. We can use the R function ncol to count the number of columns in our dataset. In our dataset, there are 4 columns, so we have 4 variables in our dataset."
  },
  {
    "objectID": "assignments/01-assignment-answers.html#question-3.",
    "href": "assignments/01-assignment-answers.html#question-3.",
    "title": "Answers to Assignment 1",
    "section": "Question 3.",
    "text": "Question 3.\n\n# 3. What is the mean and standard deviation of perceived_ease_use?\n\nmean(data_ai$perceived_ease_use) # mean() is a function that calculates the mean of a rando variable.\n\n[1] 4.078\n\nsd(data_ai$perceived_ease_use) # sd() is a function that calculates the standard deviation of a random variable.\n\n[1] 1.661758\n\n\nWhen you start R and RStudio, a handful of packages are loaded automatically. These packages make a lot of different functions immediately available to you. Two such functions are mean and sd.\nThe function mean only needs one argument—something that the use inputs—to work: a numeric R object otherwise known as a collection of numbers. mean then returns the mean value for the provided numeric object. In our class, the only numeric objects we will be providing to mean are the quantitative variables from our dataset. So to get the mean of perceived_ease_use, we use it as the argument in mean: mean(data_ai$perceived_ease_use), which returns 4.078.\nThe $ symbol used in the mean function tells the mean function to “look for” the variable perceived_ease_use in the dataset data_ai. If we did not include data_ai$ before perceived_ease_use, then the mean function would not where to find perceived_ease_use and the function would return an error message.\nSimilarly, to calculate the standard deviation of perceived_ease_use, we can use the preloaded R function: sd. Like mean, sd only requires one numeric object as an argument and it returns the standard deviation of the numeric values stored in the object. In our assignment, our numeric objects will almost always be the quantitative varialbes in our dataset. So to get the standard deviation of perceived_ease_use, we use perceived_ease_use as the argument in sd: sd(data_ai$perceived_ease_use), which returns 1.6617578.\nAgain we have to use the $ to tell sd to find perceived_ease_use in our dataset data_ai."
  },
  {
    "objectID": "assignments/01-assignment-answers.html#question-4.",
    "href": "assignments/01-assignment-answers.html#question-4.",
    "title": "Answers to Assignment 1",
    "section": "Question 4.",
    "text": "Question 4.\n\n# 4. What is the mean and standard deviation for perceived_useful?\n\nmean(data_ai$perceived_useful)\n\n[1] 4.76\n\nsd(data_ai$perceived_useful)\n\n[1] 1.755153\n\n\nJust like question 3, we can use the mean function to calculate the mean of perceived_useful and the sd function to calculate its standard deviation. Again we have to tell R where to find the variable by using data_ai$."
  },
  {
    "objectID": "assignments/01-assignment-answers.html",
    "href": "assignments/01-assignment-answers.html",
    "title": "Answers to Assignment 1",
    "section": "",
    "text": "# Load required R packages\nlibrary(tibble)\nlibrary(readr)\n\n# Read in the data from the class site\ndata_ai &lt;- readr::read_csv(\"https://alopilato88.github.io/quantitative-analysis-1/assignments/01-assignment-data.csv\")\n\n# View our data frame \nhead(data_ai)\n\nThe code chunk above tells R to do three, broad tasks:\n\nLoad the packages we need for the code in the code chunks below using the library function (e.g. library(tibble))\nRead a .csv file using its URL using the read_csv function and save the .csv file in an object named data_ai\nUse the head function to print out the first 6 rows of our new object: data_ai (head(data_ai))\n\nWe told R to use a function to accomplish those three tasks. A function is a set of code that does one specific thing. For example, read_csv is a function that only reads .csv files. It does nothing else.\nNext, a package such as readr is a collection of functions that all share a similar goal. The package readr contains different functions such as read_csv, read_file, and write_csv, which are all functions that can be used to read or write different kinds of files."
  },
  {
    "objectID": "lectures/05-lecture-page.html",
    "href": "lectures/05-lecture-page.html",
    "title": "Quantitative Analysis 1",
    "section": "",
    "text": "Next Week’s Materials »\nWelcome to your first Immersion Day!\nOn this page, you will find the schedule for our first Immersion Day, the lectures, and the R script you will need if you want to follow along the introduction to Tidyverse portion of the day."
  },
  {
    "objectID": "lectures/05-lecture-page.html#lecture-inference-for-linear-regression",
    "href": "lectures/05-lecture-page.html#lecture-inference-for-linear-regression",
    "title": "Quantitative Analysis 1",
    "section": "Lecture: Inference for Linear Regression",
    "text": "Lecture: Inference for Linear Regression\n\n\nTo download a pdf version of these slides, click here."
  },
  {
    "objectID": "assignments/01-assignment-answers.html#question-5.",
    "href": "assignments/01-assignment-answers.html#question-5.",
    "title": "Answers to Assignment 1",
    "section": "Question 5.",
    "text": "Question 5.\n\n# 5. What is the mean and standard deviation for behavioral_intention?\n\nmean(data_ai$behavioral_intention)\n\n[1] 4.314\n\nsd(data_ai$behavioral_intention)\n\n[1] 1.563423\n\n\nJust like the previous two questions, we can use the mean function to calculate the mean of behvioral_intention and the sd function to calculate its standard deviation. Again we have to tell R where to find the variable by using data_ai$."
  },
  {
    "objectID": "assignments/01-assignment-answers.html#question-6.",
    "href": "assignments/01-assignment-answers.html#question-6.",
    "title": "Answers to Assignment 1",
    "section": "Question 6.",
    "text": "Question 6.\n\n# 6a. What is the correlation between perceived_useful and perceived_ease_of_use? \n\n# cor() is a function that calculates the correlation between two random variables\n# The cor() function requires two arguments: an x variable and a y variable.\n# Below we tell R the x variable = data$perceived_useful and y variable = data_ai$perceived_ease_use.\n# You can read the $ operator as go into the data frame: data_ai and select the variable to the right of the $ sign.\n# data_ai$perceived_useful means go into data_ai and select the column perceived_useful\n\ncor(x = data_ai$perceived_useful, y = data_ai$perceived_ease_use) \n\n[1] 0.5423654\n\n# 6b. In your own words, write out an interpretation of the correlation you calculated in 6a. \n\nTo determine the correlation between two or more variables, we can use the preloaded R function: cor. For this homework, we only need to provide cor with two arguments, which are the variables we are interested in calculating a correlation coefficient for: perceived_useful and perceived_ease_use.\ncor(x = data_ai$perceived_useful, y = data_ai$perceived_ease_use) will go into our dataset, data_ai, and use the formula for the correlation coefficient to caluclate the correlation between perceived_useful and perceived_ease_use.\nWe will get the same correlation coefficient if we switch the order of our variables in the cor function: cor(x = data_ai$perceived_ease_use, y = data_ai$perceived_useful) as the correlation is symmetric. This means that the correlation between perceived_ease_use and perceived_useful is identical to the correlation between perceived_useful and perceived_ease_use.\nAs for the interpretation of the correlation, we can interpret it as:\nIn our dataset, the correlation between percieved_useful and perceived_ease_use is 0.5423654. Because the correlation is positive, we know that values of percieved_ease_use above the mean occur with values of perceived_useful that are above its mean. Similary the size of the correlation coefficient tells us that the two variables are moderately related to one another."
  },
  {
    "objectID": "assignments/01-assignment-answers.html#question-7.",
    "href": "assignments/01-assignment-answers.html#question-7.",
    "title": "Answers to Assignment 1",
    "section": "Question 7.",
    "text": "Question 7.\n\n# 7a. What is the correlation between perceived_useful and behavioral_intention? \ncor(x = data_ai$perceived_useful, y = data_ai$behavioral_intention) \n\n[1] 0.3795281\n\n# 7b. In your own words, write out an interpretation of the correlation you calculated in 7a. \n\nAgain we can use the cor function. We determine that the correlation between perceived_useful and behavioral_intention is 0.3795281, which means that perceived_useful and behavioral_intention are positively and moderately correlated to one another. High values of perceived_useful will tend to occur with high values of behavioral_intention, on average."
  },
  {
    "objectID": "assignments/01-assignment-answers.html#question-8.",
    "href": "assignments/01-assignment-answers.html#question-8.",
    "title": "Answers to Assignment 1",
    "section": "Question 8.",
    "text": "Question 8.\n\n# 8a. What is the correlation between perceived_ease_use and behavioral_intention? \ncor(x = data_ai$perceived_ease_use, y = data_ai$behavioral_intention) \n\n[1] 0.412486\n\n# 8b. In your own words, write out an interpretation of the correlation you calculated in 8a. \n\nAgain we can use the cor function. We determine that the correlation between perceived_ease_use and behavioral_intention is 0.412486, which means that perceived_ease_use and behavioral_intention are positively and moderately correlated to one another. High values of perceived_ease_use will tend to occur with high values of behavioral_intention, on average."
  },
  {
    "objectID": "assignments/01-assignment-answers.html#question-9.",
    "href": "assignments/01-assignment-answers.html#question-9.",
    "title": "Answers to Assignment 1",
    "section": "Question 9.",
    "text": "Question 9.\n\n# 9a. Estimate a simple regression model that uses perceived_ease_use to predict behavioral_intention.\n\n# The lm() function used below fits a linear regression model. The lm() code translated to the following\n# regression model: behvioral_intention = B0 + B1*perceived_ease_use. \n# In general the structure of the lm() function will look like lm(outcome_variable ~ predictor_variable, data = your_data)\n# In R, everyhing to the left of the ~ sign is an outcome variable and everything to the right is a predictor variable.\n\nmodel_1 &lt;- lm(behavioral_intention ~ perceived_ease_use, data = data_ai)\n\n# 9b. Print out the results of your regression model and write out an interpretation of the effect of perceived_ease_use on\n#     behavioral_intentions.\n\nsummary(model_1)\n\n\nCall:\nlm(formula = behavioral_intention ~ perceived_ease_use, data = data_ai)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.6718 -1.0599 -0.0599  0.9401  3.4924 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         2.73142    0.16910   16.15   &lt;2e-16 ***\nperceived_ease_use  0.38808    0.03841   10.11   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.426 on 498 degrees of freedom\nMultiple R-squared:  0.1701,    Adjusted R-squared:  0.1685 \nF-statistic: 102.1 on 1 and 498 DF,  p-value: &lt; 2.2e-16\n\n# 9c. What is the R-squared value for model_1? Write out an interpretation of the R-squared.\n\nTo estimate a linear regression model in R, we will use the lm function. The lm function is automatically made available to us when we start R and RStudio, so we do not need to tell R to load it.\nlm requires two main arguments: the lm regression formula and the dataset that contains the predictor and outcome variables for the regression model.\nFirst, the lm formula will always look like outcome_variable_name ~ predictor_variable_name_1, which we can read as estimate a linear regression model where the outcome variable is outcome_variable_name and the predictor variable is predictor_variable_name_1. In general, any variable to the left of ~ is treated as an outcome variable by lm and any variable to the right of lm is treated as a predictor variable.\nNext, the lm function needs to know to where to find the dataset that contains our outcome and predictor variables. We can tell lm where to find our dataset by providing it with its second argument: data = data_ai. Now lm knows that data_ai contains the outcome and predictor variables used in the lm formula.\nNow that we have provide lm with its two necessary arguments, we can estimate it and save it in an object we call model_1. This is all happening in the line of code: model_1 &lt;- lm(behavioral_intention ~ perceived_ease_use, data = data_ai. lm is estimating a regression model where behavioral_intention is the outcome variable and perceived_ease_use is the predictor variable, both of which it knows to find in data_ai. Then it is storing the results of that model in an object (think of it like a box) called model_1.\nWe know R is storing the results of the model into an object named model_1 because of this part of the code: model_1 &lt;-. The &lt;- is called the assignment operator and it creates an object named model_1 and assigns it the results of lm(behavioral_intention ~ perceived_ease_use, data = data_ai.\n\nModel Interpretation\nNow that we have estimated our model, we can use the function summary to display the detailed results of our model. You should see from the results above that summary provides the coefficient estimates (Estimate), there standard errors (Std. Error), and a p-value (Pr(&gt;|t|)) along with more results like the R-squared value.\nWe can interpret the results as follows. When comparing two groups who differ by one unit on their response to perceived_ease_use, the average response to behavioral_intetions for the group with the higher response to perceived_ease_use will be 0.39 units higher than the group with the lower response.\nWe could also say: For every one unit increase in an individual’s perceptions of how easy it is to use the AI tool (perceived_ease_use), we will see their intentions to use the AI tool increase by 0.39. I find that this interpretation is easier to understand when compared to the group comparison interpretation, but it is less accurate.\nThe R-squared value in our model is 0.1701447, which means that 17% of the variance in behavioral_intention can be explained by perceived_ease_use."
  },
  {
    "objectID": "assignments/01-assignment-answers.html#question-10.",
    "href": "assignments/01-assignment-answers.html#question-10.",
    "title": "Answers to Assignment 1",
    "section": "Question 10.",
    "text": "Question 10.\n\n# 10a. Estimate a simple regression model that uses perceived_useful to predict behavioral_intention. Name the model: model_2.\n\nmodel_2 &lt;- lm(behavioral_intention ~ perceived_useful, data = data_ai)\n\n# 10b. Print out the results of your regression model and write out an interpretation of the effect of perceived_useful on\n#     behavioral_intentions.\n\nsummary(model_2)\n\n\nCall:\nlm(formula = behavioral_intention ~ perceived_useful, data = data_ai)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.0713 -1.0571 -0.0571  0.9429  3.6191 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       2.70479    0.18733  14.439   &lt;2e-16 ***\nperceived_useful  0.33807    0.03693   9.154   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.448 on 498 degrees of freedom\nMultiple R-squared:  0.144, Adjusted R-squared:  0.1423 \nF-statistic:  83.8 on 1 and 498 DF,  p-value: &lt; 2.2e-16\n\n# 10c. What is the R-squared value for model_2? Write out an interpretation of the R-squared.\n\nThe steps used to answer this question are identical to those we used in question 9.\nWe can interpret the results as follows. When comparing two groups who differ by one unit on their response to perceived_useful, the average response to behavioral_intetions for the group with the higher response to perceived_useful will be 0.34 units higher than the group with the lower response.\nWe could also say: For every one unit increase in an individual’s perceptions of how useful the AI tool is (perceived_useful), we will see their intentions to use the AI tool increase by 0.34.\nThe R-squared value in our model is 0.1440416, which means that 14% of the variance in behavioral_intention can be explained by perceived_useful."
  },
  {
    "objectID": "assignments/01-assignment-answers.html#question-11.",
    "href": "assignments/01-assignment-answers.html#question-11.",
    "title": "Answers to Assignment 1",
    "section": "Question 11.",
    "text": "Question 11.\n\n# 11a. Estimate a multiple regression model that uses perceived_useful and perceived_ease_use to predict behavioral_intention.\n\nmodel_3 &lt;- lm(behavioral_intention ~ perceived_ease_use + perceived_useful, data = data_ai)\n\n# 11b. Print out the results of your multiple regression model and write out an interpretation of both partial regression \n#      coefficients.\n\n# 11c. What is the R-squared value for model_3? Write out an interpretation of the R-squared.\n\nThe steps used to answer this question are identical to those we used in questions 9 and 10. The only difference is that instead of estimating a model with one predictor variable, we are estimating a model with two predictor variables: perceived_ease_use + perceived_useful. To add more predictor variables to our model, we just write: + predictor_variable_name_1 + predictor_variable_name_2 + predictor_variable_name_3 for as many predictor variables as we would like to add. Nothing else about the lm function changes.\nWe can interpret the results as follows:\n\nWhen comparing two groups who differ by one unit on their response to perceived_useful, but who have the same response to perceived_ease_use, the average response to behavioral_intetions for the group with the higher response to perceived_useful will be 0.2 units higher than the group with the lower response.\nWhen comparing two groups who differ by one unit on their response to perceived_ease_use, but who have the same response to perceived_useful, the average response to behavioral_intetions for the group with the higher response to perceived_ease_of_use will be 0.28 units higher than the group with the lower response.\n\nWe could also say: While controlling for perceived_ease_use, for every one unit increase in an individual’s perceptions of how useful the AI tool is (perceived_useful), we will see their intentions to use the AI tool increase by 0.2.\nThe R-squared value in our model is 0.2045388, which means that 20% of the variance in behavioral_intention can be explained by perceived_useful and perceived_ease_use together."
  },
  {
    "objectID": "lectures/04-lecture-page.html#lecture-review-of-statistical-concepts",
    "href": "lectures/04-lecture-page.html#lecture-review-of-statistical-concepts",
    "title": "Quantitative Analysis 1",
    "section": "Lecture: Review of Statistical Concepts",
    "text": "Lecture: Review of Statistical Concepts\n\n\nTo download a pdf version of these slides, click here."
  },
  {
    "objectID": "lectures/05-lecture-slides.html#what-on-earth-are-degrees-of-freedom",
    "href": "lectures/05-lecture-slides.html#what-on-earth-are-degrees-of-freedom",
    "title": "Inference for Linear Regression",
    "section": "What on Earth are Degrees of Freedom?",
    "text": "What on Earth are Degrees of Freedom?\nThink of your data as “statistical cash” to be spent on estimating the regression coefficients of your model. Degrees of freedom is almost like an accounting system that tracks:\n\nTotal net worth at the start: \\(df_{Total} = N\\)\nCash spent on your model: \\(df_{Model} = k + 1\\)\nCash remaining: \\(df_{Residual} = N-k-1\\)"
  },
  {
    "objectID": "lectures/05-lecture-slides.html#confidence-interval",
    "href": "lectures/05-lecture-slides.html#confidence-interval",
    "title": "Inference for Linear Regression",
    "section": "Confidence Interval",
    "text": "Confidence Interval"
  },
  {
    "objectID": "lectures/05-lecture-slides.html#statistical-accounting-with-degress-of-freedom",
    "href": "lectures/05-lecture-slides.html#statistical-accounting-with-degress-of-freedom",
    "title": "Inference for Linear Regression",
    "section": "Statistical Accounting with Degress of Freedom",
    "text": "Statistical Accounting with Degress of Freedom"
  },
  {
    "objectID": "lectures/05-lecture-slides.html#why-is-it-called-a-t-test",
    "href": "lectures/05-lecture-slides.html#why-is-it-called-a-t-test",
    "title": "Inference for Linear Regression",
    "section": "Why is it called a t-test?",
    "text": "Why is it called a t-test?\nTo test the significance of a t-statistic, we have to use a probability distribution called the Student’s t distribution.\nWe have to use the Student’s t distribution for one very specific reason: we are using an estimate of the model error variance (\\(\\sigma^2_e\\)) because we do not know the true value. If we knew the true value, we could use that in the SE formula and then calculate the significance of the test statistic using the Normal distribution."
  },
  {
    "objectID": "lectures/05-lecture-slides.html#dont-go-broke",
    "href": "lectures/05-lecture-slides.html#dont-go-broke",
    "title": "Inference for Linear Regression",
    "section": "Don’t Go Broke",
    "text": "Don’t Go Broke\nYou always want to have a positive (and large) amount for \\(df_{Residual}\\). We need a positive amount of \\(df_{Residual}\\) in order to test how well our model fits (e.g. calculate the \\(R^2\\)). If we spend ALL of our statistical cash on estimating regression coefficients, then we will not have any cash left over to determine our model fit."
  },
  {
    "objectID": "lectures/05-lecture-slides.html#an-example-with-our-data",
    "href": "lectures/05-lecture-slides.html#an-example-with-our-data",
    "title": "Inference for Linear Regression",
    "section": "An Example with our Data",
    "text": "An Example with our Data\n\nmodel &lt;- lm(freq_use_ai ~ beh_intent_ai + tech_anx, data = data_lecture)\nsummary(model)\n\n\nCall:\nlm(formula = freq_use_ai ~ beh_intent_ai + tech_anx, data = data_lecture)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.7311 -0.6897 -0.1087  0.8499  3.1599 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    1.34183    0.07029  19.090   &lt;2e-16 ***\nbeh_intent_ai  0.56717    0.01007  56.349   &lt;2e-16 ***\ntech_anx      -0.01379    0.01218  -1.132    0.258    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.029 on 4997 degrees of freedom\nMultiple R-squared:  0.4477,    Adjusted R-squared:  0.4475 \nF-statistic:  2025 on 2 and 4997 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "lectures/05-lecture-slides.html#a-quick-note-on-p-values-for-directional-hypotheses",
    "href": "lectures/05-lecture-slides.html#a-quick-note-on-p-values-for-directional-hypotheses",
    "title": "Inference for Linear Regression",
    "section": "A Quick Note on P-Values for Directional Hypotheses",
    "text": "A Quick Note on P-Values for Directional Hypotheses\nR (and all other statistical software programs) provide the p-value for a non-directional hypothesis. It is totally fine to use this p-value even if you specify a directional hypothesis, you just need to be aware that the p-value for a directional hypothesis will always equal:\n\\[\\text{p-value}_{\\text{Directional}}=\\frac{\\text{p-value}_{\\text{Non-Directioanl}}}{2}\\]"
  },
  {
    "objectID": "lectures/05-lecture-slides.html#f-test",
    "href": "lectures/05-lecture-slides.html#f-test",
    "title": "Inference for Linear Regression",
    "section": "F-Test!?",
    "text": "F-Test!?\nThe F-Test is a test of two identical null hypotheses:\n\n\\(R^2 = 0\\)\n\\(\\beta_1=\\beta_2=...=\\beta_p=0\\)\n\nIf the p-value for the F-test is less than or equal to .05 you can conclude that the \\(R^2\\) is not equal to 0 and that at least one of your regression coefficients is not equal to 0."
  },
  {
    "objectID": "lectures/05-lecture-slides.html#the-underlying-math-of-the-f-test",
    "href": "lectures/05-lecture-slides.html#the-underlying-math-of-the-f-test",
    "title": "Inference for Linear Regression",
    "section": "The Underlying Math of the F-Test",
    "text": "The Underlying Math of the F-Test\nSee me after class if you really want to know this…"
  },
  {
    "objectID": "lectures/05-lecture-slides.html#what-is-a-confidence-interval",
    "href": "lectures/05-lecture-slides.html#what-is-a-confidence-interval",
    "title": "Inference for Linear Regression",
    "section": "What is a Confidence Interval?",
    "text": "What is a Confidence Interval?\nA confidence interval is a range of values that the true parameter likely falls within."
  },
  {
    "objectID": "lectures/05-lecture-slides.html#the-components-of-a-confidence-interval",
    "href": "lectures/05-lecture-slides.html#the-components-of-a-confidence-interval",
    "title": "Inference for Linear Regression",
    "section": "The Components of a Confidence Interval",
    "text": "The Components of a Confidence Interval\n\\[\\hat{\\beta} \\pm \\text{SE}_\\beta \\times T_{\\text{Perc.}}\\]\n\nEstimate: The regression coefficient from your model\nStandard Error: The standard error of that regression coefficient\nT Value for the 97.5th Percentile: A value that is roughly 2"
  },
  {
    "objectID": "lectures/05-lecture-slides.html#confident-vs-95-confident",
    "href": "lectures/05-lecture-slides.html#confident-vs-95-confident",
    "title": "Inference for Linear Regression",
    "section": "99% Confident vs 95% Confident",
    "text": "99% Confident vs 95% Confident"
  },
  {
    "objectID": "lectures/05-lecture-slides.html#math-behind-confidence-intervals",
    "href": "lectures/05-lecture-slides.html#math-behind-confidence-intervals",
    "title": "Inference for Linear Regression",
    "section": "Math Behind Confidence Intervals",
    "text": "Math Behind Confidence Intervals"
  },
  {
    "objectID": "lectures/05-lecture-slides.html#visualizing-a-confidence-interval-for-behavioral-intention",
    "href": "lectures/05-lecture-slides.html#visualizing-a-confidence-interval-for-behavioral-intention",
    "title": "Inference for Linear Regression",
    "section": "Visualizing a Confidence Interval for Behavioral Intention",
    "text": "Visualizing a Confidence Interval for Behavioral Intention\nWe are 95% confident that the true value for the effect of behavioral intention on frequency of AI use is in this range: 0.55, 0.59"
  },
  {
    "objectID": "lectures/05-lecture-slides.html#compatability",
    "href": "lectures/05-lecture-slides.html#compatability",
    "title": "Inference for Linear Regression",
    "section": "Compatability",
    "text": "Compatability"
  },
  {
    "objectID": "lectures/05-lecture-slides.html#confidence-interval-for-behavioral-intention",
    "href": "lectures/05-lecture-slides.html#confidence-interval-for-behavioral-intention",
    "title": "Inference for Linear Regression",
    "section": "Confidence Interval for Behavioral Intention",
    "text": "Confidence Interval for Behavioral Intention\nWe are 95% confident that the true value for the effect of behavioral intention on frequency of AI use is in this range: 0.547, 0.587"
  },
  {
    "objectID": "lectures/05-lecture-slides.html#compability-interval-confidence-intveral",
    "href": "lectures/05-lecture-slides.html#compability-interval-confidence-intveral",
    "title": "Inference for Linear Regression",
    "section": "Compability Interval > Confidence Intveral",
    "text": "Compability Interval &gt; Confidence Intveral\nI believe the best way to understand the confidence interval is as a compatibility interval. The interval tells us the range of null hypothesis values that we would not be able to reject (they would have p-values greater than or equal to .05).\n\n\n\n\n\nNull Value\nP-Value\n\n\n\n\n0.546\n0.036\n\n\n0.547\n0.050\n\n\n0.587\n0.050\n\n\n0.588\n0.039"
  },
  {
    "objectID": "lectures/05-lecture-slides.html#why-confidence-compatability-intervals-trump-p-values",
    "href": "lectures/05-lecture-slides.html#why-confidence-compatability-intervals-trump-p-values",
    "title": "Inference for Linear Regression",
    "section": "Why Confidence (Compatability Intervals) Trump P-Values",
    "text": "Why Confidence (Compatability Intervals) Trump P-Values\nHere is a example where a finding is statistically different from 0, but not practically different. The P-Value does not tell you the estimate is not practically different from 0, but the confidence interval does:\n\n\n\n\n\n\n\n\n\n\n\n\n\nEffect\nEstimate\nStd. Error\nP-Value\nLower Bound\nUpper Bound\n\n\n\n\n(Intercept)\n2.231\n4.310\n0.606\n-6.322\n10.784\n\n\nPredictor Var.\n5.075\n2.144\n0.020\n0.821\n9.329"
  },
  {
    "objectID": "lectures/06-lecture-slides.html#schedule-for-today",
    "href": "lectures/06-lecture-slides.html#schedule-for-today",
    "title": "Using Categoical Predictors in Linear Regression",
    "section": "Schedule for Today",
    "text": "Schedule for Today\n\nTalk about stats for ~75 mins (5 PM - 6:15 PM ET)\nBreak for 10 minutes\nFinish up stats for 35 mins (6:25 PM - 7:00 PM ET)"
  },
  {
    "objectID": "lectures/06-lecture-slides.html#overview",
    "href": "lectures/06-lecture-slides.html#overview",
    "title": "Categorical Predictors in Linear Regression",
    "section": "Overview",
    "text": "Overview\n\nReview of categorical predictor examples\nSetup of our working example\nIntroduction to using categorical variables in linear regression"
  },
  {
    "objectID": "lectures/06-lecture-slides.html#goals",
    "href": "lectures/06-lecture-slides.html#goals",
    "title": "Categorical Predictors in Linear Regression",
    "section": "Goals",
    "text": "Goals\n\nUnderstand how to use categorical variables as predictors\nUnderstand how to makes inferences about categorical predictors"
  },
  {
    "objectID": "lectures/06-lecture-slides.html#what-is-a-categorical-predictor",
    "href": "lectures/06-lecture-slides.html#what-is-a-categorical-predictor",
    "title": "Categorical Predictors in Linear Regression",
    "section": "What is a Categorical Predictor?",
    "text": "What is a Categorical Predictor?\nA categorical predictor is a variable where the values denote membership to a specific group."
  },
  {
    "objectID": "lectures/06-lecture-slides.html#examples-of-categorical-predictors",
    "href": "lectures/06-lecture-slides.html#examples-of-categorical-predictors",
    "title": "Categorical Predictors in Linear Regression",
    "section": "Examples of Categorical Predictors?",
    "text": "Examples of Categorical Predictors?\nSome common categorical predictors are:\n\nBiological sex assigned at birth (Male or Female)\nEducational Level (Some HS, HS, College, Graduate)\nEmployment Status (Full Time or Part Time)\n\nCan anyone think of any others?"
  },
  {
    "objectID": "lectures/06-lecture-slides.html#dichotomous-vs-multicategorical-predictors",
    "href": "lectures/06-lecture-slides.html#dichotomous-vs-multicategorical-predictors",
    "title": "Categorical Predictors in Linear Regression",
    "section": "Dichotomous vs Multicategorical Predictors",
    "text": "Dichotomous vs Multicategorical Predictors\nWe will distinguish between categorical predictors that have two groups and those that have three or more groups:\n\nDichotomous or Binary Predictors: Two groups\nMulticategorical Predictors: Three or more groups"
  },
  {
    "objectID": "lectures/06-lecture-slides.html#our-working-example",
    "href": "lectures/06-lecture-slides.html#our-working-example",
    "title": "Categorical Predictors in Linear Regression",
    "section": "Our Working Example",
    "text": "Our Working Example\nWe want to understand if an employee’s previous experience with AI-based technology affects the frequency with which they adopt to the organization’s new gen AI chatbot.\nOur data generating process (theoretical model):\n\\[Y_{\\text{Freq Use.}}=\\beta_0+\\beta_1X_{\\text{Prev. Exp. Tech.}} \\]\nWhere the variable previous experience with technology is a dichotomous variable with the values: Yes or No."
  },
  {
    "objectID": "lectures/06-lecture-slides.html#representing-a-dichotomous-predictor-with-numbers",
    "href": "lectures/06-lecture-slides.html#representing-a-dichotomous-predictor-with-numbers",
    "title": "Categorical Predictors in Linear Regression",
    "section": "Representing a Dichotomous Predictor with Numbers",
    "text": "Representing a Dichotomous Predictor with Numbers\nTo be able to use a dichotomous predictor in a statistical model, we have to represent the categories as numbers:\n\n\n\n\n\nPrev. Exp.\nCode 1\nCode 2\nCode 3\nCode 4\n\n\n\n\nNo\n-0.5\n-1\n1\n0\n\n\nYes\n0.5\n1\n2\n1"
  },
  {
    "objectID": "lectures/06-lecture-slides.html#dummy-coding",
    "href": "lectures/06-lecture-slides.html#dummy-coding",
    "title": "Using Categoical Predictors in Linear Regression",
    "section": "Dummy Coding",
    "text": "Dummy Coding\nWe can use any coding scheme as long as observations in the same group are assigned the same number and these numbers are different across groups. There is, however, one coding scheme that makes the most sense for regression: Dummy Coding.\n\n\n\n\n\nPrev. Exp.\nDummy Code\n\n\n\n\nNo\n0\n\n\nYes\n1"
  },
  {
    "objectID": "lectures/06-lecture-slides.html#storing-a-dichtomous-predictor-in-your-data",
    "href": "lectures/06-lecture-slides.html#storing-a-dichtomous-predictor-in-your-data",
    "title": "Categorical Predictors in Linear Regression",
    "section": "Storing a Dichtomous Predictor in Your Data",
    "text": "Storing a Dichtomous Predictor in Your Data\nIt is good practice to have the numerical and categorical representation of your categorical variable in your dataset:\n\n\n# A tibble: 10 × 3\n   prev_exp_cat prev_exp freq_use\n   &lt;chr&gt;           &lt;dbl&gt;    &lt;dbl&gt;\n 1 No                  0        3\n 2 No                  0        1\n 3 No                  0        3\n 4 No                  0        2\n 5 No                  0        4\n 6 No                  0        4\n 7 No                  0        1\n 8 Yes                 1        6\n 9 Yes                 1        3\n10 Yes                 1        2"
  },
  {
    "objectID": "lectures/06-lecture-slides.html#what-happens-to-the-regression-model-with-two-groups",
    "href": "lectures/06-lecture-slides.html#what-happens-to-the-regression-model-with-two-groups",
    "title": "Using Categoical Predictors in Linear Regression",
    "section": "What Happens to the Regression Model with Two Groups?",
    "text": "What Happens to the Regression Model with Two Groups?"
  },
  {
    "objectID": "lectures/06-lecture-slides.html#r-output",
    "href": "lectures/06-lecture-slides.html#r-output",
    "title": "Categorical Predictors in Linear Regression",
    "section": "R Output",
    "text": "R Output\n\nmodel &lt;- lm(freq_use ~ prev_exp_cat, data = data_lecture_multicat)\nsummary(model)\n\n\nCall:\nlm(formula = freq_use ~ prev_exp_cat, data = data_lecture_multicat)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -2.65  -0.76   0.24   1.24   3.24 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)           2.76000    0.06098  45.262  &lt; 2e-16 ***\nprev_exp_catmoderate  0.82667    0.11133   7.425  3.9e-13 ***\nprev_exp_cathigh      1.89000    0.12936  14.611  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.141 on 597 degrees of freedom\nMultiple R-squared:  0.2762,    Adjusted R-squared:  0.2737 \nF-statistic: 113.9 on 2 and 597 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "lectures/06-lecture-slides.html#interpreting-the-regression-coefficients",
    "href": "lectures/06-lecture-slides.html#interpreting-the-regression-coefficients",
    "title": "Categorical Predictors in Linear Regression",
    "section": "Interpreting the Regression Coefficients",
    "text": "Interpreting the Regression Coefficients\nThe reason we use indicator coding is because it makes the regression coefficients very easy to interpret:\n\\[\\beta_0=\\overline{Y}_{\\text{Group: 0}}\\]\n\\[\\beta_1 = \\overline{Y}_{\\text{Group: 1}}-\\overline{Y}_{\\text{Group: 0}}\\]"
  },
  {
    "objectID": "lectures/06-lecture-slides.html#setting-a-reference-group",
    "href": "lectures/06-lecture-slides.html#setting-a-reference-group",
    "title": "Categorical Predictors in Linear Regression",
    "section": "Setting a Reference Group",
    "text": "Setting a Reference Group\nThe reference group is the group that receives a 0 across all of the indicator variables for the categorical variable. In our example, the reference group is low as observations in the low group receive values of 0 across both the prev_exp_mod and prev_exp_high indicator variables.\nThere are no real statistical consequences for your choice of reference group, I generally recommend picking the group with the largest group size or the group you want to compare to all of the remaining groups."
  },
  {
    "objectID": "lectures/06-lecture-slides.html#r-output-1",
    "href": "lectures/06-lecture-slides.html#r-output-1",
    "title": "Using Categoical Predictors in Linear Regression",
    "section": "R Output",
    "text": "R Output"
  },
  {
    "objectID": "lectures/06-lecture-slides.html#mean-comparison-using-a-t-test",
    "href": "lectures/06-lecture-slides.html#mean-comparison-using-a-t-test",
    "title": "Using Categoical Predictors in Linear Regression",
    "section": "Mean Comparison using a T-Test",
    "text": "Mean Comparison using a T-Test"
  },
  {
    "objectID": "lectures/06-lecture-slides.html#equivalence-of-a-t-test-for-two-means",
    "href": "lectures/06-lecture-slides.html#equivalence-of-a-t-test-for-two-means",
    "title": "Using Categoical Predictors in Linear Regression",
    "section": "Equivalence of a t-test for two means",
    "text": "Equivalence of a t-test for two means"
  },
  {
    "objectID": "lectures/06-lecture-slides.html#including-a-continuous-predictor-variable",
    "href": "lectures/06-lecture-slides.html#including-a-continuous-predictor-variable",
    "title": "Categorical Predictors in Linear Regression",
    "section": "Including a Continuous Predictor Variable",
    "text": "Including a Continuous Predictor Variable"
  },
  {
    "objectID": "lectures/06-lecture-slides.html#r-output-2",
    "href": "lectures/06-lecture-slides.html#r-output-2",
    "title": "Using Categoical Predictors in Linear Regression",
    "section": "R Output",
    "text": "R Output"
  },
  {
    "objectID": "lectures/06-lecture-slides.html#interpreting-the-coefficients",
    "href": "lectures/06-lecture-slides.html#interpreting-the-coefficients",
    "title": "Using Categoical Predictors in Linear Regression",
    "section": "Interpreting the Coefficients",
    "text": "Interpreting the Coefficients"
  },
  {
    "objectID": "lectures/06-lecture-slides.html#examples-of-multicategorical-predictors",
    "href": "lectures/06-lecture-slides.html#examples-of-multicategorical-predictors",
    "title": "Using Categoical Predictors in Linear Regression",
    "section": "Examples of Multicategorical Predictors?",
    "text": "Examples of Multicategorical Predictors?"
  },
  {
    "objectID": "lectures/06-lecture-slides.html#building-a-mulitcategoical-predictor",
    "href": "lectures/06-lecture-slides.html#building-a-mulitcategoical-predictor",
    "title": "Using Categoical Predictors in Linear Regression",
    "section": "Building a Mulitcategoical Predictor",
    "text": "Building a Mulitcategoical Predictor"
  },
  {
    "objectID": "lectures/06-lecture-slides.html#dummy-coding-g-1-predictors",
    "href": "lectures/06-lecture-slides.html#dummy-coding-g-1-predictors",
    "title": "Using Categoical Predictors in Linear Regression",
    "section": "Dummy Coding: g-1 Predictors",
    "text": "Dummy Coding: g-1 Predictors"
  },
  {
    "objectID": "lectures/06-lecture-slides.html#storing-a-mulitcategorical-predictor-in-your-data",
    "href": "lectures/06-lecture-slides.html#storing-a-mulitcategorical-predictor-in-your-data",
    "title": "Categorical Predictors in Linear Regression",
    "section": "Storing a Mulitcategorical Predictor in Your Data",
    "text": "Storing a Mulitcategorical Predictor in Your Data\nTo numerically represent a multicategorical variable, we need to create g - 1 indicator variables, where g equals the number of groups.\n\n\n# A tibble: 10 × 4\n   prev_exp_cat prev_exp_mod prev_exp_high freq_use\n   &lt;fct&gt;               &lt;dbl&gt;         &lt;dbl&gt;    &lt;dbl&gt;\n 1 low                     0             0        3\n 2 low                     0             0        4\n 3 low                     0             0        3\n 4 low                     0             0        3\n 5 moderate                1             0        3\n 6 moderate                1             0        3\n 7 moderate                1             0        3\n 8 high                    0             1        6\n 9 high                    0             1        5\n10 high                    0             1        3"
  },
  {
    "objectID": "lectures/06-lecture-slides.html#what-happens-to-the-regression-model-with-multiple-groups",
    "href": "lectures/06-lecture-slides.html#what-happens-to-the-regression-model-with-multiple-groups",
    "title": "Using Categoical Predictors in Linear Regression",
    "section": "What Happens to the Regression Model with Multiple Groups?",
    "text": "What Happens to the Regression Model with Multiple Groups?"
  },
  {
    "objectID": "lectures/06-lecture-slides.html#r-output-3",
    "href": "lectures/06-lecture-slides.html#r-output-3",
    "title": "Using Categoical Predictors in Linear Regression",
    "section": "R Output",
    "text": "R Output"
  },
  {
    "objectID": "lectures/06-lecture-slides.html#interpreting-the-regression-coefficients-1",
    "href": "lectures/06-lecture-slides.html#interpreting-the-regression-coefficients-1",
    "title": "Using Categoical Predictors in Linear Regression",
    "section": "Interpreting the Regression Coefficients",
    "text": "Interpreting the Regression Coefficients"
  },
  {
    "objectID": "lectures/06-lecture-slides.html#setting-a-reference-group-1",
    "href": "lectures/06-lecture-slides.html#setting-a-reference-group-1",
    "title": "Using Categoical Predictors in Linear Regression",
    "section": "Setting a Reference Group",
    "text": "Setting a Reference Group"
  },
  {
    "objectID": "lectures/06-lecture-slides.html#r-output-4",
    "href": "lectures/06-lecture-slides.html#r-output-4",
    "title": "Using Categoical Predictors in Linear Regression",
    "section": "R Output",
    "text": "R Output"
  },
  {
    "objectID": "lectures/06-lecture-slides.html#group-comparison-using-an-anova",
    "href": "lectures/06-lecture-slides.html#group-comparison-using-an-anova",
    "title": "Using Categoical Predictors in Linear Regression",
    "section": "Group Comparison using an ANOVA",
    "text": "Group Comparison using an ANOVA"
  },
  {
    "objectID": "lectures/06-lecture-slides.html#anova-table",
    "href": "lectures/06-lecture-slides.html#anova-table",
    "title": "Using Categoical Predictors in Linear Regression",
    "section": "ANOVA Table",
    "text": "ANOVA Table"
  },
  {
    "objectID": "lectures/06-lecture-slides.html#equivalence-of-an-anova",
    "href": "lectures/06-lecture-slides.html#equivalence-of-an-anova",
    "title": "Using Categoical Predictors in Linear Regression",
    "section": "Equivalence of an ANOVA",
    "text": "Equivalence of an ANOVA"
  },
  {
    "objectID": "lectures/06-lecture-slides.html#including-a-continuous-predictor-variable-1",
    "href": "lectures/06-lecture-slides.html#including-a-continuous-predictor-variable-1",
    "title": "Using Categoical Predictors in Linear Regression",
    "section": "Including a Continuous Predictor Variable",
    "text": "Including a Continuous Predictor Variable"
  },
  {
    "objectID": "lectures/06-lecture-slides.html#r-output-5",
    "href": "lectures/06-lecture-slides.html#r-output-5",
    "title": "Using Categoical Predictors in Linear Regression",
    "section": "R Output",
    "text": "R Output"
  },
  {
    "objectID": "lectures/06-lecture-slides.html#interpreting-the-coefficients-1",
    "href": "lectures/06-lecture-slides.html#interpreting-the-coefficients-1",
    "title": "Using Categoical Predictors in Linear Regression",
    "section": "Interpreting the Coefficients",
    "text": "Interpreting the Coefficients"
  },
  {
    "objectID": "lectures/06-lecture-slides.html#mean-differences-in-our-data",
    "href": "lectures/06-lecture-slides.html#mean-differences-in-our-data",
    "title": "Categorical Predictors in Linear Regression",
    "section": "Mean Differences in Our Data",
    "text": "Mean Differences in Our Data\nThe table below contains the averages for freq_use by prev_exp group:"
  },
  {
    "objectID": "lectures/06-lecture-slides.html#estimating-mean-differences-with-regression",
    "href": "lectures/06-lecture-slides.html#estimating-mean-differences-with-regression",
    "title": "Categorical Predictors in Linear Regression",
    "section": "Estimating Mean Differences with Regression",
    "text": "Estimating Mean Differences with Regression\n\nmodel &lt;- lm(freq_use ~ prev_exp_cat, data = data_lecture)\nsummary(model)\n\n\nCall:\nlm(formula = freq_use ~ prev_exp_cat, data = data_lecture)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.3600 -0.8343  0.1657  1.1657  3.1657 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      2.83429    0.06485   43.71   &lt;2e-16 ***\nprev_exp_catYes  1.52571    0.11839   12.89   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.213 on 498 degrees of freedom\nMultiple R-squared:  0.2501,    Adjusted R-squared:  0.2486 \nF-statistic: 166.1 on 1 and 498 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "lectures/06-lecture-slides.html#changing-the-reference-group",
    "href": "lectures/06-lecture-slides.html#changing-the-reference-group",
    "title": "Categorical Predictors in Linear Regression",
    "section": "Changing the Reference Group",
    "text": "Changing the Reference Group\nThe reference group is the group that is assigned the value of 0. Nothing happens to the overall model fit when you switch the reference groups, the regression coefficients will change, however:\n\n\n\nCall:\nlm(formula = freq_use ~ prev_exp_cat, data = data_lecture)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.3600 -0.8343  0.1657  1.1657  3.1657 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     4.36000    0.09906   44.02   &lt;2e-16 ***\nprev_exp_catNo -1.52571    0.11839  -12.89   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.213 on 498 degrees of freedom\nMultiple R-squared:  0.2501,    Adjusted R-squared:  0.2486 \nF-statistic: 166.1 on 1 and 498 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "lectures/06-lecture-slides.html#indicator-coding",
    "href": "lectures/06-lecture-slides.html#indicator-coding",
    "title": "Categorical Predictors in Linear Regression",
    "section": "Indicator Coding",
    "text": "Indicator Coding\nWe can use any coding scheme as long as observations in the same group are assigned the same number and these numbers are different across groups. There is, however, one coding scheme that makes the most sense for regression: Indicator Coding.\n\n\n\n\n\nPrev. Exp.\nIndicator Code\n\n\n\n\nNo\n0\n\n\nYes\n1"
  },
  {
    "objectID": "lectures/06-lecture-slides.html#including-a-quantitative-predictor",
    "href": "lectures/06-lecture-slides.html#including-a-quantitative-predictor",
    "title": "Categorical Predictors in Linear Regression",
    "section": "Including a Quantitative Predictor",
    "text": "Including a Quantitative Predictor\nYou can, and often will, include quantitative (continuous) predictor in a model that contains a categorical predictor. The interpretations of the regression intercept and slope for the categorical variable change, however.\n\nIntercept: The estimated mean of the reference group when all other predictor variables equal 0.\nSlope for Categorical Variable: The mean difference between the two groups at different levels of the predictor."
  },
  {
    "objectID": "lectures/06-lecture-slides.html#our-revised-example",
    "href": "lectures/06-lecture-slides.html#our-revised-example",
    "title": "Categorical Predictors in Linear Regression",
    "section": "Our Revised Example",
    "text": "Our Revised Example\nWe are going to assume the same data generating process as earlier, but now the categorical variable previous experience with technology has three different categories: low, moderate, or high.\n\\[Y_{\\text{Freq Use.}}=\\beta_0+\\beta_1X_{\\text{Mod. Exp.}}+ \\beta_2X_{\\text{High Exp.}}\\]"
  },
  {
    "objectID": "lectures/06-lecture-slides.html#indicator-coding-for-multicategorical-predictors",
    "href": "lectures/06-lecture-slides.html#indicator-coding-for-multicategorical-predictors",
    "title": "Categorical Predictors in Linear Regression",
    "section": "Indicator Coding for Multicategorical Predictors",
    "text": "Indicator Coding for Multicategorical Predictors\nIndicator coding for multicategorical predictors works just like indicator coding for dichotomous variables with one exception: we need multiple indicator variables.\n\n\n\n\n\nPrev. Exp.\nIndicator Code: Moderate\nIndicator Code: High\n\n\n\n\nlow\n0\n0\n\n\nmoderate\n1\n0\n\n\nhigh\n0\n1"
  },
  {
    "objectID": "lectures/06-lecture-slides.html#interpreting-regression-coefficients-for-indicator-variables",
    "href": "lectures/06-lecture-slides.html#interpreting-regression-coefficients-for-indicator-variables",
    "title": "Categorical Predictors in Linear Regression",
    "section": "Interpreting Regression Coefficients for Indicator Variables",
    "text": "Interpreting Regression Coefficients for Indicator Variables\nThe reason we use indicator coding is because it makes the regression coefficients very easy to interpret:\n\\[\\beta_0=\\overline{Y}_{\\text{Group: Low (Reference)}}\\]\n\\[\\beta_{\\text{Mod.}} = \\overline{Y}_{\\text{Group: Moderate}}-\\overline{Y}_{\\text{Group: Low}}\\]\n\\[\\beta_{\\text{High}} = \\overline{Y}_{\\text{Group: High}}-\\overline{Y}_{\\text{Group: Low}}\\]"
  },
  {
    "objectID": "lectures/06-lecture-slides.html#mean-differences-in-our-data-1",
    "href": "lectures/06-lecture-slides.html#mean-differences-in-our-data-1",
    "title": "Categorical Predictors in Linear Regression",
    "section": "Mean Differences in Our Data",
    "text": "Mean Differences in Our Data\nThe table below contains the averages for freq_use by prev_exp group:"
  },
  {
    "objectID": "lectures/06-lecture-slides.html#multiple-tests",
    "href": "lectures/06-lecture-slides.html#multiple-tests",
    "title": "Categorical Predictors in Linear Regression",
    "section": "Multiple Tests",
    "text": "Multiple Tests"
  },
  {
    "objectID": "lectures/06-lecture-slides.html#do-not-artificially-group-your-data",
    "href": "lectures/06-lecture-slides.html#do-not-artificially-group-your-data",
    "title": "Categorical Predictors in Linear Regression",
    "section": "Do Not Artificially Group Your Data",
    "text": "Do Not Artificially Group Your Data\nIf you have a continuous (quantitative) predictor variable, you should not try to transform it into a categorical predictor by creating artificial groups based on the responses to the continuous predictors unless:\n\nYou are truly interested in group differences\nResponses to the continuous variable are clustering in groups"
  },
  {
    "objectID": "lectures/06-lecture-slides.html#do-not-standardize-categorical-predictors",
    "href": "lectures/06-lecture-slides.html#do-not-standardize-categorical-predictors",
    "title": "Categorical Predictors in Linear Regression",
    "section": "Do Not Standardize Categorical Predictors",
    "text": "Do Not Standardize Categorical Predictors\nStandardized categorical predictors do not make any sense as standardizing changes the scale of the variable and the scale matters for categorical predictors!"
  },
  {
    "objectID": "lectures/06-lecture-slides.html#including-a-quantitative-predictor-variable",
    "href": "lectures/06-lecture-slides.html#including-a-quantitative-predictor-variable",
    "title": "Categorical Predictors in Linear Regression",
    "section": "Including a Quantitative Predictor Variable",
    "text": "Including a Quantitative Predictor Variable\nWhen you include a quantitative predictor variable alongside your categorical predictor, you have to interpret the regression coefficients for the categorical variable as group mean differences at a specific level of the quantitative variable or group mean differences while controlling for the effects of the other variables."
  },
  {
    "objectID": "lectures/06-lecture-slides.html#comparison-to-a-t-test",
    "href": "lectures/06-lecture-slides.html#comparison-to-a-t-test",
    "title": "Categorical Predictors in Linear Regression",
    "section": "Comparison to a T-Test",
    "text": "Comparison to a T-Test\nWhen a regression model only includes a single dichotomous predictor, it is equivalent to a Student’s T-Test—a common statistical method used to test the means of two groups are significantly different:\n\n\n\n\n\nMethod\n95% Conf. Int. Lower\n95% Conf. Int. Upper\nT Value\n\n\n\n\nT-Test\n-1.76\n-1.29\n-12.89\n\n\nRegression\n-1.76\n-1.29\n-12.89"
  },
  {
    "objectID": "lectures/06-lecture-slides.html#model-summary-freq_use-prev_exp_cat",
    "href": "lectures/06-lecture-slides.html#model-summary-freq_use-prev_exp_cat",
    "title": "Categorical Predictors in Linear Regression",
    "section": "Model Summary: freq_use ~ prev_exp_cat",
    "text": "Model Summary: freq_use ~ prev_exp_cat\n\nmodel &lt;- lm(freq_use ~ prev_exp_cat, data = data_lecture_multicat)\nsummary(model)\n\n\nCall:\nlm(formula = freq_use ~ prev_exp_cat, data = data_lecture_multicat)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -2.65  -0.76   0.24   1.24   3.24 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)           2.76000    0.06098  45.262  &lt; 2e-16 ***\nprev_exp_catmoderate  0.82667    0.11133   7.425  3.9e-13 ***\nprev_exp_cathigh      1.89000    0.12936  14.611  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.141 on 597 degrees of freedom\nMultiple R-squared:  0.2762,    Adjusted R-squared:  0.2737 \nF-statistic: 113.9 on 2 and 597 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "lectures/06-lecture-slides.html#using-the-f-test",
    "href": "lectures/06-lecture-slides.html#using-the-f-test",
    "title": "Categorical Predictors in Linear Regression",
    "section": "Using the F Test",
    "text": "Using the F Test\nRemember the F test tests the hypothesis that all regression slope coefficients are equal to 0. When your model only includes a single multicategorical predictor, this is equivalent to testing if the means across the different groups are all equal as the regression slopes are tests of group mean difference.\nA significant F means that at least one group mean is different from the other group means."
  },
  {
    "objectID": "lectures/06-lecture-slides.html#comparison-to-a-one-way-anova",
    "href": "lectures/06-lecture-slides.html#comparison-to-a-one-way-anova",
    "title": "Categorical Predictors in Linear Regression",
    "section": "Comparison to a One-Way ANOVA",
    "text": "Comparison to a One-Way ANOVA\nWhen the regression model includes only a single multicategorical predictor, it is equivalent to an ANOVA—a common statistical method used to test for mean differences across three or more groups:\n\n\n\n\n\nMethod\nF value\n\n\n\n\nANOVA\n113.89\n\n\nRegression\n113.89"
  },
  {
    "objectID": "lectures/06-lecture-slides.html#how-do-we-make-multiple-comparisons",
    "href": "lectures/06-lecture-slides.html#how-do-we-make-multiple-comparisons",
    "title": "Categorical Predictors in Linear Regression",
    "section": "How Do We Make Multiple Comparisons?",
    "text": "How Do We Make Multiple Comparisons?\nIndicator coding will only give you a subset of the possible mean comparisons you can make:\n\nModerate - Low: 0.83\nHigh - Low: 1.89\nModerate - High: ??\n\nTo get all the possible comparisons, you will need to change your reference group accordingly and refit your regression model."
  },
  {
    "objectID": "lectures/06-lecture-slides.html#the-bonferonni-correction",
    "href": "lectures/06-lecture-slides.html#the-bonferonni-correction",
    "title": "Categorical Predictors in Linear Regression",
    "section": "The Bonferonni Correction",
    "text": "The Bonferonni Correction\nWhen you start making multiple comparisons, you are essentially reusing your data over and over again, which increases the chance of finding a significant finding by chance.\nTo protect against this, you need to make it harder to find a significant result by changing the threshold at which you would have declared a finding significant to:\n\\[\\text{New Threshold}=\\frac{\\text{Old Threshold (.05)}}{\\text{# of Comparisons}}\\]"
  },
  {
    "objectID": "lectures/06-lecture-slides.html#changing-prev_exp-reference-group-to-high",
    "href": "lectures/06-lecture-slides.html#changing-prev_exp-reference-group-to-high",
    "title": "Categorical Predictors in Linear Regression",
    "section": "Changing prev_exp Reference Group to high",
    "text": "Changing prev_exp Reference Group to high\n\ndata_lecture_multicat &lt;- \n  data_lecture_multicat |&gt;\n  dplyr::mutate(\n    prev_exp_cat = relevel(prev_exp_cat, ref = \"high\")\n  )\n\nlm(freq_use ~ prev_exp_cat, data = data_lecture_multicat) |&gt; summary()\n\n\nCall:\nlm(formula = freq_use ~ prev_exp_cat, data = data_lecture_multicat)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -2.65  -0.76   0.24   1.24   3.24 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)            4.6500     0.1141   40.76  &lt; 2e-16 ***\nprev_exp_catlow       -1.8900     0.1294  -14.61  &lt; 2e-16 ***\nprev_exp_catmoderate  -1.0633     0.1473   -7.22 1.59e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.141 on 597 degrees of freedom\nMultiple R-squared:  0.2762,    Adjusted R-squared:  0.2737 \nF-statistic: 113.9 on 2 and 597 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "lectures/05-lecture-slides.html#p-values-non-directional-directional-hypotheses",
    "href": "lectures/05-lecture-slides.html#p-values-non-directional-directional-hypotheses",
    "title": "Inference for Linear Regression",
    "section": "P-Values Non-Directional & Directional Hypotheses",
    "text": "P-Values Non-Directional & Directional Hypotheses\nT-Values that fall in the shaded area allow us to reject the null hypothesis."
  },
  {
    "objectID": "lectures/05-lecture-slides.html#calculating-p-values-for-directional-hypotheses",
    "href": "lectures/05-lecture-slides.html#calculating-p-values-for-directional-hypotheses",
    "title": "Inference for Linear Regression",
    "section": "Calculating P-Values for Directional Hypotheses",
    "text": "Calculating P-Values for Directional Hypotheses\nR (and all other statistical software programs) provide the p-value for a non-directional hypothesis. It is totally fine to use this p-value even if you specify a directional hypothesis, you just need to be aware that the p-value for a directional hypothesis will always equal:\n\\[\\text{p-value}_{\\text{Directional}}=\\frac{\\text{p-value}_{\\text{Non-Directional}}}{2}\\]"
  },
  {
    "objectID": "lectures/05-lecture-slides.html#critical-regions-for-non-directional-directional-hypotheses",
    "href": "lectures/05-lecture-slides.html#critical-regions-for-non-directional-directional-hypotheses",
    "title": "Inference for Linear Regression",
    "section": "Critical Regions for Non-Directional & Directional Hypotheses",
    "text": "Critical Regions for Non-Directional & Directional Hypotheses\nThe shaded areas are called critical (or rejection) regions."
  },
  {
    "objectID": "lectures/05-lecture-page.html#lecture-categorical-predictors-in-linear-regression",
    "href": "lectures/05-lecture-page.html#lecture-categorical-predictors-in-linear-regression",
    "title": "Quantitative Analysis 1",
    "section": "Lecture: Categorical Predictors in Linear Regression",
    "text": "Lecture: Categorical Predictors in Linear Regression\n\n\nTo download a pdf version of these slides, click here."
  },
  {
    "objectID": "lectures/06-lecture-page.html",
    "href": "lectures/06-lecture-page.html",
    "title": "Quantitative Analysis 1",
    "section": "",
    "text": "Next Week’s Materials »"
  },
  {
    "objectID": "lectures/06-lecture-page.html#lecture-categorical-predictors-in-linear-regression",
    "href": "lectures/06-lecture-page.html#lecture-categorical-predictors-in-linear-regression",
    "title": "Quantitative Analysis 1",
    "section": "Lecture: Categorical Predictors in Linear Regression",
    "text": "Lecture: Categorical Predictors in Linear Regression\n\n\nTo download a pdf version of these slides, click here."
  },
  {
    "objectID": "lectures/r-training-immerision-day-1.html#objects-in-r",
    "href": "lectures/r-training-immerision-day-1.html#objects-in-r",
    "title": "An Introduction to Base R & the Tidyverse",
    "section": "Objects in R",
    "text": "Objects in R\nEverything you do in R will involve some kind of object that you have created. Think of an object like a box that you can place data in, so that R can later access and manipulate the data. An important of the code below is the assignment operator &lt;- which is how R knows to assign value to object_name.\n\nobject_name &lt;- value"
  },
  {
    "objectID": "lectures/r-training-immerision-day-1.html#atomic-vectors",
    "href": "lectures/r-training-immerision-day-1.html#atomic-vectors",
    "title": "An Introduction to Base R & the Tidyverse",
    "section": "Atomic Vectors",
    "text": "Atomic Vectors\n\nAn atomic vector is just a simple vector of data.\nR recognizes six types of atomic vectors:\n\nIntegers\nDoubles (Numeric)\nCharacters\nLogicals\nComplex\nRaw"
  },
  {
    "objectID": "lectures/r-training-immerision-day-1.html#integer-numeric-vectors",
    "href": "lectures/r-training-immerision-day-1.html#integer-numeric-vectors",
    "title": "An Introduction to Base R & the Tidyverse",
    "section": "Integer & Numeric Vectors",
    "text": "Integer & Numeric Vectors\nInteger vectors contain only integers. Add L after each number so R recognizes it as an integer. Numeric (doubles) vectors contain real numbers. These are the default vectors for numbers.\n\ninteger_vec &lt;- c(1L, 2L, 50L)\nnumeric_vec &lt;- c(1, 2, 50, 45.23)"
  },
  {
    "objectID": "lectures/r-training-immerision-day-1.html#character-vector",
    "href": "lectures/r-training-immerision-day-1.html#character-vector",
    "title": "An Introduction to Base R & the Tidyverse",
    "section": "Character Vector",
    "text": "Character Vector\nCharacter vectors contain only text data also referred to as string data. Basically anything surrounded by \"\" or '' is considered string data.\n\ncharacter_vec &lt;- c(\"1\", \"abc\", \"$#2\")"
  },
  {
    "objectID": "lectures/r-training-immerision-day-1.html#logical-vector",
    "href": "lectures/r-training-immerision-day-1.html#logical-vector",
    "title": "An Introduction to Base R & the Tidyverse",
    "section": "Logical Vector",
    "text": "Logical Vector\nLogical vectors are vectors that can only contain TRUE or FALSE values also referred to as boolean values.\n\nlogical_vec &lt;- c(TRUE, FALSE)"
  },
  {
    "objectID": "lectures/r-training-immerision-day-1.html#adding-attributes",
    "href": "lectures/r-training-immerision-day-1.html#adding-attributes",
    "title": "An Introduction to Base R & the Tidyverse",
    "section": "Adding Attributes",
    "text": "Adding Attributes\nYou can think of attributes as metadata for R objects. As a user you will not need to worry too much about attributes directly, but attributes tell R how to interact with the specific object and allow the user to store information that is secondary to the analyses they are conducting."
  },
  {
    "objectID": "lectures/r-training-immerision-day-1.html#names-attribute",
    "href": "lectures/r-training-immerision-day-1.html#names-attribute",
    "title": "An Introduction to Base R & the Tidyverse",
    "section": "names Attribute",
    "text": "names Attribute\n\ndays_of_week &lt;- 1:7 \nnames(days_of_week) &lt;- c(\"mon\", \"tues\", \"wed\", \"thurs\", \"fri\", \"sat\", \"sun\")\nnames(days_of_week)\n\n[1] \"mon\"   \"tues\"  \"wed\"   \"thurs\" \"fri\"   \"sat\"   \"sun\"  \n\nattributes(days_of_week)\n\n$names\n[1] \"mon\"   \"tues\"  \"wed\"   \"thurs\" \"fri\"   \"sat\"   \"sun\""
  },
  {
    "objectID": "lectures/r-training-immerision-day-1.html#dim-attribute",
    "href": "lectures/r-training-immerision-day-1.html#dim-attribute",
    "title": "An Introduction to Base R & the Tidyverse",
    "section": "dim Attribute",
    "text": "dim Attribute\n\ndays_of_week &lt;- 1:14\ndim(days_of_week) &lt;- c(2, 7) # 2 Rows, 7 Columns\nattributes(days_of_week)\n\n$dim\n[1] 2 7\n\nclass(days_of_week)\n\n[1] \"matrix\" \"array\""
  },
  {
    "objectID": "lectures/r-training-immerision-day-1.html#creating-factors",
    "href": "lectures/r-training-immerision-day-1.html#creating-factors",
    "title": "An Introduction to Base R & the Tidyverse",
    "section": "Creating Factors",
    "text": "Creating Factors\nR stores categorical data using factors, which are integer vectors with two attributes: class and levels.\n\ndays_of_week &lt;- factor(c(\"mon\", \"tues\", \"wed\", \"thurs\", \"fri\", \"sat\", \"sun\"))\ntypeof(days_of_week)\n\n[1] \"integer\"\n\nattributes(days_of_week)\n\n$levels\n[1] \"fri\"   \"mon\"   \"sat\"   \"sun\"   \"thurs\" \"tues\"  \"wed\"  \n\n$class\n[1] \"factor\""
  },
  {
    "objectID": "lectures/r-training-immerision-day-1.html#data-frames-best-way-to-represent-data",
    "href": "lectures/r-training-immerision-day-1.html#data-frames-best-way-to-represent-data",
    "title": "An Introduction to Base R & the Tidyverse",
    "section": "Data Frames: Best way to Represent Data",
    "text": "Data Frames: Best way to Represent Data\nData frames are the best way to structure and store data in R. Data frames are sort of the R equivalent of an excel spreadsheet.\nEach column in a data frame is a vector, so a data frame can combine a numeric vector as one column with a character vector as another column.\n\ndata_frame_1 &lt;- data.frame(NUMERIC = c(1, 3), CHARACTER = c(\"a\", \"b\"), \n                           LOGICAL = c(TRUE, FALSE))\ndata_frame_1\n\n  NUMERIC CHARACTER LOGICAL\n1       1         a    TRUE\n2       3         b   FALSE"
  },
  {
    "objectID": "lectures/r-training-immerision-day-1.html#viewing-your-data",
    "href": "lectures/r-training-immerision-day-1.html#viewing-your-data",
    "title": "An Introduction to Base R & the Tidyverse",
    "section": "Viewing Your Data",
    "text": "Viewing Your Data\nYou can use View() to open up a spreadsheet-like view of your data.\n\nView(data_frame_1)"
  },
  {
    "objectID": "lectures/r-training-immerision-day-1.html#selecting-data-from-data-frames",
    "href": "lectures/r-training-immerision-day-1.html#selecting-data-from-data-frames",
    "title": "An Introduction to Base R & the Tidyverse",
    "section": "Selecting Data from Data Frames",
    "text": "Selecting Data from Data Frames\nYou will mainly select data from data frames using one of the two following methods:\n\ndata_frame_1[1, 1] # Index the row and/or column\n\n[1] 1\n\ndata_frame_1[, 1] # Leaving the column or row index blank selects the whole vector\n\n[1] 1 3\n\ndata_frame_1$NUMERIC # Use a $ operator to reference the column name\n\n[1] 1 3"
  },
  {
    "objectID": "lectures/r-training-immerision-day-1.html#functions-in-r",
    "href": "lectures/r-training-immerision-day-1.html#functions-in-r",
    "title": "An Introduction to Base R & the Tidyverse",
    "section": "Functions in R",
    "text": "Functions in R\nFunctions are objects in R that take user inputs, apply some predefined set of operations, and return an expected output.\n\nsum(c(1, 3))\n\n[1] 4"
  },
  {
    "objectID": "lectures/r-training-immerision-day-1.html#the-elements-of-a-function",
    "href": "lectures/r-training-immerision-day-1.html#the-elements-of-a-function",
    "title": "An Introduction to Base R & the Tidyverse",
    "section": "The Elements of a Function",
    "text": "The Elements of a Function\nR comes with a variety of predefined functions and they all follow the same structure:\n\nA name for the function.\nThe arguments that change across different function calls.\nThe body which contains the code that is repeated across different calls."
  },
  {
    "objectID": "lectures/r-training-immerision-day-1.html#the-elements-of-a-function-1",
    "href": "lectures/r-training-immerision-day-1.html#the-elements-of-a-function-1",
    "title": "An Introduction to Base R & the Tidyverse",
    "section": "The Elements of a Function",
    "text": "The Elements of a Function\n\nname &lt;- function(argument) {\n  body\n}"
  },
  {
    "objectID": "lectures/r-training-immerision-day-1.html#example-base-function",
    "href": "lectures/r-training-immerision-day-1.html#example-base-function",
    "title": "An Introduction to Base R & the Tidyverse",
    "section": "Example Base Function",
    "text": "Example Base Function\n\nx &lt;- c(1, 4, 6)\nsum(x) \n\n[1] 11\n\nmean(x)\n\n[1] 3.666667\n\nmin(x)\n\n[1] 1"
  },
  {
    "objectID": "lectures/r-training-immerision-day-1.html#linking-functions-together",
    "href": "lectures/r-training-immerision-day-1.html#linking-functions-together",
    "title": "An Introduction to Base R & the Tidyverse",
    "section": "Linking Functions Together",
    "text": "Linking Functions Together\nR lets you link any number of functions together by nesting them. R will start with the innermost function and then work its way outward.\n\nsum(abs(c(-1, -1, 1, 1)))\n\n[1] 4"
  },
  {
    "objectID": "lectures/r-training-immerision-day-1.html#using-the-pipe",
    "href": "lectures/r-training-immerision-day-1.html#using-the-pipe",
    "title": "An Introduction to Base R & the Tidyverse",
    "section": "Using the pipe |>",
    "text": "Using the pipe |&gt;\nThe |&gt; operator allows you to take the output of one function and feed it directly into the first argument of the next function. Using the |&gt; makes it easier to read your code, which is a good thing.\n\nc(-1, -1, 1, 1) |&gt;\n  abs() |&gt;\n  sum()\n\n[1] 4"
  },
  {
    "objectID": "lectures/r-training-immerision-day-1.html#packages-the-lifeblood-of-r",
    "href": "lectures/r-training-immerision-day-1.html#packages-the-lifeblood-of-r",
    "title": "An Introduction to Base R & the Tidyverse",
    "section": "Packages: The Lifeblood of R",
    "text": "Packages: The Lifeblood of R\nA lot of what makes R such an effective programming language (especially for statistics) is the sheer number of available R packages. An R package is a collection of functions that complement one another for a given task. New packages are always being developed and anyone can author one!"
  },
  {
    "objectID": "lectures/r-training-immerision-day-1.html#installing-loading-packages",
    "href": "lectures/r-training-immerision-day-1.html#installing-loading-packages",
    "title": "An Introduction to Base R & the Tidyverse",
    "section": "Installing & Loading Packages",
    "text": "Installing & Loading Packages\nYou can use install.packages to install a package once and then library to load that package and gain access to all of its functions.\n\ninstall.packages(\"package_name\")\nlibrary(package_name)"
  },
  {
    "objectID": "lectures/r-training-immerision-day-1.html#reading-and-writing-data",
    "href": "lectures/r-training-immerision-day-1.html#reading-and-writing-data",
    "title": "An Introduction to Base R & the Tidyverse",
    "section": "Reading and Writing Data",
    "text": "Reading and Writing Data\nThere are a number of different methods to read and write data into R. The two most common functions are:\n\ndata &lt;- read.csv(\"filepath/file-name.csv\")\n\nwrite.csv(data, \"filepath/file-name.csv\")"
  },
  {
    "objectID": "lectures/r-training-immerision-day-1.html#importing-data-from-an-r-package",
    "href": "lectures/r-training-immerision-day-1.html#importing-data-from-an-r-package",
    "title": "An Introduction to Base R & the Tidyverse",
    "section": "Importing Data from an R Package",
    "text": "Importing Data from an R Package\nOftentimes, R packages will come with their own datasets that we can load into R. The peopleanalytics package has many such datasets that we will use today:\n\ndata_employees &lt;- peopleanalytics::employees"
  },
  {
    "objectID": "lectures/r-training-immerision-day-1.html#getting-help-with-r",
    "href": "lectures/r-training-immerision-day-1.html#getting-help-with-r",
    "title": "An Introduction to Base R & the Tidyverse",
    "section": "Getting Help with R",
    "text": "Getting Help with R\nThere are two ways to get help in R:\n\nAdd ? in front of your function, which will result in RStudio displaying the help page for that function.\nGoogle what you are trying to do. More often than not, someone else has run into your problem, found a solution, and posted it. Stand on their shoulders!\n\n\n?sum()"
  },
  {
    "objectID": "lectures/r-training-immerision-day-1.html#what-is-the-tidyverse",
    "href": "lectures/r-training-immerision-day-1.html#what-is-the-tidyverse",
    "title": "An Introduction to Base R & the Tidyverse",
    "section": "What is the Tidyverse?",
    "text": "What is the Tidyverse?\nThe tidyverse is a collection of R packages that “share a common philosophy of data and R programming and are designed to work together.”"
  },
  {
    "objectID": "lectures/r-training-immerision-day-1.html#installing-packages-from-the-tidyverse",
    "href": "lectures/r-training-immerision-day-1.html#installing-packages-from-the-tidyverse",
    "title": "An Introduction to Base R & the Tidyverse",
    "section": "Installing Packages from the Tidyverse",
    "text": "Installing Packages from the Tidyverse\n\ninstall.packages(\"tidyverse\")"
  },
  {
    "objectID": "lectures/r-training-immerision-day-1.html#tibble-data-frame-of-tidyverse",
    "href": "lectures/r-training-immerision-day-1.html#tibble-data-frame-of-tidyverse",
    "title": "An Introduction to Base R & the Tidyverse",
    "section": "tibble: Data frame of Tidyverse",
    "text": "tibble: Data frame of Tidyverse\nTibbles are the tidyverse’s version of a data.frame. They can be loaded from the tidyverse package: tibble.\n\ndata_employees_tbl &lt;- tibble::as_tibble(data_employees)\ndata_employees_tbl\n\n# A tibble: 1,470 × 36\n   employee_id active stock_opt_lvl trainings   age commute_dist ed_lvl ed_field\n         &lt;int&gt; &lt;chr&gt;          &lt;int&gt;     &lt;int&gt; &lt;int&gt;        &lt;int&gt;  &lt;int&gt; &lt;chr&gt;   \n 1        1001 No                 0         0    41            1      2 Life Sc…\n 2        1002 Yes                1         3    49            8      1 Life Sc…\n 3        1003 No                 0         3    37            2      2 Other   \n 4        1004 Yes                0         3    33            3      4 Life Sc…\n 5        1005 Yes                1         3    27            2      1 Medical \n 6        1006 Yes                0         2    32            2      2 Life Sc…\n 7        1007 Yes                3         3    59            3      3 Medical \n 8        1008 Yes                1         2    30           24      1 Life Sc…\n 9        1009 Yes                0         2    38           23      3 Life Sc…\n10        1010 Yes                2         3    36           27      3 Medical \n# ℹ 1,460 more rows\n# ℹ 28 more variables: gender &lt;chr&gt;, marital_sts &lt;chr&gt;, dept &lt;chr&gt;,\n#   engagement &lt;int&gt;, job_lvl &lt;int&gt;, job_title &lt;chr&gt;, overtime &lt;chr&gt;,\n#   business_travel &lt;chr&gt;, hourly_rate &lt;int&gt;, daily_comp &lt;int&gt;,\n#   monthly_comp &lt;int&gt;, annual_comp &lt;int&gt;, ytd_leads &lt;int&gt;, ytd_sales &lt;int&gt;,\n#   standard_hrs &lt;int&gt;, salary_hike_pct &lt;int&gt;, perf_rating &lt;int&gt;,\n#   prior_emplr_cnt &lt;int&gt;, env_sat &lt;int&gt;, job_sat &lt;int&gt;, rel_sat &lt;int&gt;, …"
  },
  {
    "objectID": "lectures/r-training-immerision-day-1.html#dplyr-your-data-multitool",
    "href": "lectures/r-training-immerision-day-1.html#dplyr-your-data-multitool",
    "title": "An Introduction to Base R & the Tidyverse",
    "section": "dplyr: Your Data Multitool",
    "text": "dplyr: Your Data Multitool\nThe package dplyr should become your go-to data manipulation and structuring tool! It contains many useful functions that make it surprisingly easy to manipulate and structure your data."
  },
  {
    "objectID": "lectures/r-training-immerision-day-1.html#the-philosophy-of-dplyr-functions",
    "href": "lectures/r-training-immerision-day-1.html#the-philosophy-of-dplyr-functions",
    "title": "An Introduction to Base R & the Tidyverse",
    "section": "The Philosophy of dplyr Functions",
    "text": "The Philosophy of dplyr Functions\nEvery function in dplyr follows this philosophy:\n\nFirst argument is always a data frame.\nRemaining arguments are usually names of columns on which to operate.\nThe output is always a new data frame (tibble).\n\ndplyr functions are also further grouped by whether they operate on rows, columns, groups, or tables."
  },
  {
    "objectID": "lectures/r-training-immerision-day-1.html#using-dplyr-to-operate-on-rows",
    "href": "lectures/r-training-immerision-day-1.html#using-dplyr-to-operate-on-rows",
    "title": "An Introduction to Base R & the Tidyverse",
    "section": "Using dplyr to Operate on Rows",
    "text": "Using dplyr to Operate on Rows\nThe following dplyr functions can filter, reduce, or reorder the rows of a data frame:\n\ndplyr::filter(data_employees_tbl, job_level %in% c(4, 5))\n\ndplyr::distinct(data_employees_tbl, ed_lvl, ed_field)\n\ndplyr::arrange(data_employees_tbl, work_exp)"
  },
  {
    "objectID": "lectures/r-training-immerision-day-1.html#using-dplyr-to-operate-on-columns",
    "href": "lectures/r-training-immerision-day-1.html#using-dplyr-to-operate-on-columns",
    "title": "An Introduction to Base R & the Tidyverse",
    "section": "Using dplyr to Operate on Columns",
    "text": "Using dplyr to Operate on Columns\nThe following dplyr functions can select, rename, add/change, or relocate the columns of a data frame:\n\ndplyr::select(data_employees_tbl, dept)\n\ndplyr::rename(data_employees_tbl, job_level = job_lvl)\n\ndplyr::mutate(data_employees_tbl, salary = monthly_comp * 12)\n\ndplyr::relocate(data_employees_tbl, job_lvl, .before = employee_id)"
  },
  {
    "objectID": "lectures/r-training-immerision-day-1.html#using-dplyr-to-operate-on-groups",
    "href": "lectures/r-training-immerision-day-1.html#using-dplyr-to-operate-on-groups",
    "title": "An Introduction to Base R & the Tidyverse",
    "section": "Using dplyr to Operate on Groups",
    "text": "Using dplyr to Operate on Groups\nThe following dplyr functions can group and summarize your data by a predefined group indicator:\n\ndata_employees_tbl |&gt;\n  dplyr::group_by(\n    job_lvl\n  ) |&gt;\n  dplyr::summarize(\n    annual_comp_mean = mean(annual_comp),\n    annual_comp_median = median(annual_comp)\n  )\n\nIn this code chunk, we have grouped by an employee’s job level and summarized their annual salary by job level."
  },
  {
    "objectID": "lectures/r-training-immerision-day-1.html#using-dplyr-to-operate-on-tables",
    "href": "lectures/r-training-immerision-day-1.html#using-dplyr-to-operate-on-tables",
    "title": "An Introduction to Base R & the Tidyverse",
    "section": "Using dplyr to Operate on Tables",
    "text": "Using dplyr to Operate on Tables\nThe followingdplyr functions can be used to join different tables (data frames) together by a unique identifier:\n\ndata_job &lt;- peopleanalytics::job |&gt; tibble::as_tibble()\n\ndata_payroll &lt;- peopleanalytics::payroll |&gt; tibble::as_tibble()\n\ndata_job_payroll &lt;- \n  data_job |&gt;\n  dplyr::left_join(\n    data_payroll,\n    by = \"employee_id\"\n  )"
  },
  {
    "objectID": "lectures/r-training-immerision-day-1.html#r-resources",
    "href": "lectures/r-training-immerision-day-1.html#r-resources",
    "title": "An Introduction to Base R & the Tidyverse",
    "section": "R Resources",
    "text": "R Resources\nhttps://r4ds.hadley.nz/"
  },
  {
    "objectID": "lectures/r-training-immersion-day-01.html#objects-in-r",
    "href": "lectures/r-training-immersion-day-01.html#objects-in-r",
    "title": "An Introduction to Base R & the Tidyverse",
    "section": "Objects in R",
    "text": "Objects in R\nEverything you do in R will involve some kind of object that you have created. Think of an object like a box that you can place data in, so that R can later access and manipulate the data. An important of the code below is the assignment operator &lt;- which is how R knows to assign value to object_name.\n\nobject_name &lt;- value"
  },
  {
    "objectID": "lectures/r-training-immersion-day-01.html#atomic-vectors",
    "href": "lectures/r-training-immersion-day-01.html#atomic-vectors",
    "title": "An Introduction to Base R & the Tidyverse",
    "section": "Atomic Vectors",
    "text": "Atomic Vectors\n\nAn atomic vector is just a simple vector of data.\nR recognizes six types of atomic vectors:\n\nIntegers\nDoubles (Numeric)\nCharacters\nLogicals\nComplex\nRaw"
  },
  {
    "objectID": "lectures/r-training-immersion-day-01.html#integer-numeric-vectors",
    "href": "lectures/r-training-immersion-day-01.html#integer-numeric-vectors",
    "title": "An Introduction to Base R & the Tidyverse",
    "section": "Integer & Numeric Vectors",
    "text": "Integer & Numeric Vectors\nInteger vectors contain only integers. Add L after each number so R recognizes it as an integer. Numeric (doubles) vectors contain real numbers. These are the default vectors for numbers.\n\ninteger_vec &lt;- c(1L, 2L, 50L)\nnumeric_vec &lt;- c(1, 2, 50, 45.23)"
  },
  {
    "objectID": "lectures/r-training-immersion-day-01.html#character-vector",
    "href": "lectures/r-training-immersion-day-01.html#character-vector",
    "title": "An Introduction to Base R & the Tidyverse",
    "section": "Character Vector",
    "text": "Character Vector\nCharacter vectors contain only text data also referred to as string data. Basically anything surrounded by \"\" or '' is considered string data.\n\ncharacter_vec &lt;- c(\"1\", \"abc\", \"$#2\")"
  },
  {
    "objectID": "lectures/r-training-immersion-day-01.html#logical-vector",
    "href": "lectures/r-training-immersion-day-01.html#logical-vector",
    "title": "An Introduction to Base R & the Tidyverse",
    "section": "Logical Vector",
    "text": "Logical Vector\nLogical vectors are vectors that can only contain TRUE or FALSE values also referred to as boolean values.\n\nlogical_vec &lt;- c(TRUE, FALSE)"
  },
  {
    "objectID": "lectures/r-training-immersion-day-01.html#adding-attributes",
    "href": "lectures/r-training-immersion-day-01.html#adding-attributes",
    "title": "An Introduction to Base R & the Tidyverse",
    "section": "Adding Attributes",
    "text": "Adding Attributes\nYou can think of attributes as metadata for R objects. As a user you will not need to worry too much about attributes directly, but attributes tell R how to interact with the specific object and allow the user to store information that is secondary to the analyses they are conducting."
  },
  {
    "objectID": "lectures/r-training-immersion-day-01.html#names-attribute",
    "href": "lectures/r-training-immersion-day-01.html#names-attribute",
    "title": "An Introduction to Base R & the Tidyverse",
    "section": "names Attribute",
    "text": "names Attribute\n\ndays_of_week &lt;- 1:7 \nnames(days_of_week) &lt;- c(\"mon\", \"tues\", \"wed\", \"thurs\", \"fri\", \"sat\", \"sun\")\nnames(days_of_week)\n\n[1] \"mon\"   \"tues\"  \"wed\"   \"thurs\" \"fri\"   \"sat\"   \"sun\"  \n\nattributes(days_of_week)\n\n$names\n[1] \"mon\"   \"tues\"  \"wed\"   \"thurs\" \"fri\"   \"sat\"   \"sun\""
  },
  {
    "objectID": "lectures/r-training-immersion-day-01.html#dim-attribute",
    "href": "lectures/r-training-immersion-day-01.html#dim-attribute",
    "title": "An Introduction to Base R & the Tidyverse",
    "section": "dim Attribute",
    "text": "dim Attribute\n\ndays_of_week &lt;- 1:14\ndim(days_of_week) &lt;- c(2, 7) # 2 Rows, 7 Columns\nattributes(days_of_week)\n\n$dim\n[1] 2 7\n\nclass(days_of_week)\n\n[1] \"matrix\" \"array\""
  },
  {
    "objectID": "lectures/r-training-immersion-day-01.html#creating-factors",
    "href": "lectures/r-training-immersion-day-01.html#creating-factors",
    "title": "An Introduction to Base R & the Tidyverse",
    "section": "Creating Factors",
    "text": "Creating Factors\nR stores categorical data using factors, which are integer vectors with two attributes: class and levels.\n\ndays_of_week &lt;- factor(c(\"mon\", \"tues\", \"wed\", \"thurs\", \"fri\", \"sat\", \"sun\"))\ntypeof(days_of_week)\n\n[1] \"integer\"\n\nattributes(days_of_week)\n\n$levels\n[1] \"fri\"   \"mon\"   \"sat\"   \"sun\"   \"thurs\" \"tues\"  \"wed\"  \n\n$class\n[1] \"factor\""
  },
  {
    "objectID": "lectures/r-training-immersion-day-01.html#data-frames-best-way-to-represent-data",
    "href": "lectures/r-training-immersion-day-01.html#data-frames-best-way-to-represent-data",
    "title": "An Introduction to Base R & the Tidyverse",
    "section": "Data Frames: Best way to Represent Data",
    "text": "Data Frames: Best way to Represent Data\nData frames are the best way to structure and store data in R. Data frames are sort of the R equivalent of an excel spreadsheet.\nEach column in a data frame is a vector, so a data frame can combine a numeric vector as one column with a character vector as another column.\n\ndata_frame_1 &lt;- data.frame(NUMERIC = c(1, 3), CHARACTER = c(\"a\", \"b\"), \n                           LOGICAL = c(TRUE, FALSE))\ndata_frame_1\n\n  NUMERIC CHARACTER LOGICAL\n1       1         a    TRUE\n2       3         b   FALSE"
  },
  {
    "objectID": "lectures/r-training-immersion-day-01.html#viewing-your-data",
    "href": "lectures/r-training-immersion-day-01.html#viewing-your-data",
    "title": "An Introduction to Base R & the Tidyverse",
    "section": "Viewing Your Data",
    "text": "Viewing Your Data\nYou can use View() to open up a spreadsheet-like view of your data.\n\nView(data_frame_1)"
  },
  {
    "objectID": "lectures/r-training-immersion-day-01.html#selecting-data-from-data-frames",
    "href": "lectures/r-training-immersion-day-01.html#selecting-data-from-data-frames",
    "title": "An Introduction to Base R & the Tidyverse",
    "section": "Selecting Data from Data Frames",
    "text": "Selecting Data from Data Frames\nYou will mainly select data from data frames using one of the two following methods:\n\ndata_frame_1[1, 1] # Index the row and/or column\n\n[1] 1\n\ndata_frame_1[, 1] # Leaving the column or row index blank selects the whole vector\n\n[1] 1 3\n\ndata_frame_1$NUMERIC # Use a $ operator to reference the column name\n\n[1] 1 3"
  },
  {
    "objectID": "lectures/r-training-immersion-day-01.html#functions-in-r",
    "href": "lectures/r-training-immersion-day-01.html#functions-in-r",
    "title": "An Introduction to Base R & the Tidyverse",
    "section": "Functions in R",
    "text": "Functions in R\nFunctions are objects in R that take user inputs, apply some predefined set of operations, and return an expected output.\n\nsum(c(1, 3))\n\n[1] 4"
  },
  {
    "objectID": "lectures/r-training-immersion-day-01.html#the-elements-of-a-function",
    "href": "lectures/r-training-immersion-day-01.html#the-elements-of-a-function",
    "title": "An Introduction to Base R & the Tidyverse",
    "section": "The Elements of a Function",
    "text": "The Elements of a Function\nR comes with a variety of predefined functions and they all follow the same structure:\n\nA name for the function.\nThe arguments that change across different function calls.\nThe body which contains the code that is repeated across different calls."
  },
  {
    "objectID": "lectures/r-training-immersion-day-01.html#the-elements-of-a-function-1",
    "href": "lectures/r-training-immersion-day-01.html#the-elements-of-a-function-1",
    "title": "An Introduction to Base R & the Tidyverse",
    "section": "The Elements of a Function",
    "text": "The Elements of a Function\n\nname &lt;- function(argument) {\n  body\n}"
  },
  {
    "objectID": "lectures/r-training-immersion-day-01.html#example-base-function",
    "href": "lectures/r-training-immersion-day-01.html#example-base-function",
    "title": "An Introduction to Base R & the Tidyverse",
    "section": "Example Base Function",
    "text": "Example Base Function\n\nx &lt;- c(1, 4, 6)\nsum(x) \n\n[1] 11\n\nmean(x)\n\n[1] 3.666667\n\nmin(x)\n\n[1] 1"
  },
  {
    "objectID": "lectures/r-training-immersion-day-01.html#linking-functions-together",
    "href": "lectures/r-training-immersion-day-01.html#linking-functions-together",
    "title": "An Introduction to Base R & the Tidyverse",
    "section": "Linking Functions Together",
    "text": "Linking Functions Together\nR lets you link any number of functions together by nesting them. R will start with the innermost function and then work its way outward.\n\nsum(abs(c(-1, -1, 1, 1)))\n\n[1] 4"
  },
  {
    "objectID": "lectures/r-training-immersion-day-01.html#using-the-pipe",
    "href": "lectures/r-training-immersion-day-01.html#using-the-pipe",
    "title": "An Introduction to Base R & the Tidyverse",
    "section": "Using the pipe |>",
    "text": "Using the pipe |&gt;\nThe |&gt; operator allows you to take the output of one function and feed it directly into the first argument of the next function. Using the |&gt; makes it easier to read your code, which is a good thing.\n\nc(-1, -1, 1, 1) |&gt;\n  abs() |&gt;\n  sum()\n\n[1] 4"
  },
  {
    "objectID": "lectures/r-training-immersion-day-01.html#packages-the-lifeblood-of-r",
    "href": "lectures/r-training-immersion-day-01.html#packages-the-lifeblood-of-r",
    "title": "An Introduction to Base R & the Tidyverse",
    "section": "Packages: The Lifeblood of R",
    "text": "Packages: The Lifeblood of R\nA lot of what makes R such an effective programming language (especially for statistics) is the sheer number of available R packages. An R package is a collection of functions that complement one another for a given task. New packages are always being developed and anyone can author one!"
  },
  {
    "objectID": "lectures/r-training-immersion-day-01.html#installing-loading-packages",
    "href": "lectures/r-training-immersion-day-01.html#installing-loading-packages",
    "title": "An Introduction to Base R & the Tidyverse",
    "section": "Installing & Loading Packages",
    "text": "Installing & Loading Packages\nYou can use install.packages to install a package once and then library to load that package and gain access to all of its functions.\n\ninstall.packages(\"package_name\")\nlibrary(package_name)"
  },
  {
    "objectID": "lectures/r-training-immersion-day-01.html#reading-and-writing-data",
    "href": "lectures/r-training-immersion-day-01.html#reading-and-writing-data",
    "title": "An Introduction to Base R & the Tidyverse",
    "section": "Reading and Writing Data",
    "text": "Reading and Writing Data\nThere are a number of different methods to read and write data into R. The two most common functions are:\n\ndata &lt;- read.csv(\"filepath/file-name.csv\")\n\nwrite.csv(data, \"filepath/file-name.csv\")"
  },
  {
    "objectID": "lectures/r-training-immersion-day-01.html#importing-data-from-an-r-package",
    "href": "lectures/r-training-immersion-day-01.html#importing-data-from-an-r-package",
    "title": "An Introduction to Base R & the Tidyverse",
    "section": "Importing Data from an R Package",
    "text": "Importing Data from an R Package\nOftentimes, R packages will come with their own datasets that we can load into R. The peopleanalytics package has many such datasets that we will use today:\n\ndata_employees &lt;- peopleanalytics::employees"
  },
  {
    "objectID": "lectures/r-training-immersion-day-01.html#getting-help-with-r",
    "href": "lectures/r-training-immersion-day-01.html#getting-help-with-r",
    "title": "An Introduction to Base R & the Tidyverse",
    "section": "Getting Help with R",
    "text": "Getting Help with R\nThere are two ways to get help in R:\n\nAdd ? in front of your function, which will result in RStudio displaying the help page for that function.\nGoogle what you are trying to do. More often than not, someone else has run into your problem, found a solution, and posted it. Stand on their shoulders!\n\n\n?sum()"
  },
  {
    "objectID": "lectures/r-training-immersion-day-01.html#what-is-the-tidyverse",
    "href": "lectures/r-training-immersion-day-01.html#what-is-the-tidyverse",
    "title": "An Introduction to Base R & the Tidyverse",
    "section": "What is the Tidyverse?",
    "text": "What is the Tidyverse?\nThe tidyverse is a collection of R packages that “share a common philosophy of data and R programming and are designed to work together.”"
  },
  {
    "objectID": "lectures/r-training-immersion-day-01.html#installing-packages-from-the-tidyverse",
    "href": "lectures/r-training-immersion-day-01.html#installing-packages-from-the-tidyverse",
    "title": "An Introduction to Base R & the Tidyverse",
    "section": "Installing Packages from the Tidyverse",
    "text": "Installing Packages from the Tidyverse\n\ninstall.packages(\"tidyverse\")"
  },
  {
    "objectID": "lectures/r-training-immersion-day-01.html#tibble-data-frame-of-tidyverse",
    "href": "lectures/r-training-immersion-day-01.html#tibble-data-frame-of-tidyverse",
    "title": "An Introduction to Base R & the Tidyverse",
    "section": "tibble: Data frame of Tidyverse",
    "text": "tibble: Data frame of Tidyverse\nTibbles are the tidyverse’s version of a data.frame. They can be loaded from the tidyverse package: tibble.\n\ndata_employees_tbl &lt;- tibble::as_tibble(data_employees)\ndata_employees_tbl\n\n# A tibble: 1,470 × 36\n   employee_id active stock_opt_lvl trainings   age commute_dist ed_lvl ed_field\n         &lt;int&gt; &lt;chr&gt;          &lt;int&gt;     &lt;int&gt; &lt;int&gt;        &lt;int&gt;  &lt;int&gt; &lt;chr&gt;   \n 1        1001 No                 0         0    41            1      2 Life Sc…\n 2        1002 Yes                1         3    49            8      1 Life Sc…\n 3        1003 No                 0         3    37            2      2 Other   \n 4        1004 Yes                0         3    33            3      4 Life Sc…\n 5        1005 Yes                1         3    27            2      1 Medical \n 6        1006 Yes                0         2    32            2      2 Life Sc…\n 7        1007 Yes                3         3    59            3      3 Medical \n 8        1008 Yes                1         2    30           24      1 Life Sc…\n 9        1009 Yes                0         2    38           23      3 Life Sc…\n10        1010 Yes                2         3    36           27      3 Medical \n# ℹ 1,460 more rows\n# ℹ 28 more variables: gender &lt;chr&gt;, marital_sts &lt;chr&gt;, dept &lt;chr&gt;,\n#   engagement &lt;int&gt;, job_lvl &lt;int&gt;, job_title &lt;chr&gt;, overtime &lt;chr&gt;,\n#   business_travel &lt;chr&gt;, hourly_rate &lt;int&gt;, daily_comp &lt;int&gt;,\n#   monthly_comp &lt;int&gt;, annual_comp &lt;int&gt;, ytd_leads &lt;int&gt;, ytd_sales &lt;int&gt;,\n#   standard_hrs &lt;int&gt;, salary_hike_pct &lt;int&gt;, perf_rating &lt;int&gt;,\n#   prior_emplr_cnt &lt;int&gt;, env_sat &lt;int&gt;, job_sat &lt;int&gt;, rel_sat &lt;int&gt;, …"
  },
  {
    "objectID": "lectures/r-training-immersion-day-01.html#dplyr-your-data-multitool",
    "href": "lectures/r-training-immersion-day-01.html#dplyr-your-data-multitool",
    "title": "An Introduction to Base R & the Tidyverse",
    "section": "dplyr: Your Data Multitool",
    "text": "dplyr: Your Data Multitool\nThe package dplyr should become your go-to data manipulation and structuring tool! It contains many useful functions that make it surprisingly easy to manipulate and structure your data."
  },
  {
    "objectID": "lectures/r-training-immersion-day-01.html#the-philosophy-of-dplyr-functions",
    "href": "lectures/r-training-immersion-day-01.html#the-philosophy-of-dplyr-functions",
    "title": "An Introduction to Base R & the Tidyverse",
    "section": "The Philosophy of dplyr Functions",
    "text": "The Philosophy of dplyr Functions\nEvery function in dplyr follows this philosophy:\n\nFirst argument is always a data frame.\nRemaining arguments are usually names of columns on which to operate.\nThe output is always a new data frame (tibble).\n\ndplyr functions are also further grouped by whether they operate on rows, columns, groups, or tables."
  },
  {
    "objectID": "lectures/r-training-immersion-day-01.html#using-dplyr-to-operate-on-rows",
    "href": "lectures/r-training-immersion-day-01.html#using-dplyr-to-operate-on-rows",
    "title": "An Introduction to Base R & the Tidyverse",
    "section": "Using dplyr to Operate on Rows",
    "text": "Using dplyr to Operate on Rows\nThe following dplyr functions can filter, reduce, or reorder the rows of a data frame:\n\ndplyr::filter(data_employees_tbl, job_level %in% c(4, 5))\n\ndplyr::distinct(data_employees_tbl, ed_lvl, ed_field)\n\ndplyr::arrange(data_employees_tbl, work_exp)"
  },
  {
    "objectID": "lectures/r-training-immersion-day-01.html#using-dplyr-to-operate-on-columns",
    "href": "lectures/r-training-immersion-day-01.html#using-dplyr-to-operate-on-columns",
    "title": "An Introduction to Base R & the Tidyverse",
    "section": "Using dplyr to Operate on Columns",
    "text": "Using dplyr to Operate on Columns\nThe following dplyr functions can select, rename, add/change, or relocate the columns of a data frame:\n\ndplyr::select(data_employees_tbl, dept)\n\ndplyr::rename(data_employees_tbl, job_level = job_lvl)\n\ndplyr::mutate(data_employees_tbl, salary = monthly_comp * 12)\n\ndplyr::relocate(data_employees_tbl, job_lvl, .before = employee_id)"
  },
  {
    "objectID": "lectures/r-training-immersion-day-01.html#using-dplyr-to-operate-on-groups",
    "href": "lectures/r-training-immersion-day-01.html#using-dplyr-to-operate-on-groups",
    "title": "An Introduction to Base R & the Tidyverse",
    "section": "Using dplyr to Operate on Groups",
    "text": "Using dplyr to Operate on Groups\nThe following dplyr functions can group and summarize your data by a predefined group indicator:\n\ndata_employees_tbl |&gt;\n  dplyr::group_by(\n    job_lvl\n  ) |&gt;\n  dplyr::summarize(\n    annual_comp_mean = mean(annual_comp),\n    annual_comp_median = median(annual_comp)\n  )\n\nIn this code chunk, we have grouped by an employee’s job level and summarized their annual salary by job level."
  },
  {
    "objectID": "lectures/r-training-immersion-day-01.html#using-dplyr-to-operate-on-tables",
    "href": "lectures/r-training-immersion-day-01.html#using-dplyr-to-operate-on-tables",
    "title": "An Introduction to Base R & the Tidyverse",
    "section": "Using dplyr to Operate on Tables",
    "text": "Using dplyr to Operate on Tables\nThe followingdplyr functions can be used to join different tables (data frames) together by a unique identifier:\n\ndata_job &lt;- peopleanalytics::job |&gt; tibble::as_tibble()\n\ndata_payroll &lt;- peopleanalytics::payroll |&gt; tibble::as_tibble()\n\ndata_job_payroll &lt;- \n  data_job |&gt;\n  dplyr::left_join(\n    data_payroll,\n    by = \"employee_id\"\n  )"
  },
  {
    "objectID": "lectures/r-training-immersion-day-01.html#r-resources",
    "href": "lectures/r-training-immersion-day-01.html#r-resources",
    "title": "An Introduction to Base R & the Tidyverse",
    "section": "R Resources",
    "text": "R Resources\nhttps://r4ds.hadley.nz/"
  },
  {
    "objectID": "lectures/05-lecture-page.html#lecture-introduction-to-base-r-and-the-tidyverse",
    "href": "lectures/05-lecture-page.html#lecture-introduction-to-base-r-and-the-tidyverse",
    "title": "Quantitative Analysis 1",
    "section": "Lecture: Introduction to Base R and the Tidyverse",
    "text": "Lecture: Introduction to Base R and the Tidyverse\n\n\nTo download a pdf version of these slides, click here.\nTo download the R script that follows the R portion of the lecture, click here."
  },
  {
    "objectID": "assignments/02-assignment-page.html",
    "href": "assignments/02-assignment-page.html",
    "title": "\nAssignment 2: Statistical Inference & Categorical Predictors\n",
    "section": "",
    "text": "Assignment 2: Statistical Inference & Categorical Predictors\n\n\nInstructions\n\nIn this assignment you will practice some of the statistical methods we have talked about so far: multiple regression with categorical predictors and a t-test. This assignment will be due on or before 10-09-2023.\nFor R users:\n\nDownload the assignment R script, which can be found below.\nOpen the R script in RStudio.\nBegin the assignment.\nFor the questions that require a written answer, save your answer either as a comment directly in the R script or in a separate Word document (or something similar).\nUpload your completed R script and any other files you created, such as a Word document, into the class Brightspace.\n\nFor non-R users:\n\nDownload the assignment R script, which can be found below.\nDownload the assignment dataset.\nOpen the R script in a generic text editor as it contains the assignment questions.\nFor the questions that require a written answer, save your answers in a separate Word document (or something similar).\nSave the script you used to complete the assignment and its subsequent output as a Word or text file.\nUpload your script, output, and written answers into the class Brightspace.\n\nIf you run into any issues, please email me.\n\nMaterials\n\nAssignment R Script: Download\nAssignment Data: Download\n\nData Description\n\nYour organization is planning a wide-scale release of a new generative AI chatbot designed to support your organization’s sales force. Among other features the AI chatbot can find answers to the customer’s question, identifying new promotions tailored to the customer, and generally help the sales representative meet the customer’s demands.\nYour organization, however, is not sure if the sales representatives will begin using the new AI technology, so they have asked you design a survey study on a sample of the organization’s sales representatives to understand what predicts a sale’s representative’s intentions to use the new AI tool. You ultimately decide to measure four variables with your survey: The AI tool’s perceived ease of use (perceived_ease_use), the AI tool’s perceived usefulness for the sales job (perceived_useful), the previous experience a sales represntative has had with an AI-enhanced sales tool (previous_exp) and the sales rep’s intentions to use the new AI tool (behavioral_intention).\nThe dataset for this assignment contains the following variables:\n\nemployee_id: A unique 6-digit employee identifier.\nperceived_ease_use: Responses to the question: “I find the AI tool easy to use.”\nperceived_useful: Responses to the question: “I believe the AI tool is useful for my job.”\nprevious_exp: Responses to the question: “I have had previous experience with an AI-enhanced sales tool.”\nbehavioral_intention: Responses to the question: “I plan to use the system in the next month.”\n\nThe three survey questions, perceived_ease_use, perceived_useful, and behavioral_intention, are on a seven point agreement response scale:\n\nCompletely Disagree\nStrongly Disagree\nDisagree\nNeither Disagree, Nor Agree\nAgree\nStrongly Agree\nCompletely Agree\n\nThe survey question, previous_exp is a dichotomous variables with responses being either:\n\nyes\nno"
  },
  {
    "objectID": "lectures/05-lecture-page.html#schedule-for-today",
    "href": "lectures/05-lecture-page.html#schedule-for-today",
    "title": "Quantitative Analysis 1",
    "section": "Schedule for Today",
    "text": "Schedule for Today\n\n\n\n\n\n\n\nTopic\nTime\n\n\n\n\nStatistical Inference\n9 to 10:15 AM\n\n\nBreak\n10:15 to 10:30 AM\n\n\nStatistical Inference\n10:30 to 11:45 AM\n\n\nBreak for Lunch\n11:45 AM to 1 PM\n\n\nIntro to Categorical Predictors\n1 to 2 PM\n\n\nBreak\n2 to 2:15 PM\n\n\nIntro to Base R and Tidyverse\n2:15 to 3:15 PM\n\n\nBreak\n3:15 to 3:30 PM\n\n\nHeadstart on R Coding Assignment\n3:30 to 4 PM"
  },
  {
    "objectID": "assignments/02-assignment-answers.html",
    "href": "assignments/02-assignment-answers.html",
    "title": "Answers to Assignment 2",
    "section": "",
    "text": "library(\"tidyverse\")\n\n# Read in the data from the class site\ndata_ai &lt;- readr::read_csv(\"https://alopilato88.github.io/quantitative-analysis-1/assignments/02-assignment-data.csv\")"
  },
  {
    "objectID": "assignments/02-assignment-answers.html#assignment-setup",
    "href": "assignments/02-assignment-answers.html#assignment-setup",
    "title": "Answers to Assignment 2",
    "section": "",
    "text": "library(\"tidyverse\")\n\n# Read in the data from the class site\ndata_ai &lt;- readr::read_csv(\"https://alopilato88.github.io/quantitative-analysis-1/assignments/02-assignment-data.csv\")"
  },
  {
    "objectID": "assignments/02-assignment-answers.html#question-1",
    "href": "assignments/02-assignment-answers.html#question-1",
    "title": "Answers to Assignment 2",
    "section": "Question 1",
    "text": "Question 1\n\n# 1. What is the sample size of your dataset (hint: It's the number of rows)? \n\nnrow(data_ai)\n\n[1] 500\n\n\nThere are 500 survey respondents in my dataset."
  },
  {
    "objectID": "assignments/02-assignment-answers.html#question-2",
    "href": "assignments/02-assignment-answers.html#question-2",
    "title": "Answers to Assignment 2",
    "section": "Question 2",
    "text": "Question 2\n\n# 2. How many variables are in your dataset (hint: It's the number of columns)? \n\nncol(data_ai)\n\n[1] 5\n\n\nThere are 5 variables in my dataset."
  },
  {
    "objectID": "assignments/02-assignment-answers.html#question-3",
    "href": "assignments/02-assignment-answers.html#question-3",
    "title": "Answers to Assignment 2",
    "section": "Question 3",
    "text": "Question 3\n\n3A\n\n# 3a. Use the functions summarize (from dplyr), mean, and sd to calculate means and sds for perceived_ease_use and perceived_useful. (Some of the code has been started.)\n\ndata_ai |&gt;\nsummarize(\n  mean_perceived_ease_use = mean(perceived_ease_use),\n  sd_perceived_ease_use = sd(perceived_ease_use),\n  mean_perceived_useful = mean(perceived_useful),\n  sd_perceived_useful = sd(perceived_useful)\n)\n\n# A tibble: 1 × 4\n  mean_perceived_ease_use sd_perceived_ease_use mean_perceived_useful\n                    &lt;dbl&gt;                 &lt;dbl&gt;                 &lt;dbl&gt;\n1                    4.08                  1.66                  4.76\n# ℹ 1 more variable: sd_perceived_useful &lt;dbl&gt;\n\n\n\n\n3B\n\n# 3b. Describe what the |&gt; operator is doing in the code above.\n\nThe pipe operator, |&gt;, is taking the results from one line of code and plugging them into the function on the following line of code."
  },
  {
    "objectID": "assignments/02-assignment-answers.html#question-4",
    "href": "assignments/02-assignment-answers.html#question-4",
    "title": "Answers to Assignment 2",
    "section": "Question 4",
    "text": "Question 4\n\n4A\n\n# 4a. Estimate a multiple regression model that uses perceived_useful and perceived_ease_use to predict behavioral_intention. Save the model in an object named: model_1\n\nmodel_1 &lt;- lm(behavioral_intention ~ perceived_useful + perceived_ease_use, data = data_ai)\n\n\n\n4B\n\n# 4b. Use the summary function to print out the results of your model and write out: \n\n## 4b1: What is the null hypothesis being tested for both perceived_useful & perceived_ease_use? \n\n## 4b2: Can you reject the null hypothesis? If yes, why? If no, why? \n\nsummary(model_1)\n\n\nCall:\nlm(formula = behavioral_intention ~ perceived_useful + perceived_ease_use, \n    data = data_ai)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.7330 -0.9235 -0.0084  0.9357  3.5256 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         2.25480    0.19503  11.562  &lt; 2e-16 ***\nperceived_useful    0.19663    0.04242   4.636 4.55e-06 ***\nperceived_ease_use  0.27544    0.04480   6.148 1.61e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.397 on 497 degrees of freedom\nMultiple R-squared:  0.2045,    Adjusted R-squared:  0.2013 \nF-statistic:  63.9 on 2 and 497 DF,  p-value: &lt; 2.2e-16\n\n\nFor both perceived_useful and perceived_ease_use, we are testing the null hypothesis that the regression slope for each predictor variable is equal to 0. That is, we are testing the null hypothesis that there is no relationship between perceived_useful and behvioral_intention and perceived_ease_use and behavioral_intention.\nFor both perceived_useful and perceived_ease_use, we can reject the null hypothesis because their p-values (Pr&gt;|t|) are less than or equal to .05. This means that if there was truly no relationship between perceived_useful and behavioral_intention, then we would expect to see a value of 0.2 or larger less than 5% of the time. Because the p-value is so small, we feel safe rejecting the null hypothesis that states there is no relationship.\n\n\n4C\n\n# 4c. Calculate the confidence intervals for all three regression coefficients (intercept and two slopes) in model_1.\n\nconfint(model_1) |&gt; round(3)\n\n                   2.5 % 97.5 %\n(Intercept)        1.872  2.638\nperceived_useful   0.113  0.280\nperceived_ease_use 0.187  0.363\n\n\n\n\n4D\n\n# 4d. What information does the confidence interval for perceived_useful give you? \n\nThe confidence interval for perceived_useful tells us that we can be 95% confident that the true (population) value for perceived_useful’s regression slope is somewhere between 0.113 and 0.28.\nSaid differently, our data would not allow us to reject a null hypothesis that hypothesized the true value for perceived_useful’s regression slope was equal to a value contained in the confidence interval: 0.113 to 0.28."
  },
  {
    "objectID": "assignments/02-assignment-answers.html#question-5",
    "href": "assignments/02-assignment-answers.html#question-5",
    "title": "Answers to Assignment 2",
    "section": "Question 5",
    "text": "Question 5\n\n# 5. Finish the code below to create standardized variables for perceived_ease_use and behavioral_intention. \n#    Name the standardized variables: perceived_ease_use_stand and behavioral_intention_stand.\n\ndata_ai &lt;- \n  data_ai |&gt;\n  mutate(\n    perceived_useful_stand = (perceived_useful - mean(perceived_useful)) / sd(perceived_useful),\n    perceived_ease_use_stand = (perceived_ease_use - mean(perceived_ease_use)) / sd(perceived_ease_use),\n    behavioral_intention_stand = (behavioral_intention - mean(behavioral_intention)) / sd(behavioral_intention),\n  )"
  },
  {
    "objectID": "assignments/02-assignment-answers.html#question-6",
    "href": "assignments/02-assignment-answers.html#question-6",
    "title": "Answers to Assignment 2",
    "section": "Question 6",
    "text": "Question 6\n\n6A\n\n# 6a. Estimate a multiple regression model that uses perceived_useful_stand and perceived_ease_use_stand to predict behavioral_intention_stand. \n#     Save the model in an object named: model_2\n\nmodel_2 &lt;- lm(behavioral_intention_stand ~ perceived_useful_stand + perceived_ease_use_stand, data = data_ai)\n\n\n\n6B\n\n# 6b. Use the summary function to print out the results of your model and write out: \n\nsummary(model_2)\n\n\nCall:\nlm(formula = behavioral_intention_stand ~ perceived_useful_stand + \n    perceived_ease_use_stand, data = data_ai)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.38769 -0.59069 -0.00538  0.59851  2.25507 \n\nCoefficients:\n                           Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)              -2.920e-17  3.997e-02   0.000        1    \nperceived_useful_stand    2.207e-01  4.762e-02   4.636 4.55e-06 ***\nperceived_ease_use_stand  2.928e-01  4.762e-02   6.148 1.61e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8937 on 497 degrees of freedom\nMultiple R-squared:  0.2045,    Adjusted R-squared:  0.2013 \nF-statistic:  63.9 on 2 and 497 DF,  p-value: &lt; 2.2e-16\n\n## 6b1: Do your conclusions about the significance of perceived_useful and perceived_ease_use change compared to the conclusions you made from model_1? \n\n## 6b2: Are the t values in model_2 for the regression coefficients (intercept and slopes) different from model_1? If so, which ones are different?\n\n## 6b3: Is the model_2 multiple R-squared value different than the model_1 multiple R-squared value? \n\nNone of my conclusions about the significance of either predictor variable (perceived_useful and perceived_ease_use) change because the t-values for perceived_useful and perceived_ease_use are identical across model_1 and model_2. Similarly, the R2 is identical across model_1 and model_2. Thus, standardizing our variables had no effect on the statistical conclusions we make from our models."
  },
  {
    "objectID": "assignments/02-assignment-answers.html#question-7",
    "href": "assignments/02-assignment-answers.html#question-7",
    "title": "Answers to Assignment 2",
    "section": "Question 7",
    "text": "Question 7\n\n7A\n\n# 7a. Use the summarize function to calculate means for behavioral_intention when previous_exp = Yes and previous_exp = No: \n\ndata_ai |&gt;\n  group_by(\n    previous_exp\n  ) |&gt;\n  summarize(\n    mean_behavioral_intention = mean(behavioral_intention)\n  )\n\n# A tibble: 2 × 2\n  previous_exp mean_behavioral_intention\n  &lt;chr&gt;                            &lt;dbl&gt;\n1 no                                4.35\n2 yes                               4.22\n\n\n\n\n7B\n\n# 7b. Add-on to code above to calculate both the mean and sd for behavioral_intention when previous_exp = Yes and previous_exp = No:\ndata_ai |&gt;\n  group_by(\n    previous_exp\n  ) |&gt;\n  summarize(\n    mean_behavioral_intention = mean(behavioral_intention),\n    sd_behavioral_intention = sd(behavioral_intention)\n  )\n\n# A tibble: 2 × 3\n  previous_exp mean_behavioral_intention sd_behavioral_intention\n  &lt;chr&gt;                            &lt;dbl&gt;                   &lt;dbl&gt;\n1 no                                4.35                    1.39\n2 yes                               4.22                    1.94\n\n\n\n\n7C\n\n# 7c. Do the group means look different from one another? \n\nWhile they group means are different from one another (4.35 vs 4.22), this difference is small and unlikely to be significant."
  },
  {
    "objectID": "assignments/02-assignment-answers.html#question-8",
    "href": "assignments/02-assignment-answers.html#question-8",
    "title": "Answers to Assignment 2",
    "section": "Question 8",
    "text": "Question 8\n\n# 8. Finish the function below to create an indicator variable from previous_exp:\n\ndata_ai &lt;-\n  data_ai |&gt;\n  dplyr::mutate(\n    previous_exp_ind = if_else(previous_exp == \"no\", 1, 0)\n  )"
  },
  {
    "objectID": "assignments/02-assignment-answers.html#question-9",
    "href": "assignments/02-assignment-answers.html#question-9",
    "title": "Answers to Assignment 2",
    "section": "Question 9",
    "text": "Question 9\n\n9A\n\n# 9a. Estimate a simple regression model using previous_exp_ind to predict behavioral_intention. Call it model_3.\n\nmodel_3 &lt;- lm(behavioral_intention ~ previous_exp_ind, data = data_ai)\n\n\n\n9B\n\n# 9b. Interpret the regression coefficient for previous_exp_ind. What is it telling you? \n\nsummary(model_3)\n\n\nCall:\nlm(formula = behavioral_intention ~ previous_exp_ind, data = data_ai)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.3518 -1.2158 -0.2158  0.6482  2.7842 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        4.2158     0.1326  31.784   &lt;2e-16 ***\nprevious_exp_ind   0.1360     0.1561   0.871    0.384    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.564 on 498 degrees of freedom\nMultiple R-squared:  0.001521,  Adjusted R-squared:  -0.0004837 \nF-statistic: 0.7587 on 1 and 498 DF,  p-value: 0.3841\n\n\nThe regression coefficient (slope) for pervious_exp_ind tells me that the the mean behavioral_intention for respondents with no experience is 0.136 units greater than the mean for respondents who have previous experience.\n\n\n9C\n\n# 9c. What null hypothesis is being tested by previous_exp_ind? Can we reject the null hypothesis? Why or why not?\n\nThe null hypothesis being tested is that the regression slope for previous_exp_ind is equal to 0, which is equivalent to saying that the mean difference on behavioral_intention between the yes and no group is equal to 0. We cannot reject the null hypothesis because the p-value is greater than .05."
  },
  {
    "objectID": "assignments/02-assignment-answers.html#question-10",
    "href": "assignments/02-assignment-answers.html#question-10",
    "title": "Answers to Assignment 2",
    "section": "Question 10",
    "text": "Question 10\n\n# 10. Estimate a simple regression model using previous_exp (NOT previous_exp_ind) to predict behavioral_intention. Call it model_4.\n#     Is anything different about model_4 compared to model_3? (Hint: Is the reference category for previous_exp different from previous_exp_ind?)\n\nmodel_4 &lt;- lm(behavioral_intention ~ previous_exp, data = data_ai)\n\nsummary(model_4)\n\n\nCall:\nlm(formula = behavioral_intention ~ previous_exp, data = data_ai)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.3518 -1.2158 -0.2158  0.6482  2.7842 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      4.35180    0.08231  52.874   &lt;2e-16 ***\nprevious_expyes -0.13597    0.15610  -0.871    0.384    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.564 on 498 degrees of freedom\nMultiple R-squared:  0.001521,  Adjusted R-squared:  -0.0004837 \nF-statistic: 0.7587 on 1 and 498 DF,  p-value: 0.3841\n\n\nIn model_4, the reference category for previous_exp is yes whereas for model_3 it is no. Thus, the the sign of the regression coefficient is flipped across models (positive in model_3 and negative in model_4), but the absolute magnitude of the regression coefficient is identical across models."
  },
  {
    "objectID": "assignments/02-assignment-answers.html#bonus",
    "href": "assignments/02-assignment-answers.html#bonus",
    "title": "Answers to Assignment 2",
    "section": "Bonus",
    "text": "Bonus\n\n# Create two new R objects: \n#   behavioral_intention_exp_yes, which contains only behavioral_intention responses for the group where previous_exp == \"yes\"\n#   behavioral_intention_exp_no, which contains only the behavioral_intention responses for the group where previous_exp == \"no\".\n#\n# Estimate a t-test to test if the means of the new objects are different. Some of the code is started below:\n\nbehavioral_intention_exp_yes &lt;- data_ai$behavioral_intention[data_ai$previous_exp == \"yes\"]\nbehavioral_intention_exp_no &lt;- data_ai$behavioral_intention[data_ai$previous_exp == \"no\"]\n\n# Finish this line of code. Hint: What variable is it missing? \nt_test_result &lt;- t.test(behavioral_intention_exp_yes, behavioral_intention_exp_no, var.equal = TRUE)\n\n# Compare the results of the t-test to those from model_4. Are the t-values and degrees of freedom different? How about the p-value? \nt_test_result\n\n\n    Two Sample t-test\n\ndata:  behavioral_intention_exp_yes and behavioral_intention_exp_no\nt = -0.87106, df = 498, p-value = 0.3841\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.4426709  0.1707244\nsample estimates:\nmean of x mean of y \n 4.215827  4.351801 \n\nsummary(model_4)\n\n\nCall:\nlm(formula = behavioral_intention ~ previous_exp, data = data_ai)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.3518 -1.2158 -0.2158  0.6482  2.7842 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      4.35180    0.08231  52.874   &lt;2e-16 ***\nprevious_expyes -0.13597    0.15610  -0.871    0.384    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.564 on 498 degrees of freedom\nMultiple R-squared:  0.001521,  Adjusted R-squared:  -0.0004837 \nF-statistic: 0.7587 on 1 and 498 DF,  p-value: 0.3841\n\n\nThe results across these different models are identical. The t-values (-.871) and degrees of freedom (df = 498) are identical across models, which means the p-values will be identical as well. This tells us that a t-test of mean differences is a special case of linear regression where only a single, binary predictor variable is used to explain variation in the outcome variable."
  },
  {
    "objectID": "lectures/07-lecture-slides.html#overview",
    "href": "lectures/07-lecture-slides.html#overview",
    "title": "Model Building",
    "section": "Overview",
    "text": "Overview\n\nDifference between a theory and a model\nExplore the different relationship types conveyed by a model\nBuild and communicate a path diagram\nReview a published article"
  },
  {
    "objectID": "lectures/07-lecture-slides.html#goals",
    "href": "lectures/07-lecture-slides.html#goals",
    "title": "Model Building",
    "section": "Goals",
    "text": "Goals"
  },
  {
    "objectID": "lectures/07-lecture-slides.html#what-is-a-theory",
    "href": "lectures/07-lecture-slides.html#what-is-a-theory",
    "title": "Model Building",
    "section": "What is a Theory",
    "text": "What is a Theory\n\nA theory is a set of concepts whose proposed relationships offer explanation, understanding, or appreciation of a phenomenon of interest.\n\n— Hatch, 2013, p. 5"
  },
  {
    "objectID": "lectures/07-lecture-slides.html#what-is-a-model",
    "href": "lectures/07-lecture-slides.html#what-is-a-model",
    "title": "Model Building",
    "section": "What is a Model",
    "text": "What is a Model\nModels are derived from theories to test a particular aspect of that theory. Models are useful because they allow us to communicate and test simplified pieces of our theory, thus providing a more local understanding of our phenomenon.\n\n\nFried (2020). Theories and models: What they are, what they are for, and what they are about. Psychological Inquiry, 31(4), 336-334."
  },
  {
    "objectID": "lectures/07-lecture-slides.html#using-models-to-test-theories",
    "href": "lectures/07-lecture-slides.html#using-models-to-test-theories",
    "title": "Model Building",
    "section": "Using Models to Test Theories",
    "text": "Using Models to Test Theories"
  },
  {
    "objectID": "lectures/07-lecture-slides.html#visualizing-models-with-a-path-diagram",
    "href": "lectures/07-lecture-slides.html#visualizing-models-with-a-path-diagram",
    "title": "Model Building",
    "section": "Visualizing Models with a Path Diagram",
    "text": "Visualizing Models with a Path Diagram"
  },
  {
    "objectID": "lectures/07-lecture-slides.html#types-of-relationships-among-variables",
    "href": "lectures/07-lecture-slides.html#types-of-relationships-among-variables",
    "title": "Model Building",
    "section": "Types of Relationships Among Variables",
    "text": "Types of Relationships Among Variables\nHere are the three main relationships we will be working with in this course."
  },
  {
    "objectID": "lectures/07-lecture-slides.html#the-model-behind-your-homework",
    "href": "lectures/07-lecture-slides.html#the-model-behind-your-homework",
    "title": "Model Building",
    "section": "The Model Behind Your Homework",
    "text": "The Model Behind Your Homework"
  },
  {
    "objectID": "lectures/07-lecture-slides.html#direct-relationships",
    "href": "lectures/07-lecture-slides.html#direct-relationships",
    "title": "Model Building",
    "section": "Direct Relationships",
    "text": "Direct Relationships\nA direct relationship, causal or otherwise, is a relationship in which a predictor variable (or independent variable) has a direct impact on an outcome variable."
  },
  {
    "objectID": "lectures/07-lecture-slides.html#examples-of-direct-relationships",
    "href": "lectures/07-lecture-slides.html#examples-of-direct-relationships",
    "title": "Model Building",
    "section": "Examples of Direct Relationships",
    "text": "Examples of Direct Relationships"
  },
  {
    "objectID": "lectures/07-lecture-slides.html#moderated-relationships-the-how-strong-heuristic",
    "href": "lectures/07-lecture-slides.html#moderated-relationships-the-how-strong-heuristic",
    "title": "Model Building",
    "section": "Moderated Relationships: The “How Strong” Heuristic",
    "text": "Moderated Relationships: The “How Strong” Heuristic\nA moderated relationship, causal or otherwise, involves three variables in which the relationship between two variables, a predictor and outcome variable, changes depending on the value of a third variable, the moderating variable or moderator."
  },
  {
    "objectID": "lectures/07-lecture-slides.html#thinking-about-moderation-with-plots",
    "href": "lectures/07-lecture-slides.html#thinking-about-moderation-with-plots",
    "title": "Model Building",
    "section": "Thinking About Moderation with Plots",
    "text": "Thinking About Moderation with Plots\nWhen you are hypothesizing a moderation—also referred to as an interaction—effect, it is helpful to draw a plot of your hypothesized effect."
  },
  {
    "objectID": "lectures/07-lecture-slides.html#indirect-mediated-relationships-the-why-heuristic",
    "href": "lectures/07-lecture-slides.html#indirect-mediated-relationships-the-why-heuristic",
    "title": "Model Building",
    "section": "Indirect (Mediated) Relationships: The “Why” Heuristic",
    "text": "Indirect (Mediated) Relationships: The “Why” Heuristic"
  },
  {
    "objectID": "lectures/07-lecture-slides.html#thinking-about-indirect-direct-and-total-effects",
    "href": "lectures/07-lecture-slides.html#thinking-about-indirect-direct-and-total-effects",
    "title": "Model Building",
    "section": "Thinking About Indirect, Direct, and Total Effects",
    "text": "Thinking About Indirect, Direct, and Total Effects\nWhen we hypothesize a mediated relationship, we can talk about three different effects:\n\nThe indirect effect of X on Y\nThe direct effect of X on Y\nThe total effect of X on Y"
  },
  {
    "objectID": "lectures/07-lecture-slides.html#what-does-a-path-diagram-tell-you",
    "href": "lectures/07-lecture-slides.html#what-does-a-path-diagram-tell-you",
    "title": "Model Building",
    "section": "What Does a Path Diagram Tell You?",
    "text": "What Does a Path Diagram Tell You?"
  },
  {
    "objectID": "lectures/07-lecture-slides.html#hypotheses-and-path-diagrams",
    "href": "lectures/07-lecture-slides.html#hypotheses-and-path-diagrams",
    "title": "Model Building",
    "section": "Hypotheses and Path Diagrams",
    "text": "Hypotheses and Path Diagrams"
  },
  {
    "objectID": "lectures/07-lecture-slides.html#regression-equations-and-path-diagrams",
    "href": "lectures/07-lecture-slides.html#regression-equations-and-path-diagrams",
    "title": "Model Building",
    "section": "Regression Equations and Path Diagrams",
    "text": "Regression Equations and Path Diagrams"
  },
  {
    "objectID": "lectures/07-lecture-slides.html#an-example",
    "href": "lectures/07-lecture-slides.html#an-example",
    "title": "Model Building",
    "section": "An Example",
    "text": "An Example"
  },
  {
    "objectID": "lectures/07-lecture-slides.html#testing-theories-with-models-and-data",
    "href": "lectures/07-lecture-slides.html#testing-theories-with-models-and-data",
    "title": "Model Building",
    "section": "Testing Theories with Models and Data",
    "text": "Testing Theories with Models and Data\nTheories inform the development of models, which go on to inform what data to collect and what hypotheses or propositions we should empirically test with real data and statistical models.\nThe more empirical tests a theory survives, the more confidence we tend to have in that theory."
  },
  {
    "objectID": "lectures/07-lecture-slides.html#communicating-models-with-a-path-diagram",
    "href": "lectures/07-lecture-slides.html#communicating-models-with-a-path-diagram",
    "title": "Model Building",
    "section": "Communicating Models with a Path Diagram",
    "text": "Communicating Models with a Path Diagram\nPath diagrams are used to visually communicate models and the relationships they posit among variables.\nPath diagrams follow several graphical conventions:\n\nVariables (or phenomena) are represented by a shape usually a square or circle.\nCausal and predictive relationships among variables are represented with a unidirectional arrow.\nNon-causal and non-predictive relationships (i.e. correlations) among variables are represented with curved, non-directional arrows."
  },
  {
    "objectID": "lectures/07-lecture-slides.html#a-more-advanced-example",
    "href": "lectures/07-lecture-slides.html#a-more-advanced-example",
    "title": "Model Building",
    "section": "A More Advanced Example",
    "text": "A More Advanced Example"
  },
  {
    "objectID": "lectures/07-lecture-slides.html#mediated-indirect-relationships-the-why-heuristic",
    "href": "lectures/07-lecture-slides.html#mediated-indirect-relationships-the-why-heuristic",
    "title": "Model Building",
    "section": "Mediated (Indirect) Relationships: The “Why” Heuristic",
    "text": "Mediated (Indirect) Relationships: The “Why” Heuristic\nA mediated (indirect) relationship is one where a predictor variable influences an outcome variable indirectly through its influence on a mediating variable referred to as a mediator."
  },
  {
    "objectID": "lectures/07-lecture-slides.html#why-use-a-path-diagram",
    "href": "lectures/07-lecture-slides.html#why-use-a-path-diagram",
    "title": "Model Building",
    "section": "Why Use a Path Diagram?",
    "text": "Why Use a Path Diagram?\nA path diagram is useful as it is a simple way to graphically communicate our:\n\nThe phenomena (variables) we are interested in\nHypotheses or theoretical propositions about the relationships among variables\nThe regression equations needed to test the hypothesized model"
  },
  {
    "objectID": "lectures/07-lecture-slides.html#the-model-behind-your-homework-as-an-example",
    "href": "lectures/07-lecture-slides.html#the-model-behind-your-homework-as-an-example",
    "title": "Model Building",
    "section": "The Model Behind Your Homework as an Example",
    "text": "The Model Behind Your Homework as an Example"
  },
  {
    "objectID": "lectures/07-lecture-slides.html#path-diagrams-and-hypotheses",
    "href": "lectures/07-lecture-slides.html#path-diagrams-and-hypotheses",
    "title": "Model Building",
    "section": "Path Diagrams and Hypotheses",
    "text": "Path Diagrams and Hypotheses\nFrom the path diagram for our homework, we can infer several different hypotheses/propositions:\n\nThe effect of perceived ease of use is partially mediated through a tool’s perceived usefulness.\nThe effect of perceived ease of use on one’s intention to use the tool is moderated by one’s previous experience with a similar tool.\nThe effect of perceived usefulness on one’s intention to use the tool is moderated by one’s previous experience with a similar tool."
  },
  {
    "objectID": "lectures/07-lecture-slides.html#path-diagrams-and-regression-equations",
    "href": "lectures/07-lecture-slides.html#path-diagrams-and-regression-equations",
    "title": "Model Building",
    "section": "Path Diagrams and Regression Equations",
    "text": "Path Diagrams and Regression Equations\nWe can also infer the regression equations needed to test our hypotheses from the path diagram:\n\\[X_{\\text{Useful.}}=\\beta_0 + \\beta_1X_{\\text{Ease Use}}+\\epsilon\\] \\[Y_{\\text{Beh. Int.}}=\\beta_0+\\beta_1X_{\\text{Ease Use}}+\\beta_2X_{\\text{Useful.}}+\\beta_3Z_{\\text{Mod.}}+\\epsilon\\]"
  },
  {
    "objectID": "lectures/07-lecture-slides.html#a-published-example",
    "href": "lectures/07-lecture-slides.html#a-published-example",
    "title": "Model Building",
    "section": "A Published Example",
    "text": "A Published Example\n\nGiessner et al. (2023). The impact of supportive leadership on employee outcomes during organizational mergers: An organizational-level field study."
  },
  {
    "objectID": "lectures/07-lecture-slides.html#a-more-advanced-published-example",
    "href": "lectures/07-lecture-slides.html#a-more-advanced-published-example",
    "title": "Model Building",
    "section": "A More Advanced Published Example",
    "text": "A More Advanced Published Example\n\n\n\n\n\n\n\n\n\n\n\nSteiner et al. (2023). Crossover effects of parent work-to-family experiences on child work centrality: A moderated mediation model. Journal of Applied Psychology, 108(6), 934-953."
  },
  {
    "objectID": "lectures/07-lecture-slides.html#the-human-costs-of-mergers-a-published-example",
    "href": "lectures/07-lecture-slides.html#the-human-costs-of-mergers-a-published-example",
    "title": "Model Building",
    "section": "The Human Costs of Mergers: A Published Example",
    "text": "The Human Costs of Mergers: A Published Example\n\n\n\n\n\n\n\nGiessner et al. (2023). The impact of supportive leadership on employee outcomes during organizational mergers: An organizational-level field study. Journal of Applied Psychology, 108(4), 686-697."
  },
  {
    "objectID": "lectures/07-lecture-slides.html#mediation-hypotheses-h1-to-h3",
    "href": "lectures/07-lecture-slides.html#mediation-hypotheses-h1-to-h3",
    "title": "Model Building",
    "section": "Mediation Hypotheses: H1 to H3",
    "text": "Mediation Hypotheses: H1 to H3\n\nH1: Merging organizations will experience greater decreases in job satisfaction relative to nonmerging organizations within the same context and time period. [Total Effect]\nH2: Merging organizations will experience greater increases in absenteeism relative to nonmerging organizations within the same context and time period. [Direct Effect]\nH3: The increases in absenteeism for merging organizations will be (partially) mediated by decreases in job satisfaction. [Indirect Effect]\n\n\n\nGiessner et al. (2023). The impact of supportive leadership on employee outcomes during organizational mergers: An organizational-level field study. Journal of Applied Psychology, 108(4), 686-697."
  },
  {
    "objectID": "lectures/07-lecture-slides.html#moderation-hypothesis-h4",
    "href": "lectures/07-lecture-slides.html#moderation-hypothesis-h4",
    "title": "Model Building",
    "section": "Moderation Hypothesis: H4",
    "text": "Moderation Hypothesis: H4\n\nH4: The decreases in job satisfaction for merged versus nonmerged organizations will be moderated by changes in supportive leadership at the midlevel management level, such that this relationship will be weaker for organizations with increasing levels of supportive leadership.\n\n\n\nGiessner et al. (2023). The impact of supportive leadership on employee outcomes during organizational mergers: An organizational-level field study. Journal of Applied Psychology, 108(4), 686-697."
  },
  {
    "objectID": "lectures/07-lecture-slides.html#moderated-mediation-hypothesis-h5",
    "href": "lectures/07-lecture-slides.html#moderated-mediation-hypothesis-h5",
    "title": "Model Building",
    "section": "Moderated Mediation Hypothesis: H5",
    "text": "Moderated Mediation Hypothesis: H5\n\nH5: The increases in absenteeism for merging organizations relative to nonmerging organizations as mediated by the decreases in job satisfaction will be moderated by changes in supportive leadership at the midlevel management level, such that the indirect effect of mergers on absenteeism via job satisfaction will be weaker when there is an increase in supportive leadership.\n\n\n\nGiessner et al. (2023). The impact of supportive leadership on employee outcomes during organizational mergers: An organizational-level field study. Journal of Applied Psychology, 108(4), 686-697."
  },
  {
    "objectID": "lectures/07-lecture-slides.html#tests-of-mediation-hypotheses",
    "href": "lectures/07-lecture-slides.html#tests-of-mediation-hypotheses",
    "title": "Model Building",
    "section": "Tests of Mediation Hypotheses",
    "text": "Tests of Mediation Hypotheses"
  },
  {
    "objectID": "lectures/07-lecture-slides.html#tests-of-moderation-hypotheses",
    "href": "lectures/07-lecture-slides.html#tests-of-moderation-hypotheses",
    "title": "Model Building",
    "section": "Tests of Moderation Hypotheses",
    "text": "Tests of Moderation Hypotheses"
  },
  {
    "objectID": "lectures/07-lecture-slides.html#moderation-plot",
    "href": "lectures/07-lecture-slides.html#moderation-plot",
    "title": "Model Building",
    "section": "Moderation Plot",
    "text": "Moderation Plot\n\n\n\n\n\n\n\nGiessner et al. (2023). The impact of supportive leadership on employee outcomes during organizational mergers: An organizational-level field study. Journal of Applied Psychology, 108(4), 686-697."
  },
  {
    "objectID": "lectures/07-lecture-slides.html#regression-models-mediation-hypotheses",
    "href": "lectures/07-lecture-slides.html#regression-models-mediation-hypotheses",
    "title": "Model Building",
    "section": "Regression Models: Mediation Hypotheses",
    "text": "Regression Models: Mediation Hypotheses\n\n\n\n\n\n\n\nGiessner et al. (2023). The impact of supportive leadership on employee outcomes during organizational mergers: An organizational-level field study. Journal of Applied Psychology, 108(4), 686-697."
  },
  {
    "objectID": "lectures/07-lecture-slides.html#regression-models-moderation-hypotheses",
    "href": "lectures/07-lecture-slides.html#regression-models-moderation-hypotheses",
    "title": "Model Building",
    "section": "Regression Models: Moderation Hypotheses",
    "text": "Regression Models: Moderation Hypotheses\n\n\n\n\n\n\n\nGiessner et al. (2023). The impact of supportive leadership on employee outcomes during organizational mergers: An organizational-level field study. Journal of Applied Psychology, 108(4), 686-697."
  },
  {
    "objectID": "lectures/07-lecture-slides.html#using-theories-to-build-models",
    "href": "lectures/07-lecture-slides.html#using-theories-to-build-models",
    "title": "Model Building",
    "section": "Using Theories to Build Models",
    "text": "Using Theories to Build Models\nGiessner et al. (2023) use two broad theories to build their more narrow model:\n\nOrganizational support theory\nConservation of resources theory"
  },
  {
    "objectID": "lectures/07-lecture-slides.html#a-more-complex-published-example",
    "href": "lectures/07-lecture-slides.html#a-more-complex-published-example",
    "title": "Model Building",
    "section": "A More Complex Published Example",
    "text": "A More Complex Published Example\n\n\n\n\n\n\n\n\n\n\n\nSteiner et al. (2023). Crossover effects of parent work-to-family experiences on child work centrality: A moderated mediation model. Journal of Applied Psychology, 108(6), 934-953."
  }
]
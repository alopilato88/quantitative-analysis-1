---
title: "Inference for Linear Regression"
format: 
  revealjs:
    theme: [default, theme-lecture-slides.scss] 
    css: styles-lecture-slides.css
    slide-number: true
execute:
  echo: true
editor_options: 
  chunk_output_type: console
---

```{r}
#| label: load-packages
#| include: false
source("packages-lecture.R")
source("helper-functions-lecture.R")

```

```{r}
#| echo: false
data_lecture <- readr::read_csv("https://alopilato88.github.io/quantitative-analysis-1/data/03-lecture-data.csv")
plot_alpha <- .50
plot_fill <- "#3F72AF"
plot_color <- "#112D4E"
```

## Schedule for Today

* Talk about stats for ~75 mins (5 PM - 6:15 PM ET)
* Break for 10 minutes 
* Finish up stats for 35 mins (6:25 PM - 7:00 PM ET)

## Overview 

* A review of statistical inference
* Setup of our working example
* Introduction to inference for linear regression

## Goals

* Improve your understanding of statistical inference
* Understand the assumptions underlying linear regression and their importance
* Understand hypothesis testing under linear regression

## Inferential Statistics

Usually, when you analyze your data you want to generalize the results from your specific dataset to a **broader population** or **more general process**. This is called **statistical inference**.

## Inference about Population or Process? 

When we make statistical inferences we are generally making inferences to one of two "things":

* **Population** such as all eligible US voters
* **Data Generating Process** which is the population model you believe generated your data

## Data Generating Process & Your Collected Data

In social scientific and behavioral research, it is usually assumed that there is a **data generating process** that produces the data you have collected. The specific data you collected is useful because it allows us to make an **inference** about the underlying **data generating process**.

## Our Data Generating Process: The Regression Model

In our course examples, we have been **assuming** that our data is produced by a **process** that we can represent as a **population linear regression model**:

$$Y = \beta_0+\beta_1X_{1}+\beta_2X_2+...+\beta_pX_p + \epsilon$$

The $\beta$ coefficients and $\epsilon$ below are the population parameters or just parameters if our **data generating process**.

## Estimating Our Data Generating Process with Data

We can use our data to estimate the data generating process (population regression model in this case):

$$Y=\hat{\beta}_0+\hat{\beta_1}X_1+\hat{\beta_2}X_2+...\hat{\beta_p}X_p+\hat{\epsilon}$$

## Our Working Example for Today

To keep things simple, our assumed data generating process will be: 

$$Y_{\text{Freq. AI Use}}=\beta_0+\beta_1X_{\text{Beh. Intent.}} + \epsilon$$

## Estimating the Regression Coefficients of our DGP

It is easiest to illustrate what we mean when we say "estimating our data generating process" by looking at the equations used to **estimate** the coefficients in a simple regression model: 

$$\hat{\beta}_1=\frac{\Sigma(X - \overline{X})(Y-\overline{Y})}{\Sigma(X-\overline{X})^2}$$

$$\hat{\beta}_0=\overline{Y}-\hat{\beta}_1\overline{X}$$

Both equations, only use the **collected data**.

## Estimating Model Errors

We will never observe the true errors (residuals) of our regression model, $\epsilon$. For each observation in our data, we have to estimate $\epsilon$ by computing:

$$Y_{\text{Freq. AI Use}}-(\hat{\beta}_0+\hat{\beta}_1X_{\text{Beh. Intent.}})$$

These observed errors are commonly referred to as residuals. The bulk of the linear regression assumptions are about the distribution of the unobserved error!

## Estimating the Error Variance

Because we will never know the true model errors, we have to use the errors we computed from our regression model to estimate things like the variance and standard deviation of error: 

$$Var(Y - \hat{Y})=\sigma^2_{error}$$


## The Tools of Statistical Inference

We have two **inferential tools** to help us determine if our data supports our data generating process: 

* Statistical Significance Tests
* Confidence Intervals 

## The Four Components of a Significance Test & Confidence Interval

In order to ensure our **inferential tools** work, we need to assess the quality of their four different components:

* Statistical Model Assumptions
* Hypotheses (directional or non-directional)
* Test Statistic
* P-Value (for significance tests)

# Statistical Assumptions 

## Assumptions of Linear Regression Analysis

To ensure that inferences made about our regression coefficients are **appropriate**, we have to make six assumptions: 

1. Validity of our Data
2. Representativeness of our Data
3. Additivity & Linearity 
4. Independence of Errors
5. Equal Variance of Errors (Homoscedasticity)
6. Normality of Errors 

## Validity of Our Data

One of the most important assumptions we need to make is that **our data maps to our research questions**. We want to make sure that the data we have collected is a **valid** representation of the phenomenon in which we are interested! 

There is a whole field called **Psychometrics** that is dedicated to issues around the validity of measurements!

## Representativeness of Our Data

The data you are estimating your regression model from should be representative of the population you are making inferences about. This applies even when we are making inferences about a data generating process. 

## Additivity & Linearity

Additivity and Linearity is the assumption that the conditional mean of the outcome variable is a linear function of the different predictor variables: 

$$E[Y_{\text{AI Freq.}}|X_{\text{Beh. Intent.}}] = \beta_0 + \beta_1X_{\text{Beh. Intent.}}$$

Violations of this assumption happen when there is a nonlinear relationship between your outcome variable and at least one of your predictor variables. 

## Additivity & Linearity: A Graphical View

```{r}
#| echo: false
#| fig-align: center

mod <- lm(freq_use_ai ~ beh_intent_ai, data = data_lecture)

data_lecture <- 
  data_lecture |>
  dplyr::mutate(
    y_cond_mean = predict(mod)
  )

data_mean_plot <- 
  data_lecture |>
  dplyr::summarize(
    y_mean = mean(freq_use_ai),
    .by = beh_intent_ai
  )

plot_jitter_mean <- 
  ggplot2::ggplot(
  data = data_lecture |>
    dplyr::mutate(
      x_label = paste0(beh_intent_ai, " - ", beh_intent_ai_label)
    ),
  ggplot2::aes(
    x = as.factor(x_label),
    y = freq_use_ai
  )
) + 
  ggplot2::geom_jitter(
    fill = plot_color,
    color = plot_fill,
    width = .4,
    height = .6,
    alpha = .50
  )  + 
  ggplot2::geom_smooth(
    ggplot2::aes(
      x = beh_intent_ai,
      y = freq_use_ai
    ),
    method = lm,
    formula = y ~ x,
    se = FALSE,
    color = plot_color,
    lwd = 1.20
  ) +
  ggplot2::geom_point(
    data = data_mean_plot,
    ggplot2::aes(
      x = beh_intent_ai, 
      y = y_mean
    ),
    fill = plot_color,
    size = 5
  ) +
  lecture_ggplot_theme_barplot +
  ggplot2::theme(
    axis.text.x = element_text(angle = 90)
  ) +
  ggplot2::labs(
    x = "Behavioral Intentions Towards AI Tool Use",
    y = "Frequency of AI Tool Use"
  ) 


plot_jitter_mean

```

## Independence of Errors

The assumption of independent errors (homoscedasticity) means that once we control for all the predictor variables, then there should be no relationship among our model residuals (the part of our outcome variable that is not captured by our model).

## Independence of Errors: A Graphical View

```{r}
#| fig-align: center
#| echo: false
set.seed(54345)
np <- 30
ng <- 100
gid <- rep(1:ng, each = np)
x <- rnorm(np * ng)
gid_matrix <- model.matrix(~as.factor(gid) - 1)
b <- 1
e <- rnorm(np * ng)
ranef_1 <- rnorm(ng, sd = sqrt(1))
ranef_2 <- rnorm(ng, sd = 10)

y1 <- x + e 
y2 <- x + e + gid_matrix %*% ranef_1
y3 <- x + e + gid_matrix %*% ranef_2

m1 <- lm(y1 ~ x)
m2 <- lm(y2 ~ x)
m3 <- lm(y3 ~ x)

data_ind <- 
  tibble::tibble(
    x = rep(x, 3),
    resids = c(m1$residuals, m2$residuals, m3$residuals),
    group = rep(gid, 3),
    id1 = rep(c("No Violation", "Moderate Violation", "Strong Violation"), each = np* ng),
    id = factor(id1, levels = c("No Violation", "Moderate Violation", "Strong Violation"))
  )

ggplot2::ggplot(
  data_ind,
  ggplot2::aes(
      y = resids,
      x = group
  )
) + 
  ggplot2::geom_jitter(
    color = plot_fill,
    alpha = plot_alpha
  ) + 
  ggplot2::facet_wrap(~id, scales = "free") +
  lecture_ggplot_theme_barplot +
  ggplot2::labs(
    y = "Model Residuals",
    x = "Organization"
  )
  
```

## Equal Variance of Errors (Homoscedasticity)

The **assumption of equal error variances** means that the variance of our errors (residuals) is constant across all values of our predictor variables. A different way to say this is that our errors (residuals) are unrelated to to our predictor variables. 

## Equal Variance of Errors: A Graphical View 

```{r}
#| fig-align: center
#| echo: false
set.seed(534243)
n <- 1000
x <- runif(n)

e1 <- rnorm(n)

y1 <- x + e1
y2 <- x + e1*2*x

m1 <- lm(y1 ~ x)
m2 <- lm(y2 ~ x)

data_err_var <- 
  tibble::tibble(
    x = rep(x, 2),
    resids = c(m1$residuals, m2$residuals),
    id = rep(c("Constant Error Variance", "Non-Constant Error Variance"), each = n)
  )

ggplot2::ggplot(
  data = data_err_var,
  ggplot2::aes(
    x = x,
    y = resids
  )
) + 
  ggplot2::geom_point(
    color = plot_fill,
    alpha = plot_alpha
  ) + 
  ggplot2::facet_grid(
    ~ id,
    scales = "free"
  ) +
  lecture_ggplot_theme_barplot +
  ggplot2::labs(
    x = "Behavioral Intentions Toward AI Use",
    y = "Model Errors (Residuals)"
  )

```

## Normality of Errors

The normality of errors assumption means that our errors follow a **normal probability distribution** with a **mean of zero** and a **variance equal to $\sigma^2_{e}$**. 

Another way to state this assumption is that the distribution of your outcome variable when conditioned on all of your predictor variables follows normal probability distribution. 

**It is important to note that the distribution of your outcome variable without conditioning on your predictor variables need not follow a normal probability distribution!**

## Normality of Errors: A Graphical View

```{r}
#| echo: false
#| fig-align: center
set.seed(44623)
n <- 5000
x <- rgamma(n, shape = 1)
e <- rnorm(n, sd = sqrt(1))
y <- 2*x + e

m1 <- lm(y ~ x)
data_normal <- 
  tibble::tibble(
    outcome = c(y, m1$residuals),
    id1 = rep(c("Outcome Variable", "Model Residuals"), each = n),
    id = factor(id1, levels = c("Outcome Variable", "Model Residuals"))
  )

ggplot2::ggplot(
  data = data_normal,
  ggplot2::aes(
    x = outcome
  )
) + 
  ggplot2::geom_histogram(
    color = plot_color,
    fill = plot_fill,
    bins = 30
  ) + 
  ggplot2::facet_wrap(~ id,
                      scale = "free") + 
  ggplot2::labs(
    x = "Measurements on the Outcome Variable",
    y = "Count"
  ) + 
  lecture_ggplot_theme_barplot
```

## The Probability Model Implied By Our Assumptions

When taken all together, our linear regression assumptions imply the following probabilistic model for the **conditional distribution of our outcome variable**:

$$Y_{i}|X_{i} \sim N(\beta_0 + \beta_1X_{\text{Beh. Intent.}},\sigma^2_e )$$

$$\epsilon_i\sim N(0, \sigma^2_e)$$

# Developing Hypotheses 

## Hypotheses and the Data Generating Process

You can think of a hypothesis as a statement about the data generating process. This statement needs to be made **before your analyses**! 

To setup a significance test, you will need to develop two kinds of hypotheses: 

* Null Hypothesis ($H_0$): Specifies that a parameter in your DGP takes on a specific value (usually 0).
* Alternative Hypothesis ($H_a$): Specifies that a parameter takes on some alternative range of values (e.g. all positive values).

## Setting Up the Null Hypothesis

The null hypothesis is usually specifies that the parameter you DGP takes on is 0:

* $H_0$: There is no relationship between behavioral intentions to use the AI tool and the frequency with which the tool is used ($\beta_1=0$).

Although it is incredibly rare, the null hypothesis can specify values other than 0: 

* $H_0$: The relationship between behavioral intentions to use the AI tool and the frequency with which the tool is used is equal to -.50 ($\beta_1=-.50$).

## Setting Up the Alternative Hypothesis

The alternative hypothesis can be a directional or non-directional statement:

* Directional $H_a$: The relationship between behavioral intentions to use the AI tool and the frequency with which the tool is used is **positive** ($\beta_1 > 0$).
* Non-Directional $H_a$: **There is a relationship** between behavioral intentions to use the AI tool and the frequency with which the tool is used ($\beta_1 \neq 0$).

# Testing Our Results with a Test Statistic

## What is a Test Statistic? 

The test statistic is a number that tells us how many **standard errors** our estimate falls from the parameter value specified by the null hypothesis. Generally, a test statistic has three pieces: 

1. The estimate being tested (our estimated regression coefficient)
2. The value specified by the null hypothesis (almost always 0)
3. The standard error of the estimate

## T-Test: The Test Statistic for Linear Regression

We use a T-test to test the significance of any **single** regression coefficient:

$$t = \frac{\hat{\beta}_1-\beta_{\text{Null Hyp}}}{SE(\hat{\beta}_1)}$$


## Sampling Distribution & Standard Error

Remember the sampling distribution is a **hypothetical** distribution of all our possible $\hat{\beta}_1$ estimates and the standard error is the **standard deviation of that distribution**. 

## Standard Error of a Partial Regression Coefficient

The standard error of a partial regression coefficient tells how close our estimated coefficient is to the true value of that coefficient. The SE is affected by four different components: 

1. An estimate of the overall amount of modeling error: $\hat{\sigma}^2_{error}$
2. The variance of the predictor variable: $Var(X_j)$
3. The overall sample size: $N$
4. The proportion of variance in the predictor variable that is explained by the other predictor variables: $R^2_j$

## Formula for the Partial Regression Coefficient SE

$$SE(\hat{\beta}_j)=\sqrt{\frac{1}{1-R^2_j}}\times\sqrt{\frac{\hat{\sigma}^2_{error}}{N \times Var(X_j)}}$$

# Determining Statistical Significance 

## Why is it called a t-test? 

To test the significance of a t-statistic, we have to use a probability distribution called the **Student's t distribution**. 

We have to use the Student's t distribution for one very specific reason: we are using an **estimate** of the model error variance ($\sigma^2_e$) because we do not know the true value. If we knew the true value, we could use that in the SE formula and then calculate the significance of the test statistic using the Normal distribution.

## Student's t Distribution

The Student's t Distribution or t distribution is a probability distribution that in many ways is like the Normal distribution (symmetrical around its mean of 0). However, the t distribution allows for more extreme observations compared to the Normal distribution---the t distribution tails are fatter than the Normal distribution tails.    

```{r}
#| echo: false
#| fig-align: center
x_lim <- 7
plot_t <- 
  ggplot2::ggplot() + 
  ggplot2::geom_area(
    ggplot2::aes(x = -x_lim:x_lim),
    stat = "function",
    fun = dt,
    args = list(df = 3),
    xlim = c(-x_lim, x_lim),
    fill = plot_fill,
    color = plot_color,
    lwd = 1.20,
    alpha = 0
  ) + 
   ggplot2::geom_area(
    ggplot2::aes(x = -x_lim:x_lim),
    stat = "function",
    fun = dnorm,
    xlim = c(-x_lim, x_lim),
    color = plot_color,
    fill = plot_fill,
    lty = "longdash",
    lwd = 1.20,
    alpha = 0
  ) +
  lecture_ggplot_theme_barplot + 
  ggplot2::labs(
    y = "Probability",
    x = "Values of a Variable",
    title = "Normal Distribution (Dashed Lines) & t Distribution (Solid Lines)"
  )

plot_t
```

## Student's T Distribution & Degrees of Freedom

The shape of the t distribution is controlled by something called the **degrees of freedom**. The larger the **degrees of freedom**, the more similar the t distribution is to the Normal distribution.

```{r}
#| echo: false
#| fig-align: center
set.seed(3523)
n <- 10000
rt_var_low <- rt(n, df = 5)
rt_var_high <- rt(n, df = 1000)
rn_var <- rnorm(n)

var_vec <- c(rt_var_low, rt_var_high, rn_var)
data_plot <- 
  tibble::tibble(
    response_value = var_vec,
    id = rep(c("T Distribution (DF = 5)", "T Distribution (DF = 1000)", "Normal Distribution"), each = n)
  ) |>
  dplyr::mutate(
    id = factor(id,
                levels = c("Normal Distribution", "T Distribution (DF = 1000)", "T Distribution (DF = 5)"))
  )

plot_t <- 
  ggplot2::ggplot(
    data = data_plot,
    ggplot2::aes(
      x = response_value
    ) 
  ) + 
  ggplot2::geom_histogram(
    bins = 40,
    color = plot_color,
    fill = plot_fill
  ) +
  ggplot2::facet_wrap(
    ~ id,
    scale = "free"
  ) +
  lecture_ggplot_theme_barplot
```

## What on Earth are Degrees of Freedom?

Think of your data as **"statistical cash"** to be spent on estimating the regression coefficients of your model. **Degrees of freedom** is almost like an accounting system that tracks: 

* **Total net worth at the start**: $df_{Total} = N$
* **Cash spent on your model**: $df_{Model} = k + 1$
* **Cash remaining**: $df_{Residual} = N-k-1$

## Statistical Accounting with Degress of Freedom

![](lecture-imgs/stat-bank.png){fig-align="center"}

## Don't Go Broke

You always want to have a positive (and large) amount for $df_{Residual}$. We need a positive amount of $df_{Residual}$ in order to test how well our model fits (e.g. calculate the $R^2$). If we spend **ALL** of our statistical cash on estimating regression coefficients, then we will not have any cash left over to determine our model fit.

## P-Value for a T-Statistic 

To calculate a p-value for our t-statistic, we need three things: 

* T-Statistic
* $df_{Residual}$: This tells us which t distribution to use
* The direction of the hypothesis (directional or non-directional)

The p-value will tell us **the probability of seeing a value as large or larger** than our t-statistic, if we believe the true t-statistic should be 0." Then, if the p-value is very small (less than or equal to .05), we say it's pretty unlikely, so we can reject our null hypothesis. 

## An Example with our Data

```{r}
model <- lm(freq_use_ai ~ beh_intent_ai + tech_anx, data = data_lecture)
summary(model)
```

## Critical Regions for Non-Directional & Directional Hypotheses

The shaded areas are called critical (or rejection) regions.

```{r}
#| echo: false
#| fig-align: center
non_directional_plot <- 
  ggplot2::ggplot(
    data = NULL,
    ggplot2::aes(
      c(-4, 4)
    )
  ) + 
  ggplot2::geom_area(
    stat = "function",
    fun = dt,
    args = list(df = 4997),
    xlim = c(-4, qt(.025, df = 4997)),
    fill = plot_fill,
    color = plot_color
  ) + 
  ggplot2::geom_area(
    stat = "function",
    fun = dt,
    args = list(df = 4997),
    xlim = c(qt(.025, df = 4997), qt(.975, df = 4997)),
    fill = "#F9F7F7",
    color = plot_color
  ) + 
  ggplot2::geom_area(
    stat = "function",
    fun = dt,
    args = list(df = 4997),
    xlim = c(qt(.975, df = 4997), 4),
    fill = plot_fill,
    color = plot_color
  ) + 
  ggplot2::geom_text(
    ggplot2::aes(
      x = qt(.025, df = 4997),
      y = dt(qt(.025, df = 4997), df = 4997),
      label = paste0("Cutoff = ", round(qt(.025, df = 4997), 2))
    ),
    hjust = 1.15,
    fontface = "bold"
  ) + 
  ggplot2::geom_text(
    ggplot2::aes(
      x = qt(.975, df = 4997),
      y = dt(qt(.975, df = 4997), df = 4997),
      label = paste0("Cutoff = ", round(qt(.975, df = 4997), 2))
    ),
    hjust = -.15,
    fontface = "bold"
  ) +
  ggplot2::geom_text(
    ggplot2::aes(
      x = qt(.025, df = 4997),
      y = .01,
      label = "2.5%"
    ),
    hjust = 1,
    fontface = "bold",
    size = 3
  ) + 
  ggplot2::geom_text(
    ggplot2::aes(
      x = qt(.975, df = 4997),
      y = .01,
      label = "2.5%"
    ),
    hjust = -.10,
    fontface = "bold",
    size = 3
  ) +
  lecture_ggplot_theme_barplot + 
  ggplot2::labs(
    x = "T-Values",
    y = "Probability",
    title = "Non-Directional Hypothesis"
  )

directional_plot <- 
  ggplot2::ggplot(
    data = NULL,
    ggplot2::aes(
      c(-4, 4)
    )
  ) + 
  ggplot2::geom_area(
    stat = "function",
    fun = dt,
    args = list(df = 4997),
    xlim = c(-4, qt(.95, df = 4997)),
    fill = "#F9F7F7",
    color = plot_color
  ) + 
  ggplot2::geom_area(
    stat = "function",
    fun = dt,
    args = list(df = 4997),
    xlim = c(qt(.95, df = 4997), 4),
    fill = plot_fill,
    color = plot_color
  ) + 
  ggplot2::geom_text(
    ggplot2::aes(
      x = qt(.95, df = 4997),
      y = dt(qt(.95, df = 4997), df = 4997),
      label = paste0("Cutoff = ", round(qt(.95, df = 4997), 2))
    ),
    hjust = -.15,
    fontface = "bold"
  ) +
  ggplot2::geom_text(
    ggplot2::aes(
      x = qt(.96, df = 4997),
      y = .025, 
      label = "5%",
      fontface = "bold"
    ),
    hjust = -.15,
    fontface = "bold"
  ) +
  lecture_ggplot_theme_barplot + 
  ggplot2::labs(
    x = "T-Values",
    y = "Probability",
    title = "Directional Hypothesis"
  ) 

hyp_pw <- (non_directional_plot | directional_plot) & lecture_ggplot_theme_barplot
hyp_pw
```

## Calculating P-Values for Directional Hypotheses 

R (and all other statistical software programs) provide the p-value for a non-directional hypothesis. It is totally fine to use this p-value even if you specify a directional hypothesis, you just need to be aware that the p-value for a directional hypothesis will always equal: 

$$\text{p-value}_{\text{Directional}}=\frac{\text{p-value}_{\text{Non-Directional}}}{2}$$

## F-Test!? 

The F-Test is a test of two identical null hypotheses:

* $R^2 = 0$
* $\beta_1=\beta_2=...=\beta_p=0$

If the p-value for the F-test is less than or equal to .05 you can conclude that the $R^2$ is not equal to 0 and that **at least one** of your regression coefficients is not equal to 0. 

## The Underlying Math of the F-Test

See me after class if you really want to know this...

# Feeling Condfident with Confidence Intervals

## What is a Confidence Interval?

A confidence interval is a range of values that the true parameter likely falls within. 

## The Components of a Confidence Interval

$$\hat{\beta} \pm \text{SE}_\beta \times T_{\text{Perc.}}$$

* **Estimate**: The regression coefficient from your model
* **Standard Error**: The standard error of that regression coefficient 
* **T Value for the 97.5~th~ Percentile**: A value that is roughly 2

## Confidence Interval for Behavioral Intention

We are 95% confident that the true value for the effect of behavioral intention on frequency of AI use is in this range: `r confint(model)[2,] |> round(3)`

```{r}
#| echo: false
#| fig-align: center

estimate <- summary(model)$coefficients[2, 1]
se <- summary(model)$coefficients[2, 2]
lbound <- (estimate) - se * qt(.975, df = 4997)
ubound <- (estimate) + se * qt(.975, df = 4997)

ggplot2::ggplot(
  data = NULL,
  ggplot2::aes(
    c(0.5271689, .6071689)
  )
) +
  ggplot2::geom_area(
    stat = "function",
    fun = calc_t_confint,
    args = list(estimate = estimate, se = se, df = 4997),
    xlim = c(0.5271689, lbound),
    fill= "#F9F7F7",
    color = plot_color
  ) + 
  ggplot2::geom_area(
    stat = "function",
    fun = calc_t_confint,
    args = list(estimate = estimate, se = se, df = 4997),
    xlim = c(lbound, ubound),
    fill = plot_fill,
    color = plot_color
  ) + 
  ggplot2::geom_area(
    stat = "function",
    fun = calc_t_confint,
    args = list(estimate = estimate, se = se, df = 4997),
    xlim = c(ubound, 0.6071689),
    fill = "#F9F7F7",
    color = plot_color
  ) + 
  ggplot2::geom_segment(
    ggplot2::aes(
      x = estimate,
      xend = estimate, 
      y = 0, 
      yend = dt(0, df = 4997)
    ),
    lwd = 1.3
  ) +
  ggplot2::geom_segment(
    ggplot2::aes(
      x = lbound,
      xend = lbound, 
      y = 0, 
      yend = dt(qt(.025, df = 4997), df = 4997)
    ),
    lwd = 1.3,
    lty = "dashed"
  ) +
  ggplot2::geom_segment(
    ggplot2::aes(
      x = ubound,
      xend = ubound, 
      y = 0, 
      yend = dt(qt(.975, df = 4997), df = 4997)
    ),
    lwd = 1.3,
    lty = "dashed"
  ) + 
  ggplot2::labs(
    x = "Probable True Parameter Values",
    y = "Probability"
  ) +
  lecture_ggplot_theme_barplot
```


## Compability Interval > Confidence Intveral

I believe the best way to understand the confidence interval is as a compatibility interval. The interval tells us the range of null hypothesis values that we would not be able to reject (they would have p-values greater than or equal to .05). 

```{r}
#| echo: false
#| fig-align: center
cint <- c(.546, confint(model)[2, ], .588)
cint_trans <- (cint - estimate)/se
cint_table <- 
  tibble::tibble(
    `Null Value` = c(cint[1], cint[2], cint[3], cint[4]),
    `P-Value` = c(pt(cint_trans[1], df = 4997, lower.tail = TRUE)*2, pt(cint_trans[2], df = 4997, lower.tail = TRUE)*2,
                  pt(cint_trans[3], df = 4997, lower.tail = FALSE)*2, pt(cint_trans[4], df = 4997, lower.tail = FALSE)*2)
  ) |>
  dplyr::mutate(
    `Null Value` = round(`Null Value`, 3),
    `P-Value` = round(`P-Value`, 3)
  )

cint_table |>
  knitr::kable()
```

## Why Confidence (Compatability Intervals) Trump P-Values

Here is a example where a finding is **statistically** different from 0, but not **practically** different. The P-Value does not tell you the estimate is not practically different from 0, but the confidence interval does:

```{r}
#| echo: false
set.seed(864)
n <- 100
sdx <- 2
esd <- 2*sqrt(n)*sdx
se <- esd/(sqrt(n)*sdx)

x <- rnorm(n, sd = sdx)

y <- (.05 + qt(.975, df = n - 2, lower.tail = TRUE)*se)*x + rnorm(n, sd = esd)

mod <- lm(y ~ x)
summ_mod <- summary(mod)
cint <- confint(mod)
coef_table <- 
  as.data.frame(summ_mod$coefficients) |>
  tibble::rownames_to_column() |>
  dplyr::mutate(
    lower_bound = cint[, 1],
    upper_bound = cint[, 2],
    dplyr::across(Estimate:upper_bound, ~round(.x, 3))
  ) |>
  dplyr::rename(
    Effect = rowname,
    `P-Value` = `Pr(>|t|)`,
    `Lower Bound` = lower_bound,
    `Upper Bound` = upper_bound
  ) |>
  dplyr::select(
    -`t value`
  )

coef_table$Effect[2] <- "Predictor Var."

coef_table |>
  knitr::kable()
```



